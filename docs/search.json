[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "1 About this book\nIn the modern business landscape, data isn’t just an asset – it’s the raw material from which informed decisions are forged. Data, however, does not speak for itself. The extraction of actionable insights requires not only technical prowess, but a sophisticated understanding of causal inference. This is where the business data scientist steps in, acting as the voice of data, translating its complex signals into meaningful narratives that drive strategic decision-making. The field is particularly well-suited for those with a background in economics, as economists generally possess the analytical skills, statistical training, and problem-solving mindset essential to excel in this role.\nHowever, it’s important to note that this does not mean all economists will automatically make good business data scientists, nor that only economists are suited for this career. Other academic backgrounds that can prepare you well for this field include statistics, biostatistics, computer science, and even certain areas of psychology or sociology that emphasize quantitative methods. The key is to remember that technical knowledge alone is not sufficient. A successful business data scientist must be able to engage effectively with stakeholders, understand the decisions they’re grappling with, help frame business questions that can be answered with data, and communicate findings in a clear, actionable manner.\nThis book is your compass in the dynamic world of business data science, designed for those aspiring to not just analyze data, but to truly understand and influence the underlying causes and effects within a business context. The business data scientist is a unique breed, blending the rigor of a statistician with the acumen of a strategist. They are experts in applying an analytical lens to business problems, leveraging techniques from causal inference, advanced statistical modeling, and forecasting. They possess the ability to discern the optimal approach for a given problem, communicating complex findings clearly to both technical and non-technical stakeholders.\nWhile proficient in data extraction from large datasets (e.g., using SQL), what truly sets these professionals apart is a deep-seated understanding of the assumptions underpinning their chosen methods, allowing them to critically evaluate results and avoid blind reliance on off-the-shelf tools. Furthermore, they are adaptable problem-solvers, capable of implementing advanced methodologies from scratch or even designing entirely novel approaches when faced with unconventional challenges.\nThroughout this book, we’ll navigate the core principles of causal inference, learning how to confidently identify cause-and-effect relationships within data. We’ll delve into the critical role of experimental design, covering randomized controlled trials, from simple A/B tests to Bayesian adaptive designs. We’ll also explore observational methods for when experiments are not possible, elucidating how to analyze their outcomes to reach valid conclusions. Our focus will be on techniques to mitigate inherent biases and draw meaningful insights from non-experimental data.\nOur exploration will emphasize a “decisions first” philosophy. This approach prioritizes a clear articulation of the business problem at hand, ensuring data analysis is always laser-focused on informing and optimizing decision-making. To ground these concepts in reality, we’ll provide practical examples and case studies spanning diverse industries, showcasing how data science can be wielded to address tangible business challenges.\nTo facilitate your learning journey, we’ll incorporate code examples. These examples will illuminate the technical aspects of data analysis, empowering you to apply them to your own projects. Additionally, each chapter includes links to relevant academic papers and further resources, allowing you to dive deeper into any topic that piques your interest or demands more thorough exploration for your specific needs.\nBy the book’s conclusion, you’ll possess a basic foundation in business data science and the confidence to leverage data as a driving force for decision-making within your organization. Let’s embark on this illuminating journey together, unlocking the power of data to propel your business success.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Introduction",
    "section": "1.1 Disclaimer",
    "text": "1.1 Disclaimer\nIn its current state, I would not call this a book. At best, it represents the draft of an idea for the first draft of a book. As such, there are many elements missing, and likely several errors.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction",
    "section": "1.2 License",
    "text": "1.2 License\nThis book is licensed under the Creative Commons Attribution-NonCommercial 4.0 License.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapter_01.html",
    "href": "chapter_01.html",
    "title": "1  What Does it Mean to Be Data-Driven?",
    "section": "",
    "text": "In today’s tech-driven world, data is king. Every click, swipe, and search generates a breadcrumb of information. Alas, although most decision-makers want to be data-driven, data does not speak for itself.\nSo, what does it mean to be data-driven? At its core, it’s about using data to inform decisions, not just describe them. It’s about moving beyond correlation – the “what goes with what” – and understanding causation, the “why” behind the patterns we see. To be truly data-driven, there must be some level of evidence that the data can provide that would make you choose a different path than the one you would have otherwise taken.\nThis is where causal inference steps in. Causal inference is the science of drawing cause-and-effect conclusions from data. It allows us to answer questions like:\n\nDid a new marketing campaign actually drive sales, or was it launched during a time when sales naturally increase?\nWill a new app feature increase user engagement, or will it just annoy users?\nIs a chatbot the best way to reduce wait time and increase customer satisfaction?\n\nCausal inference is the missing piece of the data-driven puzzle. It lets us move beyond correlation and identify the true drivers of business outcomes. Impact evaluation builds on this, putting numbers to the effects of a program, policy, or intervention. Think of it as measuring the impact of a specific business decision.\nData can also be used to improve the ongoing operations and effectiveness of a program, a process known as program improvement. This involves continuously collecting data on how the program is running, identifying any bottlenecks or areas for enhancement, and making adjustments as needed. Think of program improvement as an ongoing feedback loop, constantly refining and optimizing the program based on real-world data.\nNow, let’s delve a bit deeper. Imagine you’re a decision-maker at a social media company pondering a new feature. You have data showing that users who engage with the feature spend more time on the platform. This is a correlation, but it doesn’t tell the whole story. What if those users were already naturally the most engaged?\nThis is where the concept of the counterfactual becomes crucial. The counterfactual is what would have happened if we hadn’t implemented the new feature – it’s the potential outcome had we not made the change. While Jerzy Neyman hinted at this idea in 1923 (see Neyman 1923), Donald Rubin fully developed the concept in the 1970s (see Rubin 1974; also Rubin 1978) . Given that we can only observe one potential outcome for each unit, the counterfactual is inherently missing data. Hence, causal inference can be viewed as a missing data problem. For a review of variety of causal inference methods from this perspective see Ding and Li (2018).\nChoosing the right counterfactual is critical for drawing valid causal conclusions. The wrong counterfactual can lead to misleading results and potentially disastrous business decisions. We’ll explore these challenges and different approaches to constructing counterfactuals in the coming chapters.\nBy understanding causal inference and the importance of counterfactuals, you’ll be well on your way to leveraging the true power of data to make informed decisions for your business. However, choosing the wrong counterfactual can have serious consequences. Here are some classic examples:\n\nBefore-and-After Studies: Imagine evaluating a job training program by comparing participants’ income before and after participation. What if the economy was improving during that time, and their income would have increased anyway? A simple before-and-after comparison can’t account for these external factors.\nSelf-Selection Bias: Suppose you want to assess the effect of a new exercise app. You compare those who chose to use the app to those who didn’t. What if people who downloaded the app were already more motivated to exercise? This self-selection bias can skew the results, making the app look more effective than it truly is.\n\nRemember, in order to design a good study to inform decisions, we need to know which decisions we are trying to inform. This clarity about the decision at hand allows us to choose the right counterfactual scenario for comparison. By carefully considering potential outcomes and constructing strong counterfactuals, we can leverage the power of data to make informed choices and drive better business results.\n\n\n\n\n\n\nLearn more\n\n\n\nLi, Ding, and Mealli (2023) Bayesian causal inference: a critical review.\n\n\n\n\n\n\n\n\nDing, Peng, and Fan Li. 2018. “Causal Inference.” Statistical Science 33 (2): 214–37. https://projecteuclid.org/journals/statistical-science/volume-33/issue-2/Causal-Inference-A-Missing-Data-Perspective/10.1214/18-STS645.pdf.\n\n\nLi, Fan, Peng Ding, and Fabrizia Mealli. 2023. “Bayesian Causal Inference: A Critical Review.” Philosophical Transactions of the Royal Society A 381 (2247): 20220153. https://doi.org/10.1098/rsta.2022.0153.\n\n\nNeyman, Jersey. 1923. “Sur Les Applications de La Théorie Des Probabilités Aux Experiences Agricoles: Essai Des Principes.” Roczniki Nauk Rolniczych 10 (1): 1–51.\n\n\nRubin, Donald B. 1974. “Estimating Causal Effects of Treatments in Randomized and Nonrandomized Studies.” Journal of Educational Psychology 66 (5): 688. http://www.fsb.muohio.edu/lij14/420_paper_Rubin74.pdf.\n\n\n———. 1978. “Bayesian Inference for Causal Effects: The Role of Randomization.” The Annals of Statistics, 34–58. https://www.jstor.org/stable/2958688.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What Does it Mean to Be Data-Driven?</span>"
    ]
  },
  {
    "objectID": "potential_outcome.html",
    "href": "potential_outcome.html",
    "title": "2  The Potential Outcomes Framework",
    "section": "",
    "text": "2.1 The Basic Idea\nThe potential outcomes framework, also known as the Rubin Causal Model, provides a formal mathematical approach to defining and estimating causal effects. This framework, developed by Donald Rubin building on work by Jerzy Neyman, is central to modern causal inference and has become increasingly important in business data science. At its core, the potential outcomes framework posits that each unit (e.g., person, company, product) has a set of potential outcomes corresponding to each possible treatment condition. For instance:\nThe causal effect for an individual is then defined as the difference between these potential outcomes: \\(Y(1) - Y(0)\\). However, we face what Holland (1986) termed the “fundamental problem of causal inference” - we can only observe one of these potential outcomes for each unit. If a customer is exposed to the new interface, we observe \\(Y(1)\\) but \\(Y(0)\\) remains unobserved (and vice versa). This makes causal inference inherently a missing data problem, a concept we’ll explore further later in this chapter.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Potential Outcomes Framework</span>"
    ]
  },
  {
    "objectID": "potential_outcome.html#the-basic-idea",
    "href": "potential_outcome.html#the-basic-idea",
    "title": "2  The Potential Outcomes Framework",
    "section": "",
    "text": "A tech company testing a new app interface might consider:\n\n\\(Y(1)\\): user engagement if exposed to the new interface\n\\(Y(0)\\): user engagement if exposed to the old interface\n\nAn e-commerce platform implementing a recommendation system might examine:\n\n\\(Y(1)\\): customer purchase amount with personalized recommendations\n\\(Y(0)\\): customer purchase amount without personalized recommendations\n\nA SaaS company offering a free trial could look at:\n\n\\(Y(1)\\): conversion rate if offered a 30-day free trial\n\\(Y(0)\\): conversion rate if not offered a free trial",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Potential Outcomes Framework</span>"
    ]
  },
  {
    "objectID": "potential_outcome.html#key-concepts-and-estimands",
    "href": "potential_outcome.html#key-concepts-and-estimands",
    "title": "2  The Potential Outcomes Framework",
    "section": "2.2 Key Concepts and Estimands",
    "text": "2.2 Key Concepts and Estimands\nSeveral important concepts and estimands are central to the potential outcomes framework. The Average Treatment Effect (ATE) is the average causal effect across the entire population, defined as \\(E[Y(1) - Y(0)]\\). This gives us an overall measure of the treatment’s impact.\nWhen we’re interested in how the treatment effect varies across different subgroups, we look at the Conditional Average Treatment Effect (CATE). This is defined as \\(E[Y(1) - Y(0) | X]\\), where X represents a specific set of covariates. CATE allows us to understand how the treatment effect might differ for various segments of our population.\nSometimes, we’re particularly interested in the effect on those who actually received the treatment. This is captured by the Average Treatment Effect on the Treated (ATT), defined as \\(E[Y(1) - Y(0) | W = 1]\\), where W is the treatment indicator. In certain scenarios, such as when using instrumental variables, we might focus on the Local Average Treatment Effect (LATE), which represents the average treatment effect for a specific subpopulation of compliers.\nA crucial assumption in many causal analyses is ignorability. This assumes that treatment assignment is independent of the potential outcomes given observed covariates. Mathematically, this can be expressed as: \\((Y(1), Y(0)) ⊥ W | X\\) where \\(W\\) is the treatment assignment and \\(X\\) are the observed covariates. For instance, in our e-commerce recommendation system example, ignorability would mean that whether a customer sees personalized recommendations (W) is independent of how much they would potentially purchase with or without recommendations (\\(Y(1), Y(0)\\)), once we account for observed factors like browsing history, past purchases, etc. (\\(X\\)).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Potential Outcomes Framework</span>"
    ]
  },
  {
    "objectID": "potential_outcome.html#experimental-vs-observational-studies",
    "href": "potential_outcome.html#experimental-vs-observational-studies",
    "title": "2  The Potential Outcomes Framework",
    "section": "2.3 Experimental vs Observational Studies",
    "text": "2.3 Experimental vs Observational Studies\nThe potential outcomes framework can be applied to both experimental and observational studies, each with its own strengths and challenges:\nExperimental Studies: In randomized controlled trials, treatment assignment is controlled by the researcher. This control ensures that the ignorability assumption holds by design. For example, when A/B testing a new website design, the randomization of which users see which version ensures that potential outcomes are independent of the assignment. This makes causal inference more straightforward but may not always be feasible in business settings due to ethical, practical, or cost constraints.\nObservational Studies: These are more common in business contexts but present more challenges. For instance, if we want to study the effect of a loyalty program on customer retention, customers typically choose whether to join the program rather than being randomly assigned. In these cases, we need to carefully consider and account for potential confounders to approximate the conditions of an experiment. This often involves sophisticated statistical techniques to adjust for differences between the treatment and control groups, such as propensity score matching or inverse probability weighting.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Potential Outcomes Framework</span>"
    ]
  },
  {
    "objectID": "potential_outcome.html#heterogeneous-treatment-effects",
    "href": "potential_outcome.html#heterogeneous-treatment-effects",
    "title": "2  The Potential Outcomes Framework",
    "section": "2.4 Heterogeneous Treatment Effects",
    "text": "2.4 Heterogeneous Treatment Effects\nIn business applications, it’s crucial to consider that the effect of an intervention might vary across different subgroups of customers or products. This heterogeneity can be masked when looking only at average effects. For example:\n\nA new marketing strategy might have a positive effect on one customer segment but a negative effect on another.\nA product feature might significantly boost engagement for power users but have minimal impact on casual users.\n\nUnderstanding these heterogeneous effects can lead to more targeted and effective business strategies, such as personalized marketing campaigns or tailored product features.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Potential Outcomes Framework</span>"
    ]
  },
  {
    "objectID": "potential_outcome.html#selection-bias",
    "href": "potential_outcome.html#selection-bias",
    "title": "2  The Potential Outcomes Framework",
    "section": "2.5 Selection Bias",
    "text": "2.5 Selection Bias\nSelection bias occurs when the individuals who select into the treatment group differ systematically from those who do not. In business contexts, this is a common issue. For example:\n\nCustomers who choose to use a new feature might be systematically different from those who don’t, making it challenging to isolate the true effect of the feature on outcomes like engagement or sales.\nEarly adopters of a product might have different characteristics and behaviors compared to later adopters, potentially skewing our understanding of the product’s impact.\n\nRecognizing and addressing selection bias is crucial for making valid causal inferences in business settings.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Potential Outcomes Framework</span>"
    ]
  },
  {
    "objectID": "potential_outcome.html#connections-to-missing-data",
    "href": "potential_outcome.html#connections-to-missing-data",
    "title": "2  The Potential Outcomes Framework",
    "section": "2.6 Connections to Missing Data",
    "text": "2.6 Connections to Missing Data\nThe link between causal inference and missing data is profound. In the potential outcomes framework, we’re always missing at least one potential outcome for each unit. This is similar to the problem of missing data in surveys or experiments where some values are unobserved.\nMethods developed for handling missing data have direct analogues in causal inference. For example, multiple imputation techniques can be adapted to impute missing potential outcomes. Inverse probability weighting, commonly used in missing data problems, is analogous to propensity score weighting in causal inference.\nThe assumptions underlying missing data methods also have parallels in causal inference. The assumption of “Missing At Random” (MAR) in missing data literature is similar to the ignorability assumption in causal inference. Both assume that the missingness (or treatment assignment) is independent of the unobserved data, given the observed data.\nUnderstanding these connections can provide valuable insights and tools for addressing the inherent missing data problem in causal inference. By leveraging techniques from both fields, researchers can develop more robust methods for estimating causal effects in a variety of real-world scenarios.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Potential Outcomes Framework</span>"
    ]
  },
  {
    "objectID": "potential_outcome.html#conclusion",
    "href": "potential_outcome.html#conclusion",
    "title": "2  The Potential Outcomes Framework",
    "section": "2.7 Conclusion",
    "text": "2.7 Conclusion\nThe potential outcomes framework provides a powerful tool for business data scientists to approach causal questions rigorously. By understanding the fundamental concepts, key estimands, and challenges associated with this framework, data scientists can make more informed decisions about experimental design, analysis methods, and interpretation of results. As we delve deeper into specific techniques and applications in the following chapters, keep these foundational ideas in mind – they will serve as the bedrock for more advanced causal inference methods in business contexts.\n\n\n\n\n\n\nLearn more\n\n\n\nCunningham (2021) Causal Inference: The Mixtape. Potential Outcomes Causal Model\n\n\n\n\n\n\n\n\nCunningham, Scott. 2021. “Potential Outcomes Causal Model.” In Causal Inference: The Mixtape. Yale University Press. https://mixtape.scunning.com/04-potential_outcomes.\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.” Journal of the American Statistical Association 81 (396): 945–60. https://doi.org/10.2307/2289064.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Potential Outcomes Framework</span>"
    ]
  },
  {
    "objectID": "decisions_first.html",
    "href": "decisions_first.html",
    "title": "3  A Decisions First Framework",
    "section": "",
    "text": "3.1 Define the Decision(s): The Cornerstone of Data-Driven Choices\nThe first, and arguably most crucial, step is to clearly articulate the decision(s) you need to make. This might involve launching a new product, adjusting pricing strategies, or optimizing marketing campaigns. Some decisions will be binary, but that’s not always the case. The key is that the optimal decision should hinge on the information available. If no amount of evidence will change your mind, there’s little point in designing a study to collect it.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Decisions First Framework</span>"
    ]
  },
  {
    "objectID": "decisions_first.html#formulate-data-driven-questions-asking-the-right-questions",
    "href": "decisions_first.html#formulate-data-driven-questions-asking-the-right-questions",
    "title": "3  A Decisions First Framework",
    "section": "3.2 Formulate Data-Driven Questions: Asking the Right Questions",
    "text": "3.2 Formulate Data-Driven Questions: Asking the Right Questions\nWith your decision clearly defined, it’s time to craft questions that data can answer and that directly inform your decision. These questions should be focused, actionable, and revolve around meaningful thresholds, rather than fixating solely on the null hypothesis (zero effect).\nFor instance, in a scenario involving the implementation of a chatbot, you might ask:\n\n“Will a chatbot reduce average customer wait time by at least 15%?” Here, the threshold of interest isn’t whether there’s any reduction in wait time, but whether the reduction is substantial enough (15% or more) to justify implementing the chatbot.\n“Will a chatbot increase customer satisfaction scores by at least 10 points?” Similarly, the focus is on a meaningful increase in satisfaction, not just any statistically significant difference from the current baseline.\n\nBy establishing these thresholds, we align our data analysis with the real-world impact of our decisions. A 5% reduction in wait times, even if statistically significant, might not justify the cost of implementing a chatbot.\nIn many cases, a well-structured question can be surprisingly simple, often following the format “Does A do B among C compared to D?”\n\nA: The intervention or action under evaluation (e.g., chatbot, new pricing).\nB: Your clear definition of success (e.g., reduce wait times, increase sales).\nC: The target population (e.g., all customers, a specific segment).\nD: The alternative or baseline for comparison (e.g., no chatbot, current pricing).\n\nHowever, sometimes you’ll encounter more nuanced questions like “What works for whom?” These situations involve evaluating multiple alternatives with the understanding that different options might be optimal for different groups within your population.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Decisions First Framework</span>"
    ]
  },
  {
    "objectID": "decisions_first.html#design-the-study-tailoring-research-to-your-decision",
    "href": "decisions_first.html#design-the-study-tailoring-research-to-your-decision",
    "title": "3  A Decisions First Framework",
    "section": "3.3 Design the Study: Tailoring Research to Your Decision",
    "text": "3.3 Design the Study: Tailoring Research to Your Decision\nThis stage involves selecting the appropriate research methodology to answer your questions. Crucially, the study design must be tailored to the specific decision you’re facing. Factors to consider include data availability, experimental design, and potential biases. Additionally, ethical considerations should be at the forefront of your design, ensuring the study does no harm and respects the rights of participants.\n\nChatbot Example: If you’re exploring whether to offer a chatbot as an option, an A/B test where some customers are offered the chatbot while others follow the standard process might be suitable. However, if you’re considering making the chatbot the only option, your study design needs to reflect this forced-choice scenario.\nEvent Invitation Example: If you want to understand the value of inviting people to an event, randomizing invitations and analyzing attendance rates is a valid approach. But if you want to understand the value of actually attending the event, you’d focus on outcomes for attendees, even if the data comes from the same experiment.\n\nThe key is to ensure your study design mirrors the real-world conditions of the decision as closely as possible.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Decisions First Framework</span>"
    ]
  },
  {
    "objectID": "decisions_first.html#present-findings-and-implications-communicating-clearly-and-transparently",
    "href": "decisions_first.html#present-findings-and-implications-communicating-clearly-and-transparently",
    "title": "3  A Decisions First Framework",
    "section": "3.4 Present Findings and Implications: Communicating Clearly and Transparently",
    "text": "3.4 Present Findings and Implications: Communicating Clearly and Transparently\nAfter conducting your study, present the results clearly, concisely, and accessibly. Avoid jargon that could confuse your audience. Even a meticulously designed study can lead to misinformed decisions if the findings are poorly communicated. Be transparent about your learnings, acknowledge any limitations of the study, and highlight new questions that have arisen.\nIn discussing limitations, it’s crucial to distinguish between Internal Validity (the confidence that the observed effects are due to the factor you’re studying) and External Validity (the extent to which the results can be generalized to other situations). Even with a flawless experimental design, questions of external validity might remain. For example, a chatbot study conducted with tech-savvy users might not apply to an older demographic.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Decisions First Framework</span>"
    ]
  },
  {
    "objectID": "decisions_first.html#real-world-constraints-and-the-path-forward",
    "href": "decisions_first.html#real-world-constraints-and-the-path-forward",
    "title": "3  A Decisions First Framework",
    "section": "3.5 Real-World Constraints and the Path Forward",
    "text": "3.5 Real-World Constraints and the Path Forward\nReal-world constraints often prevent us from conducting the “perfect” study with both impeccable internal and external validity. Therefore, it’s essential to view evidence quality as a spectrum, not a binary. Learning is an ongoing process. Embrace uncertainty, acknowledging that no single study provides all the answers. Instead of thinking in terms of “success” or “failure,” consider the weight of evidence, the specific context, and the potential risks and rewards when making decisions.\nBy adopting the “Decisions First” framework and embracing a Bayesian approach, you can transform data analysis from a ritualistic exercise into a powerful tool for making informed, impactful decisions. This approach not only aligns your research with your business objectives but also acknowledges the complexities and uncertainties inherent in real-world decision-making.\n\n\n\n\n\n\nGigerenzer, Gerd, Stefan Krauss, and Oliver Vitouch. 2004. “The Null Ritual.” The Sage Handbook of Quantitative Methodology for the Social Sciences, 391–408.\n\n\nManski, Charles F. 2020. “The Lure of Incredible Certitude.” Economics & Philosophy 36 (2): 216–45. https://www.nber.org/system/files/working_papers/w24905/w24905.pdf.\n\n\nWasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA Statement on p-Values: Context, Process, and Purpose.” The American Statistician. Taylor & Francis. https://www.tandfonline.com/doi/pdf/10.1080/00031305.2016.1154108.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Decisions First Framework</span>"
    ]
  },
  {
    "objectID": "rce.html",
    "href": "rce.html",
    "title": "4  Rapid-Cycle Evaluation for Program Improvement",
    "section": "",
    "text": "4.1 What is Rapid-Cycle Evaluation?\nIn today’s fast-paced business environment, the ability to adapt and improve is crucial. Rapid-cycle evaluation (RCE) is a powerful approach that leverages data and state-of-the-art research methods to support continuous improvement. By embedding feedback and data into decision-making processes, RCE allows for real-time adjustments and enhancements to programs and initiatives.\nRCE is not just about evaluation; it’s about empowering programs to reach their full potential. This approach, while originating in the public policy sphere, builds on a long history of using data for continuous quality improvement, such as A/B testing, and is readily applicable to the private sector.\nRCE is a flexible framework that can be tailored to specific program needs and contexts. It may involve a mix of quantitative and qualitative methods, including:\nThe key is to gather information quickly and efficiently, enabling program implementers to make informed decisions without delay.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Rapid-Cycle Evaluation for Program Improvement</span>"
    ]
  },
  {
    "objectID": "rce.html#what-is-rapid-cycle-evaluation",
    "href": "rce.html#what-is-rapid-cycle-evaluation",
    "title": "4  Rapid-Cycle Evaluation for Program Improvement",
    "section": "",
    "text": "Rapid-Cycle Evaluation Process\n\n\n\n\nExperimental and quasi-experimental designs\nOutcome measurement and large sample surveys\nIn-depth qualitative research",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Rapid-Cycle Evaluation for Program Improvement</span>"
    ]
  },
  {
    "objectID": "rce.html#why-use-rapid-cycle-evaluation",
    "href": "rce.html#why-use-rapid-cycle-evaluation",
    "title": "4  Rapid-Cycle Evaluation for Program Improvement",
    "section": "4.2 Why Use Rapid-Cycle Evaluation?",
    "text": "4.2 Why Use Rapid-Cycle Evaluation?\n\nEarly Optimization: RCE can be used in the early stages of a program to test different designs and activities, ensuring the program is on the right track from the outset.\nContinuous Improvement: During implementation, RCE helps identify and address bottlenecks or challenges as they arise.\nInnovation in Established Programs: Even for established programs, RCE is invaluable for testing new ideas and refining existing strategies.\nPreparation for Impact Evaluation: By using RCE for program improvement, we set the stage for measuring impacts of fully designed programs, allowing for more confident investment decisions based on impact evaluation data.\n\n\n\n\n\n\n\nWarning\n\n\n\nJumping directly to measuring the impact of a new initiative when it’s just a minimum viable product can lead to premature program cuts. RCE allows for optimization before full-scale impact evaluation.\n\n\n\n\n\n\n\n\nLearn more\n\n\n\nBagby and Rangarajan (2023) Using Rapid-Cycle Evaluation to Improve Program Design and Delivery.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Rapid-Cycle Evaluation for Program Improvement</span>"
    ]
  },
  {
    "objectID": "rce.html#integrating-behavioral-economics-with-rce",
    "href": "rce.html#integrating-behavioral-economics-with-rce",
    "title": "4  Rapid-Cycle Evaluation for Program Improvement",
    "section": "4.3 Integrating Behavioral Economics with RCE",
    "text": "4.3 Integrating Behavioral Economics with RCE\nBehavioral economics bridges psychology and economics, revealing how people actually make decisions, often deviating from classical economic models of perfect rationality. Integrating behavioral economics insights with RCE can significantly enhance program design and effectiveness.\n\nKey Concepts in Behavioral Economics\n\nHeuristics and Biases: Mental shortcuts that simplify decision-making but can introduce biases. Understanding these can help guide user decisions strategically.\nFraming: The way information is presented can significantly impact choices. Positive framing (emphasizing gains) or negative framing (highlighting losses) can be powerful tools depending on the context.\nProspect Theory: People value gains and losses differently, with losses typically having a greater emotional impact (loss aversion). This insight can be applied to marketing strategies and program design.\nNudging: Subtly guiding choices without restricting options, as popularized by Thaler and Sunstein (2009). This involves leveraging behavioral insights to influence decision-making through minor adjustments in “choice architecture.”\n\n\n\n\n\n\n\nKey Takeaway\n\n\n\nCombining RCE with behavioral economics offers a powerful toolkit for optimizing programs, products, and services. By understanding how people actually make decisions, we can design interventions that are more impactful and aligned with real-world behavior.\n\n\n\n\n\n\n\n\nLearn more\n\n\n\nThaler and Sunstein (2021) Nudge: The final edition. Yale University Press.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Rapid-Cycle Evaluation for Program Improvement</span>"
    ]
  },
  {
    "objectID": "rce.html#conclusion",
    "href": "rce.html#conclusion",
    "title": "4  Rapid-Cycle Evaluation for Program Improvement",
    "section": "4.4 Conclusion",
    "text": "4.4 Conclusion\nBy embracing RCE and incorporating behavioral economics insights, organizations can move beyond incrementalism and create programs that are not only impactful but also adaptable to changing circumstances. This approach fosters a culture of continuous learning and improvement, ensuring that programs and initiatives are constantly evolving to meet their full potential.\n\n\n\n\n\n\nBagby, Emilie, and Anu Rangarajan. 2023. Using Rapid-Cycle Evaluation to Improve Program Design and Delivery. Oxford University Press. https://doi.org/10.1093/oxfordhb/9780190059668.013.7.\n\n\nThaler, Richard H, and Cass R Sunstein. 2009. Nudge: Improving Decisions about Health, Wealth, and Happiness. Penguin.\n\n\n———. 2021. Nudge: The Final Edition. Yale University Press.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Rapid-Cycle Evaluation for Program Improvement</span>"
    ]
  },
  {
    "objectID": "surrogates.html",
    "href": "surrogates.html",
    "title": "5  Surrogates",
    "section": "",
    "text": "5.1 What are Surrogates?\nA surrogate, in essence, is a stand-in for a long-term outcome that is difficult or time-consuming to measure directly. For example, instead of waiting to see how a new feature impacts annual revenue, we might look at its effect on daily active users or click-through rates. By understanding the relationship between these short-term surrogates and the long-term outcome, we can gain insights into how our actions are likely to impact the metrics we truly care about.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Surrogates</span>"
    ]
  },
  {
    "objectID": "surrogates.html#what-are-surrogates",
    "href": "surrogates.html#what-are-surrogates",
    "title": "5  Surrogates",
    "section": "",
    "text": "Importance of Surrogates\nIn many cases, a single surrogate may not fully capture the complexity of the long-term outcome. This is where the concept of a surrogate index comes in. By combining multiple surrogates into a single metric, we can create a more comprehensive and nuanced picture of the impact of our decisions. For example, we might combine metrics like user engagement, satisfaction ratings, and retention rates into a single index that reflects the overall health of our product.\n\n\nValidity of Surrogate Studies\nTo ensure the validity of a surrogate study, two key ingredients are essential:\n\nValidity of Surrogates: The surrogates must be valid, meaning that the policy or decision we are interested in truly affects the ultimate outcome through the chosen surrogates.\nRobust Identification: We need to robustly identify the policy’s effects on the surrogates. This is often achieved through randomized experiments, but other methods can also be used.\n\nThe tech sector is awash in data, offering a wide range of potential surrogates. These might include metrics like website traffic, click-through rates, user engagement, social media mentions, and app downloads. The specific surrogates to choose will depend on the nature of the decision being studied and the available data.\n\n\nChoosing the Right Surrogates\nSelecting appropriate surrogates requires a deep understanding of the domain and the long-term outcomes of interest. It’s crucial to choose surrogates that are closely linked to these outcomes. For example:\n\nE-commerce: For an online store, short-term surrogates might include cart abandonment rates, average order value, and customer reviews.\nHealthcare: In a clinical trial, blood pressure and cholesterol levels can be surrogates for long-term cardiovascular health.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Surrogates</span>"
    ]
  },
  {
    "objectID": "surrogates.html#estimating-effects-in-practice",
    "href": "surrogates.html#estimating-effects-in-practice",
    "title": "5  Surrogates",
    "section": "5.2 Estimating Effects in Practice",
    "text": "5.2 Estimating Effects in Practice\nThere are several ways to estimate long-term effects using a surrogate index. The simplest approach is to use an experimental setup, where we know the probability of a unit being assigned to a particular group. In this case, we can estimate the surrogate index and then use it to impute the long-term outcome.\n\nMethods for Estimating Surrogacy\nThe method used to estimate the surrogacy index is flexible, as long as it captures the average effect of the index on the long-term outcome. In scenarios with many potential surrogates, techniques like LASSO (Least Absolute Shrinkage and Selection Operator) can be used to identify the most relevant ones. Other machine learning techniques such as Random Forests or Gradient Boosting can also be employed to handle complex, high-dimensional data.\n\n\nPractical Example\nTo illustrate the application of surrogate indices, consider a tech company running an A/B test to evaluate a new feature. The company might track several metrics during the experiment, such as user engagement, time spent on the platform, and frequency of feature usage. By combining these metrics into a surrogate index, the company can predict the feature’s impact on long-term user retention and revenue growth.\n\nExample with Code\n\n# TODO WRITE EXAMPLE!\n\n\n\n\nConclusion\nSurrogates are powerful tools that can help bridge the gap between short-term metrics and long-term outcomes. By carefully selecting and validating surrogates, and employing robust estimation methods, we can make more informed decisions and predict the long-term impact of our actions with greater precision.\n\n\n\n\n\n\nLearn more\n\n\n\n\nAthey et al. (2019) The surrogate index: Combining short-term proxies to estimate long-term treatment effects more rapidly and precisely.\nImbens et al. (2022) Long-term causal inference under persistent confounding via data combination.\nChen and Ritzwoller (2023) Semiparametric estimation of long-term treatment effects.\nZhang et al. (2023) Evaluating the Surrogate Index as a Decision-Making Tool Using 200 A/B Tests at Netflix.\n\n\n\n\n\n\n\n\n\nAthey, Susan, Raj Chetty, Guido W Imbens, and Hyunseung Kang. 2019. “The Surrogate Index: Combining Short-Term Proxies to Estimate Long-Term Treatment Effects More Rapidly and Precisely.” National Bureau of Economic Research.\n\n\nChen, Jiafeng, and David M Ritzwoller. 2023. “Semiparametric Estimation of Long-Term Treatment Effects.” Journal of Econometrics 237 (2): 105545.\n\n\nImbens, Guido, Nathan Kallus, Xiaojie Mao, and Yuhao Wang. 2022. “Long-Term Causal Inference Under Persistent Confounding via Data Combination.” arXiv Preprint arXiv:2202.07234.\n\n\nZhang, Vickie, Michael Zhao, Anh Le, and Nathan Kallus. 2023. “Evaluating the Surrogate Index as a Decision-Making Tool Using 200 a/b Tests at Netflix.” arXiv Preprint arXiv:2311.11922.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Surrogates</span>"
    ]
  },
  {
    "objectID": "rct.html",
    "href": "rct.html",
    "title": "The Gold Standard",
    "section": "",
    "text": "Randomized controlled trials (RCTs) have a rich history, dating back to the 1920s when researchers used them to compare crop yields under different conditions. In medicine, RCTs gained prominence in the mid-20th century, thanks to pioneers like Austin Bradford Hill, who demonstrated their power in evaluating the effectiveness of treatments like streptomycin for tuberculosis. Today, RCTs are considered the gold standard for establishing causality in many fields, from healthcare to social policy.\nIn the tech sector, a simplified version of the RCT – the A/B test – has become ubiquitous. Companies like Google, Amazon, and Facebook routinely run A/B tests to evaluate new features, website designs, and marketing campaigns. The allure of A/B testing lies in its simplicity: randomly assign users to different groups, expose them to different versions of a product or experience, and measure the outcomes. This allows for a clean comparison, isolating the effect of the change from other factors that might influence user behavior.",
    "crumbs": [
      "The Gold Standard"
    ]
  },
  {
    "objectID": "rct_basic.html",
    "href": "rct_basic.html",
    "title": "6  The Power of Randomization",
    "section": "",
    "text": "The Importance of SUTVA: The Cornerstone of Valid Inference\nHowever, it’s crucial to remember that the success of both RCTs and A/B tests hinges on a fundamental assumption: the Stable Unit Treatment Value Assumption (SUTVA). SUTVA has two main components:\nIn simpler terms, SUTVA ensures that the effect of the treatment is solely due to the treatment itself and not influenced by other factors or interactions between units.",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Power of Randomization</span>"
    ]
  },
  {
    "objectID": "rct_basic.html#stratified-randomization",
    "href": "rct_basic.html#stratified-randomization",
    "title": "6  The Power of Randomization",
    "section": "6.1 Stratified Randomization",
    "text": "6.1 Stratified Randomization\nWhile random assignment is effective, unforeseen factors can sometimes lead to imbalanced groups. Stratified randomization addresses this by dividing users into subgroups (strata) based on relevant characteristics that are believed to influence the outcome metric. Randomization is then performed within each stratum, ensuring that both treatment and control groups have a similar proportion of users from each subgroup.\nThis approach strengthens experiments by creating balanced groups. For instance, if user location is expected to affect the outcome metric, users can be stratified by location (e.g., urban vs. rural), followed by randomization within each location. This ensures a similar distribution of user attributes across treatment and control groups, controlling for confounding factors—user traits that impact both exposure to the new feature and the desired outcome. With balanced groups, any observed differences in the outcome metric are more likely due to the new feature itself, leading to more precise and reliable results.\n\nExamples\n\nTargeting Mobile App Engagement: In an RCT to evaluate a new in-app notification, user location (urban vs. rural) is suspected to influence user response. Stratification by location, followed by randomization within each stratum, can control for this factor.\nPersonalizing a Recommendation Engine: When A/B testing a revamped recommendation engine, past purchase history is hypothesized to influence user response. Stratification by purchase history categories (e.g., frequent buyers of clothing vs. electronics), followed by randomization within each category, can account for this.\n\n\n\nAdvantages:\n\nReduced bias: Stratification helps isolate the true effect of the new feature by controlling for the influence of confounding factors. This leads to more reliable conclusions about the feature’s impact on user behavior.\nImproved decision-making: By pinpointing the feature’s effect on specific user groups (e.g., urban vs. rural in the notification example), stratified randomization can inform decisions about targeted rollouts or further iterations based on subgroup performance.\n\n\n\nDisadvantages:\n\nIncreased complexity: Designing and implementing stratified randomization requires careful planning to choose the right stratification factors and ensure enough users within each stratum for valid analysis.\nNeed for larger sample sizes: Maintaining balance across strata might necessitate a larger overall sample size compared to simple random assignment.\n\n\n\nExample with code\nLet’s illustrate the concept of stratified randomization with a practical example. Consider a scenario where we have data on 10,000 individuals, each described by two continuous variables (x1 and x2) and two categorical variables (x3 and x4). We suspect that variable x3 might be a confounding factor influencing the outcome of our experiment.\n\nmy_data &lt;- tibble::tibble(\n  x1 = rnorm(10000), \n  x2 = rnorm(10000),\n  x3 = rep(c(\"A\", \"B\"), 5000),\n  x4 = rep(c(\"C\", \"D\"), 5000)\n)\n\n# Create a Randomizer Object\nrandomized &lt;- im::randomizer$new(\n  data = my_data, \n  seed = 12345, \n  max_attempts = 1000,\n  variables = c(\"x1\", \"x2\"), \n  standard = \"Not Concerned\",\n  group_by = \"x3\"\n)\n\n# Generate Balance Plot\nrandomized$balance_plot\n\n\n\n\n\n\n\n\nIn this code, we’re using the im::randomizer function to create an object that will help us perform stratified randomization. We specify x3 as the variable to stratify by, ensuring that the treatment and control groups have a balanced distribution of individuals from both categories of x3.\nBy incorporating stratified randomization into our experimental design, we can effectively control for the influence of variable x3, enhancing the internal validity of our study and allowing for more precise estimates of causal effects.\nIn conclusion, stratified randomization offers a powerful way to enhance the rigor and precision of experiments, particularly when dealing with potential confounding factors. While it may introduce some additional complexity and potentially require larger sample sizes, the benefits in terms of internal validity and the ability to draw more nuanced conclusions often outweigh these drawbacks. The thoughtful use of stratified randomization can be a valuable asset in the causal inference toolkit.\n\n\n\n\n\n\nChernozhukov, Victor, Christian Hansen, Nathan Kallus, Martin Spindler, and Vasilis Syrgkanis. 2024. “Applied Causal Inference Powered by ML and AI” 12 (1): 338. https://causalml-book.org/assets/chapters/CausalML_chap_2.pdf.\n\n\nWainer, Howard. 2007. “The Most Dangerous Equation.” American Scientist 95 (3): 249.\n\n\nWWC. 2020. “What Works Clearinghouse Baseline Equivalence Standard.” U.S. Department of Education, Institute of Education Sciences, National Center for Education Evaluation; Regional Assistance. https://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/WWC-Baseline-Brief-v6_508.pdf.",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Power of Randomization</span>"
    ]
  },
  {
    "objectID": "bad.html",
    "href": "bad.html",
    "title": "7  Bayesian Adaptive Design",
    "section": "",
    "text": "7.1 The Idea\nIn the realm of randomized trials, the concept of “adaptive design” introduces a dynamic element. Unlike traditional designs where the course of the experiment is fixed from the outset, adaptive designs allow for modifications during the trial based on the accumulating data. This flexibility can be harnessed to enhance the efficiency and effectiveness of the experiment.\nOne particularly powerful approach to adaptive design is the Bayesian adaptive design. This method leverages Bayesian statistics, which allows for incorporating prior knowledge and the continuous updating of beliefs as new data become available. In the context of randomized trials, this means that the allocation of participants to different treatment arms can be adjusted in real time based on the observed outcomes.\nFor instance, if early data suggest that a particular treatment arm is showing promising results, the Bayesian adaptive design might allocate more participants to that arm, increasing our ability to distinguish signal from noise. Conversely, if a treatment arm appears to be ineffective or even harmful, the design might reduce or even stop the allocation of participants to that arm, thus protecting them from unnecessary exposure.",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bayesian Adaptive Design</span>"
    ]
  },
  {
    "objectID": "bad.html#the-idea",
    "href": "bad.html#the-idea",
    "title": "7  Bayesian Adaptive Design",
    "section": "",
    "text": "Advantages:\n\nIncreased Efficiency: By focusing resources on the most promising treatment arms, Bayesian adaptive designs can potentially reduce the sample size needed to detect a significant effect, saving time and costs.\nEthical Considerations: The ability to adapt the trial based on emerging data can help protect participants from ineffective or harmful treatments.\nImproved Decision-Making: The continuous updating of beliefs based on real-time data can lead to more informed decisions about the allocation of resources and the selection of the most effective interventions.\n\n\n\nChallenges:\n\nComplexity: Designing and implementing Bayesian adaptive designs can be more complex than traditional fixed designs, requiring expertise in Bayesian statistics and careful planning.\nStatistical Considerations: The adaptive nature of these designs can introduce statistical challenges, such as the need to adjust for multiple comparisons and the potential for bias if the adaptation process is not carefully controlled.\n\n\n\n\n\n\n\nLearn more\n\n\n\nFinucane, Martinez, and Cody (2018) What works for whom? A Bayesian approach to channeling big data streams for public program evaluation.\n\n\n\n\nExample with code\nTODO\n\n\n\n\n\n\nFinucane, Mariel McKenzie, Ignacio Martinez, and Scott Cody. 2018. “What Works for Whom? A Bayesian Approach to Channeling Big Data Streams for Public Program Evaluation.” American Journal of Evaluation 39 (1): 109–22. https://journals.sagepub.com/doi/abs/10.1177/1098214017737173.",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bayesian Adaptive Design</span>"
    ]
  },
  {
    "objectID": "factorial.html",
    "href": "factorial.html",
    "title": "8  Bayesian Factorial Design",
    "section": "",
    "text": "In many scenarios, your goal is to use data to design the optimal treatment. This is where factorial designs come into play. A factorial design is an experimental setup where multiple factors are manipulated or varied simultaneously. Each factor can have multiple levels, and the experiment involves testing all possible combinations of these factor levels. This allows us to not only assess the main effects of each factor but also to investigate the interactions between factors.\nBayesian analysis offers a powerful approach to factorial designs, especially when dealing with complex experiments with many factors and levels. By incorporating prior knowledge and using hierarchical models, Bayesian methods can improve the precision of estimates and control the risk of false positives from multiple comparisons.\nIn a Bayesian factorial design, we start with prior distributions for the effects of each factor level and the interactions between them. These priors can be based on previous research, expert opinion, or simply reflect our uncertainty about the effects. As we collect data from the experiment, we update these priors using Bayes’ theorem, resulting in posterior distributions that reflect our updated beliefs about the effects.\nOne of the key advantages of Bayesian factorial designs is the ability to “borrow strength” across different factor levels and interactions. This means that if the data for one factor level are limited, the model can use information from other factor levels to improve the estimate for that level. This is particularly useful in complex experiments where some factor combinations might have smaller sample sizes.\n\n\n\n\n\n\nLearn more\n\n\n\nKassler, Nichols-Barrer, and Finucane (2018) Beyond “Treatment versus Control”: How Bayesian Analysis Makes Factorial Experiments Feasible in Education Research.\n\n\n\nExample with code\nTODO\n\n\n\n\n\n\nKassler, Daniel, Ira Nichols-Barrer, and Mariel Finucane. 2018. “Beyond ‘Treatment Versus Control’: How Bayesian Analysis Makes Factorial Experiments Feasible in Education Research.” https://doi.org/10.1177/0193841X18818903.",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Factorial Design</span>"
    ]
  },
  {
    "objectID": "iv.html",
    "href": "iv.html",
    "title": "9  Randomized Encouragement Design",
    "section": "",
    "text": "9.1 Understanding REDs: Compliance and Potential Outcomes\nREDs rely on understanding how users respond to encouragement:\nThe potential outcomes framework is crucial for understanding these designs. For each participant, we consider:",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Randomized Encouragement Design</span>"
    ]
  },
  {
    "objectID": "iv.html#understanding-reds-compliance-and-potential-outcomes",
    "href": "iv.html#understanding-reds-compliance-and-potential-outcomes",
    "title": "9  Randomized Encouragement Design",
    "section": "",
    "text": "Compliers: Change their behavior based on the encouragement.\nAlways-takers: Would take the action regardless of encouragement.\nNever-takers: Wouldn’t take the action regardless of encouragement.\nDefiers: (Rarely assumed to exist) Do the opposite of what’s encouraged.\n\n\n\n\\(Y(1)\\): The outcome if encouraged\n\\(Y(0)\\): The outcome if not encouraged\n\\(D(1)\\): The treatment status if encouraged\n\\(D(0)\\): The treatment status if not encouraged",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Randomized Encouragement Design</span>"
    ]
  },
  {
    "objectID": "iv.html#what-we-want-to-know",
    "href": "iv.html#what-we-want-to-know",
    "title": "9  Randomized Encouragement Design",
    "section": "9.2 What We Want to Know",
    "text": "9.2 What We Want to Know\nIn the world of REDs, our curiosity often leads us down two paths. Sometimes, we’re primarily interested in the value of the encouragement itself—the nudge, the prompt, the incentive. In this case, we estimate the Intent-to-Treat (ITT) effect: the average change in the outcome simply due to being encouraged, regardless of whether individuals actually comply and take the action. Mathematically, this is represented as:\n\\[ITT = E[Y(1) - Y(0)]\\]\nHowever, in other scenarios, our focus shifts to the true causal impact of the action itself. We want to know what happens when people actually take that step, change that behavior, or adopt that product. This is where the Complier Average Causal Effect (CACE), also known as the Local Average Treatment Effect (LATE), comes in. It isolates the average effect of the treatment specifically for those who are swayed by the encouragement—the compliers. The CACE is calculated as:\n\\[CACE = \\frac{E[Y|Z=1] - E[Y|Z=0]}{E[D|Z=1] - E[D|Z=0]}\\]\nwhere \\(Z\\) is the encouragement assignment and \\(D\\) is the actual treatment received.",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Randomized Encouragement Design</span>"
    ]
  },
  {
    "objectID": "iv.html#making-it-work-key-assumptions",
    "href": "iv.html#making-it-work-key-assumptions",
    "title": "9  Randomized Encouragement Design",
    "section": "9.3 Making it Work: Key Assumptions",
    "text": "9.3 Making it Work: Key Assumptions\nFor REDs to yield valid causal conclusions, a few things need to hold true:\n\nIgnorability of the instrument: The encouragement assignment is random and independent of potential outcomes.\nExclusion restriction: Encouragement affects the outcome only through its effect on treatment take-up.\nMonotonicity: There are no defiers (i.e., no one does the opposite of what they’re encouraged to do).\nRelevance: The encouragement actually affects treatment take-up (i.e., there are compliers).\n\nWhich path we choose—ITT or CACE—depends entirely on the business decisions we’re trying to inform. Are we evaluating the effectiveness of our marketing campaigns, where the encouragement itself is the key lever? Or are we trying to understand the inherent value of a product feature, where the actual usage is what matters most? REDs give us the flexibility to answer both questions, guiding our decisions with robust causal insights.",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Randomized Encouragement Design</span>"
    ]
  },
  {
    "objectID": "iv.html#examples-with-synthetic-data",
    "href": "iv.html#examples-with-synthetic-data",
    "title": "9  Randomized Encouragement Design",
    "section": "9.4 Examples with synthetic data",
    "text": "9.4 Examples with synthetic data",
    "crumbs": [
      "The Gold Standard",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Randomized Encouragement Design</span>"
    ]
  },
  {
    "objectID": "other_methods.html",
    "href": "other_methods.html",
    "title": "Non Experimental Methods",
    "section": "",
    "text": "In an ideal world, every causal question in business and technology could be answered through carefully designed randomized controlled trials (RCTs). However, the reality of decision-making in these fields often precludes such luxury. Budget constraints, ethical considerations, logistical challenges, or simply the rapid pace of technological change frequently render experimental approaches impractical or impossible. This is where non-experimental methods come into play, offering powerful tools to infer causality from observational data.\nThis section delves into a suite of sophisticated techniques designed to approximate experimental conditions using data that wasn’t generated through randomized experiments. Each of these methods comes with its own set of assumptions, strengths, and limitations. We’ll discuss not only how to implement these techniques but also how to critically evaluate their applicability to your specific business context.\nRemember, while these methods can be incredibly useful, they are not magical solutions. The key to their successful application lies in a deep understanding of the underlying causal mechanisms at play in your business scenario, careful consideration of potential confounders, and a healthy dose of skepticism in interpreting results.\nAs we navigate through these methods, we’ll emphasize the importance of sensitivity analyses, robustness checks, and transparent reporting of assumptions. By the end of this section, you’ll be equipped with a powerful toolkit for causal inference in non-experimental settings, enabling you to make more informed decisions even when randomized experiments are out of reach.\nLet’s embark on this journey to unlock the causal insights hidden within your observational data!",
    "crumbs": [
      "Non Experimental Methods"
    ]
  },
  {
    "objectID": "causalimpact.html",
    "href": "causalimpact.html",
    "title": "11  Bayesian Structural Time Series",
    "section": "",
    "text": "11.1 Cautionary tales\nMisinterpreting the findings of a {causalImpact} study can occur in several ways. First, overlooking selection bias can lead to erroneous conclusions. For instance, …\nSecond, insufficient data to discern seasonality can also distort results. To illustrate,",
    "crumbs": [
      "Non Experimental Methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bayesian Structural Time Series</span>"
    ]
  },
  {
    "objectID": "causalimpact.html#cautionary-tales",
    "href": "causalimpact.html#cautionary-tales",
    "title": "11  Bayesian Structural Time Series",
    "section": "",
    "text": "Learn more\n\n\n\nBrodersen et al. (2015) Inferring causal impact using Bayesian structural time-series models.\n\n\n\n\n\n\n\n\nBrodersen, Kay H, Fabian Gallusser, Jim Koehler, Nicolas Remy, and Steven L Scott. 2015. “Inferring Causal Impact Using Bayesian Structural Time-Series Models.” Annals of Applied Statistics 9: 247–74. https://doi.org/10.1214/14-AOAS788.",
    "crumbs": [
      "Non Experimental Methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bayesian Structural Time Series</span>"
    ]
  },
  {
    "objectID": "bsynth.html",
    "href": "bsynth.html",
    "title": "12  Bayesian Synthetic Control",
    "section": "",
    "text": "12.1 Key Concepts and Principles\nBefore we dive into the Bayesian approach, let’s review some fundamental concepts:\nPre-treatment Fit: The credibility of a synthetic control estimator hinges on how well it can track the trajectory of the outcome variable for the treated unit before the intervention. A close pre-treatment fit makes for more reliable post-treatment estimates.\nConvex Hull Condition: The synthetic control method works best when the characteristics of the treated unit fall within the convex hull of the donor pool units’ characteristics. This ensures that the treated unit can be approximated by a weighted average of donor units.\nSparse Solutions: Synthetic control estimates typically involve only a few donor pool units with non-zero weights. This sparsity aids in interpretability and helps reduce overfitting.\nNo Anticipation: The method assumes that there are no anticipation effects before the intervention. If such effects exist, it’s advisable to backdate the intervention in the dataset.\nSufficient Pre- and Post-intervention Information: The credibility of the estimates depends on having enough pre-intervention periods to establish a good fit and enough post-intervention periods to observe the full effect of the intervention.\nNo Interference: The method assumes that the intervention does not affect the outcomes of the untreated units. This assumption should be carefully considered in the study design.",
    "crumbs": [
      "Non Experimental Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian Synthetic Control</span>"
    ]
  },
  {
    "objectID": "bsynth.html#the-bayesian-advantage",
    "href": "bsynth.html#the-bayesian-advantage",
    "title": "12  Bayesian Synthetic Control",
    "section": "12.2 The Bayesian Advantage",
    "text": "12.2 The Bayesian Advantage\nPrior Information: Bayesian methods allow us to incorporate prior knowledge or beliefs about the data. This can be particularly useful when we have relevant information from past studies or expert opinions.\nPosterior Distribution: By combining the prior distribution with the likelihood of the observed data, we get a posterior distribution. This distribution represents our updated beliefs about the parameters after taking into account the new data.\nUncertainty Quantification: One of the key strengths of Bayesian methods is their ability to quantify uncertainty. The posterior distribution gives us a range of plausible values for the treatment effects, along with associated probabilities.\nHierarchical Models: Bayesian synthetic control models can be built with hierarchical structures. This allows for more complex relationships and dependencies within the data.\n\nMathematical Formulation\nIn the Bayesian approach, we typically use a Dirichlet distribution as the prior for the weights, ensuring they are positive and sum to 1. We can also introduce a scaling matrix, often denoted as Γ, to control the importance of different predictors.\nLet’s formalize this with some notation:\n\n\\(X_1\\): A \\(k \\times 1\\) matrix of predictors for the treated unit.\n\\(X_0\\): A \\(k \\times J\\) matrix of predictors for the donor units.\n\\(w\\): A \\(J\\times 1\\) vector of weights for the synthetic control.\n\\(\\sigma\\): A scaling parameter.\n\\(\\Gamma\\) A \\(k \\times k\\) scaling matrix.\n\nA simple Bayesian synthetic control model can be formulated as:\n\\[\n\\begin{aligned}\nX_1 | w, \\sigma &\\sim N(X_0w , \\text{diag}(\\Gamma)^{-2}\\sigma^2) \\\\\nw &\\sim \\text{Dir}(1)\\\\\n\\sigma &\\sim N^+(0,1)\\\\\n\\Gamma &\\sim Dir((v_1, \\dots, v_k)') \\quad \\text{s.t. } 1'v = 1 \\\\\n\\end{aligned}\n\\]\n\n\nPractical Implementation: The German Re-unification Example\nIn 1989, a monumental event occurred: the reunification of East and West Germany. A natural question for policymakers was: “What impact did reunification have on West Germany’s GDP?”\nThis very question was addressed in one of the seminal papers on synthetic control (see Abadie, Diamond, and Hainmueller 2015). Using a Bayesian approach, we can not only estimate the effect of reunification but also quantify the uncertainty around that estimate.\nThe {bsynth} package in R provides a convenient way to apply Bayesian synthetic control methods. Let’s see how we can analyze the German reunification data:\n\nlibrary(\"bsynth\")\nload(\"germany.rda\")\ngermany_synth &lt;- bayesianSynth$new(data = germany,\n                                   time = year,\n                                   id = country,\n                                   treated = D,\n                                   outcome = gdp,\n                                   ci_width = 0.95,\n                                   predictor_match = FALSE)\n\nTransforming data\n\ngermany_synth$timeTiles + ggplot2::xlab(\"Year\") + ggplot2::ylab(\"Country\")\n\n\n\n\n\n\n\n\nIn this example, we’re starting with a simple model that doesn’t include predictor matching. We’ll fit the model and visualize the results:\n\ngermany_synth$fit(cores = 4)\n\n# Vizualize the Bayesian Synthetic Control\ngermany_synth$synthetic + \n  ggplot2::xlab(\"Year\") +\n  ggplot2::ylab(\"Per Capita GDP (PPP, 2002 USD)\") +\n  ggplot2::scale_y_continuous(labels=scales::dollar_format())\n\n\n\n\n\n\n\n\nWe can also examine the estimated lift (the cumulative effect of the treatment) over a specific time period:\n\ngermany_synth$liftDraws(from = lubridate::as_date(\"1990-01-01\"), \n                        to = lubridate::as_date(\"2002-01-01\"))\n\n\n\n\n\n\n\nWhen Things Go Wrong: The Pitfalls of Synthetic Controls\nIt’s crucial to remember that synthetic control isn’t a magic bullet. Things can go awry, and you could end up with estimates that are entirely off the mark. Here are some common pitfalls to watch out for:\n\nPoor Pre-treatment Fit: If your synthetic control doesn’t accurately replicate the treated unit’s pre-treatment behavior, don’t use it. It’s as simple as that.\nOverfitting: Even with a perfect pre-treatment fit, there’s the danger of overfitting. This is more likely to happen if you have a short pre-treatment period, a large donor pool, noisy data, or if you relax the weight constraints and allow for extrapolation.\n\nBe careful when using synthetic controls, things co go bad and you could end up with an estimate that is the wrong sign!! The weight restriction allows us to cleanly characterize an upper bound for the bias:\n\\[\\begin{align*}\nE[|\\hat{\\tau}_{1t} - \\tau_{1t}|] \\lesssim \\underbrace{C_1\\mathbb{E}\\text{MAD}\\left(Y_1^P, \\hat{Y}_j^P\\right) + k C_2 \\mathbb{E}\\text{MAD}\\left(Z_1^1,\\hat{Z}_j^1\\right)}_{\\text{First Order}} + \\underbrace{C_3 J^{1/3} \\frac{\\bar{\\sigma}}{T_0^{1/2}}}_{\\text{Second Order}}\n\\end{align*}\\]\n\nFit matters most: If the synthetic control can not replicate the treated unit over time, you should not use it.\nDon’t chase noise: Even with perfect pre-treatment fit there is the danger that you are over-fitting to the pre-treatment period.\n\nOver-fitting is more likely in the following situations:\n\nYou have a short pre-treatment period (small \\(T_0\\)).\nYou have a large donor pool (large \\(J\\)) or the units are not similar to your treated unit.\nYou have very noisy data.\nYou allow for extrapolation by relaxing the weight constraints. In this case, you might have perfect pre-treatment fit but you will likely have significant bias from over-fitting.\n\n\n\nCheck the Bias of your Bayesian Synthetic Controls\nThe ‘bsynth’ package offers you a nice and easy way to check how likely it is that your estimate is badly biased! By computing an upper bound on the relative bias we get an estimate of the probability that your effect could change signs because of the bias.\nIn the case of the German re-unification this is unlikely when we consider the full post-treatment period of 12 years.\n\ngermany_synth$biasDraws(small_bias = 0.2, \n                        firstT = lubridate::as_date(\"1990-01-01\"), \n                        lastT = lubridate::as_date(\"2002-01-01\"))\n\n\n\n\n\nHowever, for a smaller time frame of just 5 years after the re-unification, the bias could overturn the effect! Be careful when you choose a time period to measure cumulative effects as it will change the relative bias too.\n\ngermany_synth$biasDraws(small_bias = 0.2, \n                        firstT = lubridate::as_date(\"1990-01-01\"), \n                        lastT = lubridate::as_date(\"1994-01-01\"))\n\n\n\n\n\n\n\n\n\n\n\nLearn more\n\n\n\n\nAbadie (2021) Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects.\nAbadie and Vives-i-Bastida (2022) Synthetic Controls in Action.\nMartinez and Vives-i-Bastida (2023) Bayesian and Frequentist Inference for Synthetic Controls.\n\n\n\n\n\n\n\n\n\nAbadie, Alberto. 2021. “Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects.” Journal of Economic Literature 59 (2): 391–425.\n\n\nAbadie, Alberto, Alexis Diamond, and Jens Hainmueller. 2015. “Comparative Politics and the Synthetic Control Method.” American Journal of Political Science 59 (2): 495–510.\n\n\nAbadie, Alberto, and Jaume Vives-i-Bastida. 2022. “Synthetic Controls in Action.” https://arxiv.org/abs/2203.06279.\n\n\nAthey, Susan, and Guido W Imbens. 2017. “The State of Applied Econometrics: Causality and Policy Evaluation.” Journal of Economic Perspectives 31 (2): 3–32.\n\n\nMartinez, Ignacio, and Jaume Vives-i-Bastida. 2023. “Bayesian and Frequentist Inference for Synthetic Controls.” https://arxiv.org/abs/2206.01779.",
    "crumbs": [
      "Non Experimental Methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian Synthetic Control</span>"
    ]
  },
  {
    "objectID": "glm.html",
    "href": "glm.html",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "Generalized Linear Models (GLMs) offer a versatile extension to ordinary linear regression, broadening its applicability to a wider range of data types and relationships. These models consist of three key components:\n\nRandom Component: This specifies the probability distribution of the outcome variable, allowing for flexibility beyond the normal distribution assumed in ordinary linear regression.\nLinear Predictor: A familiar linear combination of covariates (independent variables) that contributes to explaining the outcome.\nLink Function: This crucial element connects the random component and linear predictor, transforming the linear combination to align with the scale and range of the outcome variable.\n\nThe following chapters will cover very simple, yet very useful, models. After you master the basics, you will probably want to write your own models from scratch.\n\n\n\n\n\n\nLearn more\n\n\n\n\nGoodrich et al. (2020) rstanarm: {Bayesian} applied regression modeling via {Stan}.\nBürkner (2017) {brms}: An {R} Package for {Bayesian} Multilevel Models Using {Stan}.\nCarpenter et al. (2017) Stan: A Probabilistic Programming Language.\n\n\n\n\n\n\n\n\n\nBürkner, Paul-Christian. 2017. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\nCarpenter, Bob, Andrew Gelman, Matthew D. Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. 2017. “Stan: A Probabilistic Programming Language.” Journal of Statistical Software 76 (1): 1–32. https://doi.org/10.18637/jss.v076.i01.\n\n\nGoodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2020. “Rstanarm: Bayesian Applied Regression Modeling via Stan.” https://mc-stan.org/rstanarm.",
    "crumbs": [
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "blm.html",
    "href": "blm.html",
    "title": "13  Bayesian Linear Regression",
    "section": "",
    "text": "While Ordinary Least Squares (OLS) is a popular frequentist method for linear regression, the Bayesian approach is arguably better suited for informing business decisions.\nOLS aims to find the line that minimizes the sum of squared differences between observed and predicted values. It treats model parameters as fixed but unknown quantities, estimating them by minimizing residuals. Inference relies on hypothesis testing and p-values to assess the significance of relationships. However, this approach can lead to a rigid focus on statistical significance rather than practical relevance.\nA Bayesian Linear Model, while similar in structure to OLS, views parameters as random variables with probability distributions reflecting uncertainty. Prior distributions incorporate existing knowledge or assumptions, and Bayes’ theorem combines this prior information with observed data to estimate the posterior distribution of parameters. Inference focuses on posterior probabilities to quantify uncertainty and interpret the strength of evidence.\nThe Bayesian approach offers several advantages for business decision-making:\n\nIncorporating Prior Knowledge: Bayesian models allow you to explicitly include prior knowledge or beliefs about the parameters, which can be valuable in business contexts where historical data or expert opinions exist.\nLearning from New Data: The Bayesian framework naturally shows how new data updates and refines your understanding of the relationships between variables.\nThinking in Bets: Instead of relying on the binary and often arbitrary concept of statistical significance, Bayesian analysis encourages thinking in terms of probabilities and bets. This aligns well with business decisions, where you often need to weigh potential risks and rewards.\nPractical Significance: While anything can be statistically significant with a large enough sample size, Bayesian analysis focuses on the magnitude and probability of effects that are practically meaningful for your business goals. Even if a result isn’t statistically significant, it could still be a good bet if the posterior probability of a meaningful impact is sufficiently high.\n\nThe Bayesian approach embraces the inherent uncertainty in data analysis, providing a richer and more nuanced understanding of the relationships between variables, ultimately leading to more informed and effective business decisions.\n\nAn example with synthetic data:\nImagine that you are faced with a decision: should you discontinue a product? You would like to keep the product if, and only if, its impact on your outcome of interest is at least 0.1. To help you make this decision, you’ve conducted a well-designed experiment. Let’s illustrate this with some synthetic data:\n\nlibrary(dplyr)\nset.seed(9782)\nN  &lt;- 200\nfake_data &lt;- tibble::tibble(\n  x = rnorm(n = N, mean = 0, sd = 1),\n  t = sample(x = c(T,F), size = N, replace = T, prob = c(0.5,0.5)),\n  e = rnorm(n = N, mean = 0, sd = 0.4)\n  ) %&gt;% \n1  mutate(y = 7.1 + 0.6*x + 0.02*t + e)\n\n\n1\n\nNote that the true impact is 0.02, suggesting that the correct decision would be to not discontinue the product. However, what happens if you analyze this data using a traditional frequentist approach?\n\n\n\n\n\nFrequentist approach:\n\nlibrary(ggplot2)\nlibrary(broom)\n\nlm1 &lt;- lm(data = fake_data, formula = y ~ x + t) %&gt;% \n  tidy(., conf.int=T, conf.level=0.95) %&gt;% \n  filter(term==\"tTRUE\") \n\nplot &lt;- ggplot(data = lm1, aes(y=estimate, x= term)) +\n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +\n  geom_hline(yintercept = 0, linetype = \"dotted\", color = \"blue\") + \n  scale_y_continuous(breaks = seq(-0.2, 0.2, by = 0.02)) +\n  theme_bw(base_size = 18) +\n  xlab(\"\") + \n  ylab(\"Impact\") + \n  theme(\n  axis.text.x = element_blank(),\n  axis.ticks.x = element_blank())\n\nplot\n\n\n\n\n\n\n\n\nIn this case, the point estimate is 0.08, the p-value 0.17 is greater than 0.05, and the 95% confidence interval ranges from -0.04 to 0.19. How would a decision-maker typically use this information? Unfortunately, many might decide to discontinue the product, misinterpreting the results (see Chandler et al. 2020).\n\n\n\n\n\n\nThe null ritual, Gigerenzer, Krauss, and Vitouch (2004):\n\n\n\n\nSet up a statistical null hypothesis of “no mean difference” or “zero correlation.” Don’t specify the predictions of your research hypothesis or of any alternative substantive hypotheses.\nUse 5% as a convention for rejecting the null. If significant, accept your research hypothesis.\nAlways performing this procedure.\n\n\n\nThis problem was so widespread that in 2016, the American Statistical Association issued a statement cautioning against this practice (see Wasserstein and Lazar 2016). Confidence intervals are also frequently misinterpreted (see Hoekstra et al. 2014).\n\n\n\n\n\n\nIncorrect interpretations:\n\n\n\n\nThe probability that the true mean is greater than 0 is at least 95%.\nThe probability that the true mean equals 0 is smaller than 5%.\nThe “null hypothesis” that the true mean equals 0 is likely to be incorrect.\nThere is a 95% probability that the true mean lies between -0.04 and 0.19.\nWe can be 95% confident that the true mean lies between -0.04 and 0.19.\nIf we were to repeat the experiment over and over, then 95% of the time the true mean falls between 0.1 and 0.19.\n\n\n\n\n\n\n\n\n\nCorrect interpretations:\n\n\n\nA particular procedure, when used repeatedly across a series of hyptothetical data sets, yields intervals that contain the true parameter value 95% of the cases. The key is that the CIs do not provide a statement about the parameter as it relates to the particular sample at hand.\n\n\nThis example starkly illustrates the disconnect between what decision-makers want to say and what a frequentist approach allows them to say. The good news? Bayesian methods offer a way to answer business questions directly and in plain language.\n\n\nBayesian approach:\nThe Bayesian approach to linear regression fundamentally shifts how we interpret and utilize data in decision-making. Rather than relying on point estimates and p-values, it focuses on understanding the probability distributions of parameters, providing a richer, more nuanced picture.\nIn a Bayesian Linear Model, parameters are viewed as random variables with their own probability distributions. This perspective allows us to incorporate prior knowledge into the model: prior distributions reflect existing knowledge or beliefs about parameters before observing the current data, which can be based on historical data, expert opinions, or theoretical considerations. The likelihood represents the probability of the observed data given the parameters, similar to the frequentist approach. Posterior distributions combine the prior distribution and the likelihood using Bayes’ theorem, reflecting updated beliefs about the parameters after observing the data. The beauty of the Bayesian approach lies in its flexibility and adaptability. As new data becomes available, the posterior distribution from one analysis can serve as the prior for the next, continually refining our understanding.\nBusiness decisions often leverage historical data and expert judgment, and Bayesian models explicitly incorporate this information, leading to more informed and credible inferences. Bayesian analysis naturally adapts to new information. As fresh data is collected, the model updates its estimates, providing a dynamic and current understanding of the business environment. Instead of fixating on binary outcomes (significant vs. non-significant), Bayesian analysis assesses probabilities, aligning perfectly with the real-world decision-making process, which is inherently probabilistic and involves weighing risks and rewards. Bayesian models emphasize the magnitude and probability of effects that matter in practice. This focus is crucial in business, where even small but reliable improvements can have substantial impacts.\nThe {im} package fits a Bayesian linear model using weakly informative priors for the covariates and allows the user to set more informative priors for the impact of the intervention. If \\(y\\) is the outcome of interest, the model is specified as follows:\n\\[\n\\begin{aligned}\ny & \\sim N(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + X^\\star\\beta + \\color{red}{\\eta} t\n\\end{aligned}\n\\]\nWe standardize the data as follows:\n\\[\n\\begin{aligned}\ny^\\star & = \\frac{y - \\mu_y}{\\sigma_y} \\\\\n& \\sim N(\\mu^\\star, \\sigma^\\star) \\\\\n\\mu^\\star & = \\alpha^\\star + \\frac{X - \\mu_X}{\\sigma_X} \\beta^\\star +\n\\eta^\\star t \\\\\n\\alpha^\\star & \\sim N(0,1) \\\\\n\\beta^\\star & \\sim N(0,1) \\\\\n\\color{red}{\\eta^\\star} & \\color{red}{\\sim N(\\mu_\\eta, \\sigma_\\eta)} \\\\\n\\sigma^\\star & \\sim N^+(0,1) \\\\\n\\end{aligned}\n\\]\nTherefore\n\\[\n\\begin{aligned}\n\\frac{y - \\mu_y}{\\sigma_y} & = \\alpha^\\star +\n\\frac{X - \\mu_X}{\\sigma_X} \\beta^\\star + \\eta^\\star t \\\\\ny & = (\\alpha^\\star +\n\\frac{X - \\mu_X}{\\sigma_X} \\beta^\\star + \\eta^\\star t) \\sigma_y + \\mu_y \\\\\n\\color{red}\\eta = \\eta^\\star \\sigma_y\n\\end{aligned}\n\\]\nNotice that if you have better priors, you should use them. To use this simple model, you just need to run the following code:\n\nlibrary(im)\n\nfitted_blm &lt;- blm$new(\n  y = \"y\", \n  x = c(\"x\"),\n  treatment = \"t\", \n  data = fake_data, \n  eta_mean = 0,\n  eta_sd = 0.5\n)\n\nIt is always a good idea to look at the traceplot. A traceplot is a diagnostic tool used to visualize the “path” that a Markov Chain Monte Carlo (MCMC) sampler takes as it explores the parameter space. It helps assess the convergence and mixing of the chains, which is crucial for ensuring reliable inference from the model.\n\nfitted_blm$tracePlot()\n\n\n\n\n\n\n\n\n\nAssessing Convergence:\n\nA well-converged chain should exhibit a “hairy caterpillar” pattern, where the trace fluctuates around a stable value without any trends or drifts. This indicates that the sampler has adequately explored the parameter space and reached a stationary distribution.\nConversely, non-converging chains might show trends, jumps, or slow mixing, suggesting that the sampler is stuck in a local region or hasn’t adequately explored the posterior distribution. Inferences drawn from such chains can be unreliable and misleading.\n\nDiagnosing Mixing:\n\nGood mixing implies that the chains effectively explore the entire parameter space and don’t get stuck in local regions. This is visually represented by well-intertwined lines from different chains on the traceplot.\nPoorly mixed chains show distinct separation among lines, indicating they haven’t adequately explored the entire posterior distribution. This can lead to biased and inaccurate estimates of the parameters and their uncertainty.\n\nIdentifying Issues:\n\nTraceplots can reveal potential issues in the model specification, priors, or MCMC settings. For example, highly correlated parameters might exhibit synchronized movement in the traceplot, suggesting a dependence relationship that needs further investigation.\nOverall, examining traceplots is a valuable diagnostic step in Bayesian statistical analysis. They provide valuable insights into the convergence and mixing of MCMC chains, aiding in the valid and reliable interpretation of the model results.\nIt is prudent to verify that our model’s data generating process is compatible with the data used to fit the model. To do this, we can compare the kernel density of draws from the posterior distribution to the density of our data.\n\nfitted_blm$ppcDensOverlay(n = 50)\n\n\n\n\n\n\n\n\nThe next step is to use the fitted Bayesian model to answer our business question directly. In this example, we want to determine the likelihood that the product’s impact is at least \\(0.01\\). We can calculate this probability with a single line of code:\n\nfitted_blm$posteriorProb(threshold = 0.01)\n\nGiven the data, we estimate that the probability that the effect is more than 0.01 is 89%.\n\n\nWith this information, we can make a much more informed decision about whether to keep the product than if we were merely assessing the rejection of a null hypothesis. Moreover, we may care about multiple thresholds for this decision. For instance, if the impact exceeds \\(0.2\\), we might consider doubling our investment.\nThe {im} package enables the creation of interactive visualizations that effectively demonstrate our data insights and summarize the risks associated with various decisions.\n\nfitted_blm$vizdraws(breaks = c(0.01, 0.2),\n                    break_names = c(\"Discontinue\", \"Keep\", \"Double down\"),\n                    display_mode_name = TRUE)\n\n\n\n\n\nThe plot generated by this code not only answers our business question directly but also illustrates how much we have learned from the data and how our initial priors have evolved. This comprehensive view is crucial for making better business decisions.\nBayesian analysis provides probabilities directly aligned with decision-making needs. For example, if the probability that the product’s impact exceeds \\(0.01\\) is low, we can confidently discontinue it. Conversely, if there’s a reasonable probability of a positive impact, we might decide to retain the product, potentially conducting further investigations or collecting more data.\nIn conclusion, the Bayesian approach offers a powerful, flexible, and intuitive framework for business decision-making. By focusing on probabilities and incorporating prior knowledge, it provides a clearer and more practical basis for making informed decisions in an uncertain world. This methodology enhances our ability to navigate uncertainty, ultimately leading to more effective and strategic business outcomes.\n\n\n\n\n\n\nChandler, Jesse J, Ignacio Martinez, Mariel M Finucane, Jeffrey G Terziev, and Alexandra M Resch. 2020. “Speaking on Data’s Behalf: What Researchers Say and How Audiences Choose.” Evaluation Review 44 (4): 325–53.\n\n\nGigerenzer, Gerd, Stefan Krauss, and Oliver Vitouch. 2004. “The Null Ritual.” The Sage Handbook of Quantitative Methodology for the Social Sciences, 391–408.\n\n\nHoekstra, Rink, Richard D Morey, Jeffrey N Rouder, and Eric-Jan Wagenmakers. 2014. “Robust Misinterpretation of Confidence Intervals.” Psychonomic Bulletin & Review 21: 1157–64. https://doi.org/10.3758/s13423-013-0572-3.\n\n\nWasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA Statement on p-Values: Context, Process, and Purpose.” The American Statistician. Taylor & Francis. https://www.tandfonline.com/doi/pdf/10.1080/00031305.2016.1154108.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bayesian Linear Regression</span>"
    ]
  },
  {
    "objectID": "meta-analysis.html",
    "href": "meta-analysis.html",
    "title": "14  Meta-Analysis",
    "section": "",
    "text": "14.1 A Simple Example Using Synthetic Data\nLet’s ground this concept with a practical scenario. Suppose your objective is to estimate the incremental lift of an intervention, and you have access to multiple studies that have investigated its effects. Faced with a variety of estimates, how would you determine which one to rely on for your decision-making? While you might be tempted to simply choose the number you prefer, mentally combine the estimates, or calculate a simple average, a more rigorous approach is to conduct a Bayesian meta-analysis.\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nstudies &lt;- tibble(\n  study = LETTERS[1:5],\n  lift_hat = c(0.09, 0.06, 0.06, 0.07, 0.04),\n  std_err = c(0.01, 0.01, 0.007, 0.007, 0.01)\n) %&gt;%\n  mutate(\n    LB = lift_hat - 1.96 * std_err,\n    UB = lift_hat + 1.96 * std_err\n  )\nlibrary(ggiraph)\n\ntooltip_css &lt;-\n  \"background-color:gray;color:white;font-style:italic;padding:10px;border-radius:5px;font-size:16px;\"\n\n\nstudies &lt;- studies %&gt;%\n  mutate(\n    tooltip = glue::glue(\n      \"lift is {scales::percent(lift_hat)} with a 95% confidence interval between {scales::percent(LB, accuracy=1)} and {scales::percent(UB, accuracy=1)}\"\n    )\n  )\n\nmy_plot &lt;- ggplot(data = studies, aes(x = study, y = lift_hat)) +\n  geom_pointrange_interactive(aes(\n    ymin = LB,\n    ymax = UB,\n    tooltip = tooltip\n  )) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  ylab(\"Lift\") +\n  xlab(\"Study\") +\n  theme_bw(base_size = 20)\n\ngirafe(\n  code = print(my_plot),\n  pointsize = 20,\n  width_svg = 7,\n  height_svg = 3.5,\n  options = list(opts_tooltip(css = tooltip_css))\n)\nGiven these studies, what insights can you glean? Let’s say you need to make a decision based on whether the lift is at least 5%. Would you be able to confidently determine the answer?\nTo address this, you can fit a straightforward model to the data:\n\\[\n\\begin{aligned}\ny_i &\\sim N(\\theta_i,s_i)  \\\\\n\\theta_i &\\sim N(\\mu, \\tau) \\\\\n\\mu &\\sim N(0, 0.05) \\\\\n\\tau &\\sim N^+(0, 0.05)\n\\end{aligned}\n\\] where:\nTo fit this model you can use im::metaAnalysis():\nlibrary(im)\n\ntest_meta &lt;- metaAnalysis$new(data = studies, point_estimates = lift_hat,\n                              standard_errors = std_err, id = study)\nCalculating the probability that the lift is at least 5% becomes remarkably simple:\ntest_meta$probability(a = 0.05)\n\nThe probability that lift is more than 5% is 89%.\nTo visualize the insights gained from the meta-analysis, you can plot the posterior probability:\n# Plot the lift's prior and posterior distributions\ntest_meta$PlotLift(\n  breaks = c(0, 0.01, 0.05, 0.1),\n  break_names = c(\" &lt; 0\", \"(0,1%)\", \"(1%,5%)\", \"(5%,10%)\", \"&gt; 10%\"),\n  display_mode_name = TRUE\n)",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Meta-Analysis</span>"
    ]
  },
  {
    "objectID": "meta-analysis.html#a-simple-example-using-synthetic-data",
    "href": "meta-analysis.html#a-simple-example-using-synthetic-data",
    "title": "14  Meta-Analysis",
    "section": "",
    "text": "\\(y_i\\) is the estimated lift in study \\(i\\)\n\\(s_i\\) is the standard error of the lift in study \\(i\\)\n\\(\\theta_i\\) is the true lift in study \\(i\\)\n\\(\\mu\\) is the true lift in the population\n\\(\\tau\\) is the standard deviation in lift",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Meta-Analysis</span>"
    ]
  },
  {
    "objectID": "meta-analysis.html#a-more-complex-meta-anlysis",
    "href": "meta-analysis.html#a-more-complex-meta-anlysis",
    "title": "14  Meta-Analysis",
    "section": "14.2 A more complex meta-anlysis",
    "text": "14.2 A more complex meta-anlysis\nLet’s delve deeper into the realm of Bayesian meta-analysis by incorporating study-specific characteristics that can offer richer insights into the factors influencing the impact of interventions. Imagine that each study in your collection comes with valuable metadata, such as the geographical location where the intervention was implemented, the specific modality of the intervention, or any other relevant attribute. By weaving these details into our model, we can uncover whether the impact varies significantly across different locations or modalities.\nTo achieve this, we can refine our model as follows:\n\\[\n\\begin{aligned}\ny_i & \\sim N(\\theta_i,s_i)  \\\\\n\\theta_i & \\sim N(\\mu_i, \\tau) \\\\\n\\mu_i & = \\mu_0 + X\\beta\n\\end{aligned}\n\\]\nIn this augmented model \\(X\\) is a matrix of study-specific characteristics. By fitting this model, we can not only estimate the overall effect size but also discern whether the impact is significantly higher or lower for specific locations, modalities, or any other characteristic captured in the metadata. This granular understanding allows for more targeted decision-making, enabling you to tailor interventions to specific contexts and maximize their effectiveness.\nHowever, it is crucial to remember that the quality of your meta-analysis hinges on the quality of the data you feed into it. As with any statistical model, the adage “garbage in, garbage out” holds true. If the underlying studies are flawed or biased, the meta-analysis will not magically erase those imperfections. For instance, to conduct even a simple meta-analysis, you ideally need at least five high-quality randomized controlled trials (RCTs) to ensure robust results.\nIn essence, Bayesian meta-analysis acts as a versatile instrument, empowering you to harmonize diverse sources of evidence, account for heterogeneity, and extract actionable insights from study-level characteristics. As you continue your exploration of causal inference in the tech industry, remember that Bayesian methods offer a robust framework for navigating uncertainty, optimizing interventions, and driving impactful decisions that propel your company forward. However, the success of this endeavor rests on the foundation of sound data and rigorous study design.\n\n\n\n\n\n\nLearn more\n\n\n\nGelman et al. (2013) Parallel Experiments in Eight Schools.\n\n\n\n\n\n\n\n\nGelman, A., J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin. 2013. “Parallel Experiments in Eight Schools.” In Bayesian Data Analysis, Third Edition, 119. Chapman & Hall/CRC Texts in Statistical Science. New York: Taylor & Francis. https://books.google.com/books?id=ZXL6AQAAQBAJ.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Meta-Analysis</span>"
    ]
  },
  {
    "objectID": "beyond_normality.html",
    "href": "beyond_normality.html",
    "title": "15  Beyond Normality",
    "section": "",
    "text": "15.1 Binary Outcomes: The Coin Flips of Data\nBinary outcomes are the coin flips of the data world – two sides, two possibilities. Think success/failure, yes/no, or the ever-important adopt/reject decision. To model these, we turn to the trusty logistic regression. While linear probability models are also used, logistic regression has the distinct advantage of keeping our predictions bounded between the sensible limits of 0 and 1.\nThis workhorse of a model is a type of generalized linear model, employing a logit link function:\n\\[\n\\text{BernoulliLogit}(y|\\theta) = \\text{Bernoulli}(y|\\text{logit}^{-1}(\\theta))\n\\]\nwhere\n\\[\n\\text{logit}^{-1}(\\theta) = \\frac{1}{1 + \\exp(-\\theta)}\n\\]\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(dplyr)\n\ninv_logit &lt;- function(theta) {\n  return(1 / (1 + exp(-theta)))\n}\n\nggplot2::ggplot() +\n  ggplot2::stat_function(fun = inv_logit) +\n  ggplot2::theme_minimal() +\n  ggplot2::xlim(-5, 5) +\n  ggplot2::xlab(expression(theta))\nTo illustrate, let’s cook up some fake data:\n# Fake data\nN &lt;- 2000\nK &lt;- 2\nset.seed(1982)\nfake_data &lt;- tibble::tibble(\n  x1 = rnorm(N, mean = 0, sd = 1),\n  x2 = rnorm(N, mean = 0, sd = 1),\n  treat = sample(\n    x = c(TRUE, FALSE), size = N, replace = TRUE,\n    prob = c(0.5, 0.5)\n  ),\n  r = runif(n = N, min = 0, max = 1)\n) %&gt;%\n  dplyr::mutate(\n    p0 = inv_logit(theta = -3 + 0.1 * x1 + 0.25 * x2),\n    p1 = inv_logit(theta = -3 + 0.1 * x1 + 0.25 * x2 + 0.2),\n    y0 = dplyr::case_when(p0 &gt; r ~ 1, TRUE ~ 0),\n    y1 = dplyr::case_when(p1 &gt; r ~ 1, TRUE ~ 0),\n    y = dplyr::case_when(\n      treat ~ as.logical(y1),\n      TRUE ~ as.logical(y0)\n    )\n  )\ndplyr::glimpse(fake_data)\n\nRows: 2,000\nColumns: 9\n$ x1    &lt;dbl&gt; 0.685092067, -0.005550195, -0.777641329, 1.875702830, -0.3771291…\n$ x2    &lt;dbl&gt; -0.665502269, -1.256150229, -0.230714338, 0.743955915, -0.630752…\n$ treat &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE,…\n$ r     &lt;dbl&gt; 0.45401970, 0.20720548, 0.15142911, 0.31274073, 0.88143116, 0.97…\n$ p0    &lt;dbl&gt; 0.04319535, 0.03507396, 0.04166872, 0.06745600, 0.03933915, 0.03…\n$ p1    &lt;dbl&gt; 0.05225914, 0.04250932, 0.05042906, 0.08117855, 0.04763407, 0.04…\n$ y0    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y1    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y     &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n\nmean_y0 &lt;- mean(fake_data$y0)\nmean_y1 &lt;- mean(fake_data$y1)\nimpact &lt;- round((mean(fake_data$y1) - mean(fake_data$y0)) * 100, 2)\nIn this fabricated dataset, we’ve engineered a scenario where, without intervention, only 4% would have adopted the feature. Yet, with the intervention applied universally, adoption would have jumped to 5%. The true impact of this intervention is a hefty 1 percentage points.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Beyond Normality</span>"
    ]
  },
  {
    "objectID": "beyond_normality.html#binary-outcomes-the-coin-flips-of-data",
    "href": "beyond_normality.html#binary-outcomes-the-coin-flips-of-data",
    "title": "15  Beyond Normality",
    "section": "",
    "text": "Prior predictive checking\nAs Bayesians, we’re not just number crunchers; we’re storytellers. We weave narratives about data, and our priors are the opening chapters. So, before we unleash our model, let’s ponder what our priors imply.\nOur model takes this form:\n\\[\n\\begin{aligned}\n\\theta &=  \\alpha + X \\beta + \\tau  T \\\\\n\\alpha &\\sim N(\\mu_{\\alpha}, sd_{\\alpha}) \\\\\n\\beta_j &\\sim N(\\mu_{\\beta_j}, sd_{\\beta_j}) \\\\\n\\tau &\\sim N(\\mu_{\\tau}, sd_{\\tau})\n\\end{aligned}\n\\]\n\nlogit &lt;- im::logit$new(\n  data = fake_data,\n  y = \"y\", # this will not be used\n  treatment = \"treat\",\n  x = c(\"x1\", \"x2\"),\n  mean_alpha = -3,\n  sd_alpha = 2,\n  mean_beta = c(0, 0),\n  sd_beta = c(1, 1),\n  tau_mean = 0.05,\n  tau_sd = 0.5,\n  fit = FALSE # we will not be fitting the model\n)\n\nlogit$plotPrior()\n\n\n\n\n\n\n\n\n\n\nFitting the Model: Where Theory Meets Data\nSatisfied with our priors, we’re ready to fit the model to the data:\n\nlogit &lt;- im::logit$new(\n  data = fake_data,\n  y = \"y\",\n  treatment = \"treat\",\n  x = c(\"x1\", \"x2\"),\n  mean_alpha = -3,\n  sd_alpha = 2,\n  mean_beta = c(0, 0),\n  sd_beta = c(1, 1),\n  tau_mean = 0.05,\n  tau_sd = 0.5,\n  fit = TRUE\n)\n\nLet’s glance at the trace plot of tau to ensure our chains mixed well and converged:\n\nlogit$tracePlot()\n\n\n\n\n\n\n\n\nTo sum up our findings, we have a few handy methods at our disposal:\n\nlogit$pointEstimate()\n\n[1] 1.35\n\nlogit$credibleInterval(width = 0.95)\n\nGiven the data, we estimate that there is a 95% probability that the effect is between -1 and 4 percentage points.\n\nlogit$calcProb(a = 0)\n\nGiven the data, we estimate  that the probability that the effect is more than 0 percentage points is 89%.\n\n\nWe can also use the prediction function to predict new data and compare the differences between groups. The predict function takes the new_data and name argument to name the group. For example, here we will predict the data as if all units are treated, then make another prediction as if all units are not treated and summarize the two groups.\n\nfake_treated_data &lt;- fake_data %&gt;% mutate(treat = TRUE)\nfake_control_data &lt;- fake_data %&gt;% mutate(treat = FALSE)\nlogit$predict(\n  new_data = fake_treated_data,\n  name = \"y1\"\n)\nlogit$predict(\n  new_data = fake_control_data,\n  name = \"y0\"\n)\nlogit$predSummary(name = \"y1\", width = 0.95, a = 0)\n\n[1] \"Given the data, we estimate that for group: y1, the point estimate of the group average is 5%. With 95% probability, the point estimate is between 4 and 7 percentage points. Given the data, we estimate  that the probability that the group average is more than 0 percentage points is 100%.\"\n\nlogit$predSummary(name = \"y0\", width = 0.95, a = 0)\n\n[1] \"Given the data, we estimate that for group: y0, the point estimate of the group average is 4%. With 95% probability, the point estimate is between 3 and 6 percentage points. Given the data, we estimate  that the probability that the group average is more than 0 percentage points is 100%.\"\n\n\nWe can also compare the differences between two groups of predictions.\n\nlogit$predCompare(name1 = \"y1\", name2 = \"y0\", width = 0.95, a = 0)\n\n[1] \"Given the data, we estimate that the point estimate of the group difference is 1%. With 95% probability, the point estimate is between -1 and 3 percentage points. Given the data, we estimate  that the probability that the group difference is more than 0 percentage points is 89%.\"\n\n\nWe can also summarize and compare the predictions conditioning on subgroups.\n\nlogit$predSummary(\n  name = \"y1\",\n  subgroup = (fake_data$x1 &gt; 0),\n  width = 0.95, a = 0\n)\n\n[1] \"Given the data, we estimate that for group: y1, the point estimate of the group average is 5%. With 95% probability, the point estimate is between 3 and 8 percentage points. Given the data, we estimate  that the probability that the group average is more than 0 percentage points is 100%.\"\n\nlogit$predCompare(\n  name1 = \"y1\",\n  name2 = \"y0\",\n  subgroup1 = (fake_treated_data$x1 &gt; 0),\n  subgroup2 = (fake_control_data$x1 &gt; 0),\n  width = 0.95, a = 0\n)\n\n[1] \"Given the data, we estimate that the point estimate of the group difference is 1%. With 95% probability, the point estimate is between -1 and 4 percentage points. Given the data, we estimate  that the probability that the group difference is more than 0 percentage points is 85%.\"\n\n\nFinally, we can get the posterior predictive draws for advanced analysis.\n\npred &lt;- logit$getPred(name = \"y1\")",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Beyond Normality</span>"
    ]
  },
  {
    "objectID": "beyond_normality.html#ordinal-outcomes",
    "href": "beyond_normality.html#ordinal-outcomes",
    "title": "15  Beyond Normality",
    "section": "15.2 Ordinal Outcomes",
    "text": "15.2 Ordinal Outcomes\nOrdinal outcomes are a rather curious beast in the realm of causal inference. They have a natural order - think of a survey respondent rating their satisfaction on a scale from “very dissatisfied” to “very satisfied” - but the intervals between the values don’t necessarily hold equal weight. The distance between “dissatisfied” and “neutral” may not be the same as that between “satisfied” and “very satisfied.” This lack of equal spacing is a challenge we must address head-on when analyzing the impact of an intervention or treatment.\nThis nuance of ordinal outcomes is also present in other domains, such as user experience ratings (e.g., poor, fair, good, excellent), or educational attainment levels (e.g., less than high school, high school diploma, some college, college degree). In each case, there’s a clear order, but the spacing between levels is not uniform.\nThis lack of uniform spacing can complicate our analysis, particularly when applying traditional regression models designed for continuous outcomes. We need a tailored approach that respects the ordinal nature of the data, while still allowing us to draw meaningful causal inferences.\nTODO",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Beyond Normality</span>"
    ]
  },
  {
    "objectID": "beyond_normality.html#count-outcomes",
    "href": "beyond_normality.html#count-outcomes",
    "title": "15  Beyond Normality",
    "section": "15.3 Count Outcomes",
    "text": "15.3 Count Outcomes\nIn this section, we’re talking about events we can tally—website visits, customer complaints, product sales, you name it. These outcomes are inherently non-negative integers, reflecting the discrete nature of the events we’re counting.\nThe go-to tool for analyzing count outcomes is often Poisson regression. However, real-world data frequently throws us a curveball in the form of overdispersion, where the variance of the count data outstrips its mean. This is where negative binomial regression swoops in to save the day. It’s a souped-up version of Poisson regression that accounts for overdispersion by adding an extra parameter to the model.\nCount data pops up in all sorts of scenarios. We might be interested in the effect of a new marketing campaign on app downloads or the impact of a software update on user logins. With count outcomes, the possibilities are as endless as the events we can count.\n\nAn Example with Fake Data: Video Views Galore\nLet’s say we’re interested in the number of views a video receives in a given period. This is a perfect opportunity to use the negative binomial distribution to model the underlying data-generating process. Here’s how we can express this:\n\\[\n\\begin{aligned}\ny_i  & \\sim \\text{NB}(\\mu_i, \\phi) \\\\\nlog(\\mu_i)  & = \\alpha + X\\beta\n\\end{aligned}\n\\]\nIn this model, \\(\\mu\\) represents the mean (which must be positive, hence the log link function), and the inverse of \\(\\phi\\) controls the overdispersion. A small \\(\\phi\\) means the negative binomial distribution significantly deviates from a Poisson distribution, while a large \\(\\phi\\) brings it closer to a Poisson. This becomes clear when we look at the variance:\n\\[\nVar(y_i)  \\sim \\mu_i + \\frac{\\mu_i^2}{\\phi}\n\\]\nLet’s whip up some fake data to illustrate:\n\nset.seed(9782)\nlibrary(dplyr)\nlibrary(ggplot2)\nN &lt;- 1000\n\nfake_data &lt;-\n  tibble::tibble(x1 = runif(N, 2, 9), x2 = rnorm(N, 0, 1)) %&gt;%\n  dplyr::mutate(\n    mu = exp(0.5 + 1.7 * x1 + 4.2 * x2),\n    y0 = rnbinom(N, size = 2, mu = mu), # here size is phi\n    y1 = y0 * 1.05,\n    t = sample(c(TRUE, FALSE), size = n(), replace = TRUE, prob = c(0.5, 0.5)),\n    y = case_when(t ~ as.integer(y1),\n      .default = as.integer(y0)\n    )\n  ) %&gt;%\n  filter(y &gt; 0)\n\n# Plotting the histogram using ggplot2\nggplot(fake_data, aes(x = y)) +\n  geom_histogram(\n    bins = 100,\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  facet_wrap(~t) +\n  labs(title = \"Histogram of y\", x = \"y\", y = \"Frequency\") +\n  xlim(0, 1e4) +\n  ylim(0, 20)\n\n\n\n\n\n\n\n\n\n\nOLS: A Common, But Flawed, Approach\nA typical way to estimate the lift of an intervention in this scenario is to run ordinary least squares (OLS) on the natural logarithm of the outcome. Then, folks often look at the point estimate and 95% confidence interval for the treatment effect. Doing this with our fake data might lead you to conclude that the intervention is ineffective, as the 95% confidence interval includes zero.\n\nlm(data = fake_data, log(y) ~ x1 + x2 + t) %&gt;% broom::tidy(conf.int = TRUE)\n\n# A tibble: 4 × 7\n  term        estimate std.error statistic    p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   0.422     0.0861     4.89  0.00000116   0.253      0.591\n2 x1            1.68      0.0137   122.    0            1.65       1.70 \n3 x2            4.15      0.0293   142.    0            4.09       4.20 \n4 tTRUE         0.0254    0.0547     0.464 0.643       -0.0819     0.133\n\n\n\n\nTaming Overdispersion: The Bayesian Negative Binomial Advantage\nEnter the Bayesian negative binomial model, easily implemented using the {im} package in R. This approach has two key advantages. First, it better captures the true data-generating process. Second, being Bayesian, it lets us incorporate prior information and express our findings in a way that’s more directly relevant to business decisions.\n\nlibrary(im)\nnb &lt;- negativeBinomial$new(\n  data = fake_data, y = \"y\", x = c(\"x1\", \"x2\"),\n  treatment = \"t\", tau_mean = 0.0, tau_sd = 0.025\n)\n\nA quick check of the trace plot is always a good idea:\n\nnb$tracePlot()\n\n\n\n\n\n\n\n\nNow for the payoff. We can readily obtain a point estimate for the impact using nb$pointEstimate() (4%) and a 95% credible interval using nb$credibleInterval(width = 0.95, round = 2) (Given the data, we estimate that there is a 95% probability that the effect is between 0 and 0.08.). But what if we want to know the probability that the impact is at least 1% (or any other threshold)? Easy peasy! We use nb$posteriorProb(threshold = 0.01) (Given the data, we estimate that the probability that the effect is more than 0.01 is 93%.). Finally, to visualize the evolution of our understanding from prior to posterior, we employ nb$vizdraws().\n\nnb$vizdraws(display_mode_name = TRUE, breaks = 0.01,\n            break_names = c(\"&lt; 0.01\", \"&gt; 0.01\"))",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Beyond Normality</span>"
    ]
  },
  {
    "objectID": "stochastictrees.html",
    "href": "stochastictrees.html",
    "title": "Stochastic Trees",
    "section": "",
    "text": "This section will delve into a specific family of non-parametric models that has gained considerable traction in the causal inference world: Bayesian Additive Regression Trees (BART) and some of its notable derivatives. These models offer a unique blend of flexibility and interpretability, making them particularly well-suited for causal inference tasks.\nBefore we dive into the specifics of these models, it’s crucial to understand the fundamental assumptions that underpin their use in causal inference. Like many causal inference methods, BART and its derivatives rely on two key assumptions: the Stable Unit Treatment Value Assumption (SUTVA) and strong ignorability.\n\nStable Unit Treatment Value Assumption (SUTVA): SUTVA consists of two parts:\n\nNo interference: The treatment applied to one unit does not affect the outcomes of other units. In business terms, this might mean that a marketing campaign applied to one customer doesn’t influence the purchasing behavior of other customers. b) No hidden variations of treatments: There is only one version of each treatment level. For instance, if we’re studying the effect of a new training program, all employees who receive the training get the same version of it. Strong Ignorability:\n\nThis assumption has two components: a) Unconfoundedness: Given the observed covariates, treatment assignment is independent of the potential outcomes. In other words, if we control for all relevant variables, whether a unit receives treatment or not is unrelated to what their outcomes would be under treatment or control. b) Positivity (or overlap): Every unit has a non-zero probability of receiving each treatment level. In a business context, this might mean that every customer has some chance of being exposed to a new marketing campaign, regardless of their characteristics.\n\nThese assumptions are crucial because they allow us to interpret the relationships uncovered by BART and its derivatives as causal, rather than merely associational. When these assumptions hold, we can confidently say that the differences in outcomes we observe between treated and untreated units are due to the treatment itself, rather than some other confounding factor.\nWe’ll explore three key models in this family: BART, Bayesian Causal Forests (BCF), and LongBet. Each of these models builds upon its predecessors, offering improvements in terms of causal effect estimation, handling of confounding, and applicability to different data structures.\nIn our exploration, we’ll be leveraging the stochtree R package (Herren et al. 2024), which implements these models using a technique called “warm-start” as introduced by Krantsevich, He, and Hahn (2023). The warm-start approach is a computational innovation that significantly improves the efficiency and effectiveness of these models, particularly for large datasets.\nThe warm-start technique works by using a fast approximation method (XBART) to generate initial tree structures, which are then used as starting points for the full Bayesian MCMC algorithm. This approach combines the speed of approximate methods with the statistical rigor of full Bayesian inference, resulting in models that are both computationally efficient and statistically robust.\nBy using warm-start, we can fit these sophisticated models to larger datasets and explore more complex causal relationships than was previously feasible. This makes these models particularly valuable for business data science applications, where we often deal with large, complex datasets and need to uncover nuanced causal relationships.\nLet’s begin our journey into the world of stochastic trees and their applications in causal inference, keeping in mind the critical assumptions that allow us to draw causal conclusions from these powerful tools.\n\n\n\n\n\n\nLearn more\n\n\n\n\nKrantsevich, He, and Hahn (2023) Stochastic Tree Ensembles for Estimating Heterogeneous Effects.\nHerren et al. (2024) Stochastic tree ensembles (XBART and BART) for supervised learning and causal inference.\n\n\n\n\n\n\n\n\n\nHerren, Drew, Richard Hahn, Jared Murray, Carlos Carvalho, and Jingyu He. 2024. Stochtree: Stochastic Tree Ensembles (XBART and BART) for Supervised Learning and Causal Inference. https://stochastictree.github.io/stochtree-r/.\n\n\nKrantsevich, Nikolay, Jingyu He, and P. Richard Hahn. 2023. “Stochastic Tree Ensembles for Estimating Heterogeneous Effects.” In Proceedings of the 26th International Conference on Artificial Intelligence and Statistics, edited by Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, 206:6120–31. Proceedings of Machine Learning Research. PMLR. https://proceedings.mlr.press/v206/krantsevich23a.html.",
    "crumbs": [
      "Stochastic Trees"
    ]
  },
  {
    "objectID": "bart.html",
    "href": "bart.html",
    "title": "16  Bayesian Additive Regression Trees (BART)",
    "section": "",
    "text": "16.1 BART: Bayesian Additive Regression Trees\nBART is a Bayesian nonparametric, machine learning, ensemble predictive modeling method introduced by Chipman, George, and McCulloch (2010). It can be expressed as:\n\\[\nY = \\sum_{j=1}^m g(X; T_j, M_j) + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2)\n\\]\nwhere \\(g(X; T_j, M_j)\\) represents the \\(j\\)-th regression tree with structure \\(T_j\\) and leaf node parameters \\(M_j\\). The model uses \\(m\\) trees, typically set to a large number (e.g., 200), with each tree acting as a weak learner. BART employs a carefully designed prior that encourages each tree to play a small role in the overall fit, resulting in a flexible yet robust model.\nHill (2011) proposed using BART for causal inference, recognizing that its flexibility in modeling complex response surfaces made it well-suited for estimating causal effects, particularly in observational studies with many covariates. The key idea was to use BART to model the response surface:\n\\[\nE[Y | X, Z] = f(X, Z)\n\\]\nwhere \\(Y\\) is the outcome, \\(X\\) are the covariates, and \\(Z\\) is the treatment indicator. The causal effect can then be estimated as:\n\\[\n\\begin{aligned}\n\\tau(x) & = E[Y | X=x, Z=1] - E[Y | X=x, Z=0] \\\\\n& = f(x, 1) - f(x, 0)\n\\end{aligned}\n\\]\nThis formulation leverages BART’s inherent ability to automatically capture intricate interactions and non-linear relationships, making it a potent tool for causal inference, especially in high-dimensional scenarios.\nThe effectiveness of BART in causal inference has been further validated in recent competitions. Thal and Finucane (2023) report on the 2022 American Causal Inference Conference (ACIC) data challenge, where BART-based methods were among the top performers, particularly for estimating heterogeneous treatment effects. They found that BART’s regularizing priors were especially effective in controlling error for subgroup estimates, even in small subgroups, and its flexibility in modeling confounding relationships was crucial for improved causal inference in complex scenarios.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bayesian Additive Regression Trees (BART)</span>"
    ]
  },
  {
    "objectID": "bart.html#example-with-a-single-covariate",
    "href": "bart.html#example-with-a-single-covariate",
    "title": "16  Bayesian Additive Regression Trees (BART)",
    "section": "16.2 Example with a Single Covariate",
    "text": "16.2 Example with a Single Covariate\nTo illustrate how BART can be used for estimating the impact of an intervention, Hill (2011) presents a simple example:\n\\[\n\\begin{aligned}\nZ &\\sim \\mbox{Bernoulli}(0.5) \\\\\nX | Z = 1 &\\sim N(40,10^2) \\\\\nX | Z = 0 &\\sim N(20,10^2) \\\\\nY(0) | X &\\sim N(72 + 3\\sqrt{X},1) \\\\\nY(1) | X &\\sim N(90 + exp(0.06X),1)\n\\end{aligned}\n\\]\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(broom) \n\n# 1. Define True Outcome Functions\nf_treated &lt;- function(x) 90 + exp(0.06 * x)\nf_control &lt;- function(x) 72 + 3 * sqrt(x)\n\n# 2. Visualize True Outcome Functions\nggplot(data.frame(x = 6:62), aes(x = x)) +  # Expanded x range for clarity\n  stat_function(fun = f_control, aes(color = \"Truth - Control\"), linewidth = 1) +  \n  stat_function(fun = f_treated, aes(color = \"Truth - Treatment\"), linewidth = 1) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  labs(title = \"True Outcome Functions\", x = \"X\", y = \"Y\", color = \"\") +\n  theme_bw() +   \n  theme(legend.position = \"bottom\")  \n\n\n\n\n\n\n\n\nWe can generate a sample from that data generating process as follows:\n\nset.seed(123)\nn_samples &lt;- 120\n\nsimulated_data &lt;- tibble(\n  treatment_group = sample(c(\"Treatment\", \"Control\"), size = n_samples, replace = TRUE),\n  is_treated = treatment_group == \"Treatment\",\n  X = if_else(is_treated, rnorm(n_samples, 40, 10), rnorm(n_samples, 20, 10)),\n  Y1 = rnorm(n_samples, mean = f_treated(X), sd = 1),  \n  Y0 = rnorm(n_samples, mean = f_control(X), sd = 1),  \n  Y = if_else(is_treated, Y1, Y0),\n  true_effect = Y1 - Y0\n)\n\n\n# 4. Visualize Simulated Data with True Functions\nggplot(simulated_data, aes(x = X, y = Y, color = treatment_group)) +\n  geom_point(size = 2) + \n  stat_function(fun = f_control, aes(color = \"Truth - Control\")) +\n  stat_function(fun = f_treated, aes(color = \"Truth - Treatment\")) +\n  scale_color_manual(values = c(\"Truth - Control\" = \"red\", \"Truth - Treatment\" = \"blue\")) +  # Mapping colors correctly\n  labs(title = \"Simulated Data with True Outcome Functions\", \n       x = \"X\", y = \"Y\", color = \"\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\") \n\n\n\n\n\n\n\ncat(glue::glue(\"The true Sample Average Treatment Effect is {round(mean(simulated_data$true_effect),2)}\"))\n\nThe true Sample Average Treatment Effect is 10.84\n\n\nNotice that in our sample, there is not very good overlap for low and high values of X. This means that we will have to do a lot of extrapolation when doing inference for those cases, which is a common challenge in causal inference. Now, suppose we just fit OLS to the model to try to estimate the average treatment effect:\n\nlinear_model &lt;- lm(Y ~ X + is_treated, data = simulated_data)\nlm_fit &lt;- broom::tidy(linear_model)\nlm_fit\n\n# A tibble: 3 × 5\n  term           estimate std.error statistic  p.value\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)      70.1      1.29       54.6  4.71e-85\n2 X                 0.715    0.0486     14.7  2.24e-28\n3 is_treatedTRUE    4.74     1.30        3.64 4.14e- 4\n\ncat(glue::glue(\"A linear model finds an Average Treatment Effect equal to {round(lm_fit$estimate[lm_fit$term=='is_treatedTRUE'],2)}\"))\n\nA linear model finds an Average Treatment Effect equal to 4.74\n\n\nIt’s important to note that the linear model is misspecified given the true nonlinear relationships, which contributes to its poor performance. Let’s add the findings from OLS to our plot:\n\nprediction_data &lt;- expand.grid(\n  X = seq(min(simulated_data$X), max(simulated_data$X), length.out = 1000),\n  is_treated = c(TRUE, FALSE)\n) %&gt;%\n  mutate(\n    treatment_group = if_else(is_treated, \"Treatment\", \"Control\"),\n    linear_prediction = predict(linear_model, newdata = .)\n  )\n\n# 6. Visualize Simulated Data, True Functions, and Linear Predictions\nggplot() +\n  geom_point(data = simulated_data, aes(x = X, y = Y, color = treatment_group), size = 2) +\n  stat_function(fun = f_control, aes(color = \"Truth - Control\")) +\n  stat_function(fun = f_treated, aes(color = \"Truth - Treatment\")) +\n  geom_line(data = prediction_data, aes(x = X, y = linear_prediction, \n                                        color = treatment_group), linetype = \"dashed\") +\n  scale_color_manual(values = c(\"Truth - Control\" = \"red\",  \n                                \"Truth - Treatment\" = \"blue\",\n                                \"Control\" = \"red\",           \n                                \"Treatment\" = \"blue\")) +    \n  labs(title = \"Simulated Data, True Functions, and Linear Model Predictions\",\n       x = \"X\", y = \"Y\", color = \"\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\") \n\n\n\n\n\n\n\n\nTo fit BART we can use the {stochtree} package.\n\nxstart &lt;- 20-15\nlength &lt;- 4\ntext_left &lt;- 26-15\nyVals &lt;- seq(110,130,by=4)\n\nX_train &lt;- simulated_data %&gt;% \n                    select(X, is_treated) %&gt;% \n                    as.matrix()\n\nX_test &lt;- prediction_data %&gt;% \n                    select(X, is_treated) %&gt;% \n                    as.matrix()\n\nbart_model &lt;-\n  stochtree::bart(X_train = X_train,\n                  X_test = X_test,\n                  y_train = simulated_data$Y,\n                  num_burnin = 1000,\n                  num_mcmc = 2000)\n\nAfter fitting, it is important to examine the traceplot of \\(\\sigma^2\\) to assess if the model has converged. We can do this by running the following code:\n\ntrace_plot_data &lt;-\n  tibble(\n    iteration = 1:length(bart_model$sigma2_samples),\n    sigma2_samples = bart_model$sigma2_samples\n  )\nggplot(aes(x = iteration, y = sigma2_samples), data = trace_plot_data) +\n  geom_line(color = \"blue\", alpha = 0.5) +\n  labs(title = \"Trace Plot for sigma^2\",\n       x = \"Iteration\",\n       y = \"sigma^2\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nNow we can plot the predictions from BART and compare them to the truth with the following code:\n\nprediction_data &lt;- prediction_data %&gt;%\n  mutate(bart_pred = rowMeans(bart_model$y_hat_test)) \n\n\nggplot() +\n  geom_point(data = simulated_data,\n             aes(x = X, y = Y, color = treatment_group),\n             size = 2) +\n  stat_function(fun = f_control, aes(color = \"Truth - Control\")) +\n  stat_function(fun = f_treated, aes(color = \"Truth - Treatment\")) +\n  scale_color_manual(\n    values = c(\n      \"Truth - Control\" = \"red\",\n      \"Truth - Treatment\" = \"blue\",\n      \"Control\" = \"red\",\n      \"Treatment\" = \"blue\"\n    )\n  ) +\n  labs(\n    title = \"Simulated Data, True Functions, and BART Predictions\",\n    x = \"X\",\n    y = \"Y\",\n    color = \"\"\n  ) +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  geom_line(data = prediction_data,\n            aes(x = X, y = bart_pred, color = treatment_group),\n            linetype = \"dashed\") +\n   scale_color_manual(\"\", values = c(\"red\", \"blue\",\"red\", \"blue\", \"red\", \"blue\", \"red\", \"blue\")) +\n   annotate(geom = \"segment\", x = xstart, y = yVals[1], xend = xstart+length, yend = yVals[1], color = \"red\") +\n   annotate(geom = \"text\", x = text_left, y = yVals[1], label = c(\"truth - control\"), hjust = 0) +\n   annotate(geom = \"segment\", x = xstart, y = yVals[2], xend = xstart+length, yend = yVals[2], color = \"blue\") +\n   annotate(geom = \"text\", x = text_left, y = yVals[2], label = c(\"truth - treatment\"), hjust = 0) +\n   geom_point(aes(x=xstart+length/2, y=yVals[3]), color = c(\"red\")) +\n   annotate(geom = \"text\", x = text_left, y = yVals[3], label = c(\"simulated data - control\"), hjust = 0) +\n   geom_point(aes(x=xstart+length/2, y=yVals[4]), color = c(\"blue\")) +\n   annotate(geom = \"text\", x = text_left, y = yVals[4], label = c(\"simulated data - treatment\"), hjust = 0) +\n   annotate(geom = \"segment\", x = xstart, y = yVals[5], xend = xstart+length, yend = yVals[5], color = \"red\", linetype = \"dashed\") +\n   annotate(geom = \"text\", x = text_left, y = yVals[5], label = c(\"BART - control\"), hjust = 0) +\n   annotate(geom = \"segment\", x = xstart, y = yVals[6], xend = xstart+length, yend = yVals[6], color = \"blue\", linetype = \"dashed\") +\n   annotate(geom = \"text\", x = text_left, y = yVals[6], label = c(\"BART - treatment\"), hjust = 0) \n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\nNotice that BART does a very good job when we have overlap between the treatment and control groups, but when extrapolating for high values of X, BART cannot get the true control curve right because it has no data in that region. This highlights the importance of understanding how any method works.\nOnce we have fitted the BART model, we can calculate the sample average treatment effect by predicting the outcome for every individual in our sample under both treatment and control conditions. The difference between these predictions gives us the posterior distribution of the treatment effect for each individual. The sample average treatment effect is then the mean of this posterior distribution.\n\nx0 &lt;- simulated_data %&gt;% mutate(is_treated=FALSE) %&gt;% select(X,is_treated)\nx1 &lt;- simulated_data %&gt;% mutate(is_treated=TRUE) %&gt;% select(X,is_treated)\n\npred0 &lt;- predict(bart_model, as.matrix(x0))\n\npred1 &lt;- predict(bart_model, as.matrix(x1))\n\ntau_draws &lt;- pred1$y_hat - pred0$y_hat  \nsate_draws &lt;- colMeans(tau_draws)\ncat(glue::glue(\"BART finds an Average Treatment Effect equal to {round(mean(sate_draws),2)}\"))\n\nBART finds an Average Treatment Effect equal to 9.79\n\n\nHowever, as we discussed before, point estimates are often not very useful for decision-making. For instance, we might make different decisions if the impact of the intervention is more than 9, between 0 and 9, or less than 0. Calculating probabilities for different effect sizes is more useful for decision-making than point estimates alone because it provides a more nuanced understanding of the potential outcomes and the uncertainty associated with our estimates. ::: {.content-visible when-format=“pdf”} We can easily calculate these probabilities using the draws from the posterior probability that we just calculated. ::: ::: {.content-visible when-format=“html”} We can easily calculate these probabilities using the posterior draws and visualize them:\n\nvizdraws::vizdraws(\n  posterior = sate_draws,\n  breaks = c(0, 9),\n  break_names = c(\"Discontinue\", \"Continue\", \" Expand\")\n)\n\n\n\n\n\n:::\n\n\n\n\n\n\nLearn more\n\n\n\nHill (2011) Bayesian nonparametric modeling for causal inference.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bayesian Additive Regression Trees (BART)</span>"
    ]
  },
  {
    "objectID": "bart.html#accelerated-bart-xbart",
    "href": "bart.html#accelerated-bart-xbart",
    "title": "16  Bayesian Additive Regression Trees (BART)",
    "section": "16.3 Accelerated BART (XBART)",
    "text": "16.3 Accelerated BART (XBART)\nTODO\n\n\n\n\n\n\nChipman, Hugh A., Edward I. George, and Robert E. McCulloch. 2010. “BART: Bayesian additive regression trees.” The Annals of Applied Statistics 4 (1): 266–98. https://doi.org/10.1214/09-AOAS285.\n\n\nHill, Jennifer L. 2011. “Bayesian Nonparametric Modeling for Causal Inference.” Journal of Computational and Graphical Statistics 20 (1): 217–40. https://doi.org/10.1198/jcgs.2010.08162.\n\n\nThal, Dan RC, and Mariel M Finucane. 2023. “Causal Methods Madness: Lessons Learned from the 2022 ACIC Competition to Estimate Health Policy Impacts.” Observational Studies 9 (3): 3–27. https://doi.org/110.1353/obs.2023.0023.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bayesian Additive Regression Trees (BART)</span>"
    ]
  },
  {
    "objectID": "bcf.html",
    "href": "bcf.html",
    "title": "17  Bayesian Causal Forest (BCF)",
    "section": "",
    "text": "17.1 Introduction to Bayesian Causal Forest\nWhile BART has proven to be a powerful tool for causal inference, it has some limitations when applied to heterogeneous treatment effect estimation. To address these shortcomings, Hahn, Murray, and Carvalho (2020) introduced the Bayesian Causal Forest (BCF) model. BCF builds upon BART’s foundation but incorporates key modifications that make it particularly well-suited for estimating heterogeneous treatment effects.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bayesian Causal Forest (BCF)</span>"
    ]
  },
  {
    "objectID": "bcf.html#the-bcf-model",
    "href": "bcf.html#the-bcf-model",
    "title": "17  Bayesian Causal Forest (BCF)",
    "section": "17.2 The BCF Model",
    "text": "17.2 The BCF Model\nThe BCF model can be expressed as: \\[\nY_i = \\mu(x_i, \\hat{\\pi}(x_i)) + \\tau(x_i)z_i + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\n\\]\nwhere:\n\n\\(Y_i\\) is the outcome for individual \\(i\\)\n\\(x_i\\) are the covariates\n\\(z_i\\) is the treatment indicator\n\\(\\hat{\\pi}(x_i)\\) is an estimate of the propensity score\n\\(\\mu(\\cdot)\\) is the prognostic function\n\\(\\tau(\\cdot)\\) is the treatment effect function\n\nThe key innovation of BCF lies in its separation of the prognostic function \\(\\mu(\\cdot)\\) and the treatment effect function \\(\\tau(\\cdot)\\). Both functions are modeled using BART, but with different priors that reflect their distinct roles.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bayesian Causal Forest (BCF)</span>"
    ]
  },
  {
    "objectID": "bcf.html#key-features-of-bcf",
    "href": "bcf.html#key-features-of-bcf",
    "title": "17  Bayesian Causal Forest (BCF)",
    "section": "17.3 Key Features of BCF",
    "text": "17.3 Key Features of BCF\n\nSeparation of Prognostic and Treatment Effects: By modeling \\(\\mu(\\cdot)\\) and \\(\\tau(\\cdot)\\) separately, BCF allows for different levels of regularization for each component. This is particularly useful when the treatment effect is expected to be simpler or more homogeneous than the overall prognostic effect.\nInclusion of Propensity Score: The inclusion of \\(\\hat{\\pi}(x_i)\\) in the prognostic function helps to mitigate issues related to regularization-induced confounding, which can occur when strong confounding is present.\nTargeted Regularization: BCF employs a prior on the treatment effect function that encourages shrinkage towards homogeneous effects. This can lead to more stable and accurate estimates, especially when the true treatment effect heterogeneity is modest.\nHandling of Targeted Selection: BCF is designed to perform well in scenarios where treatment assignment is based on expected outcomes under control, a phenomenon referred to as “targeted selection”.\nImproved Treatment Effect Estimation: BCF often yields more accurate and stable estimates of conditional average treatment effects (CATEs), especially in scenarios with strong confounding.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bayesian Causal Forest (BCF)</span>"
    ]
  },
  {
    "objectID": "bcf.html#examples",
    "href": "bcf.html#examples",
    "title": "17  Bayesian Causal Forest (BCF)",
    "section": "17.4 Examples",
    "text": "17.4 Examples\nLet’s consider the following data generating process from Hahn, Murray, and Carvalho (2020), which is also covered in one of the vignettes from Herren et al. (2024).\n\\[\n\\begin{aligned}\ny &= \\mu(X) + \\tau(X) Z + \\epsilon\\\\\n\\epsilon &\\sim N\\left(0,\\sigma^2\\right)\\\\\n\\mu(X) &= 1 + g(X) + 6 \\lvert X_3 - 1 \\rvert\\\\\n\\tau(X) &= 1 + 2 X_2 X_4\\\\\ng(X) &= \\mathbb{I}(X_5=1) \\times 2 - \\mathbb{I}(X_5=2) \\times 1 - \\mathbb{I}(X_5=3) \\times 4\\\\\ns_{\\mu} &= \\sqrt{\\mathbb{V}(\\mu(X))}\\\\\n\\pi(X) &= 0.8 \\phi\\left(\\frac{3\\mu(X)}{s_{\\mu}}\\right) - \\frac{X_1}{2} + \\frac{2U+1}{20}\\\\\nX_1,X_2,X_3 &\\sim N\\left(0,1\\right)\\\\\nX_4 &\\sim \\text{Bernoulli}(1/2)\\\\\nX_5 &\\sim \\text{Categorical}(1/3,1/3,1/3)\\\\\nU &\\sim \\text{Uniform}\\left(0,1\\right)\\\\\nZ &\\sim \\text{Bernoulli}\\left(\\pi(X)\\right)\n\\end{aligned}\n\\]\nLet’s generate data from this DGP and fit a BCF model using the {stochtree} package:\n\n# Define the functions based on the provided model\ng &lt;- function(x5) {\n  return(ifelse(x5 == 1, 2, ifelse(x5 == 2, -1, ifelse(x5 == 3, -4, 0))))\n}\n\nmu &lt;- function(X) {\n  return(1 + g(X[, 5]) + 6 * abs(X[, 3] - 1))\n}\n\ntau &lt;- function(X) {\n  return(1 + 2 * X[, 2] * X[, 4])\n}\n\n# Set parameters\nn &lt;- 500\nsnr &lt;- 3\n\n# Generate covariates\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\nx3 &lt;- rnorm(n)\nx4 &lt;- as.numeric(rbinom(n, 1, 0.5))\nx5 &lt;- as.numeric(sample(1:3, n, replace = TRUE))\n\nX &lt;- cbind(x1, x2, x3, x4, x5)\ncolnames(X) &lt;- paste0(\"X\", 1:5)\n\n# Calculate mu(X) and tau(X)\nmu_x &lt;- mu(X)\ntau_x &lt;- tau(X)\n\n# Calculate s_mu\ns_mu &lt;- sd(mu_x)\n\n# Calculate pi(X)\npi_x &lt;- 0.8 * pnorm((3 * mu_x / s_mu) - 0.5 * X[, 1]) + 0.05 + runif(n) / 10\n\n# Generate treatment assignment Z\nZ &lt;- rbinom(n, 1, pi_x)\n\n# Generate the outcome y\nE_XZ &lt;- mu_x + Z * tau_x\ny &lt;- E_XZ + rnorm(n, 0, 1) * (sd(E_XZ) / snr)\n\n# Convert X to data frame and factorize categorical variables\nX &lt;- as.data.frame(X)\nX$X4 &lt;- factor(X$X4, ordered = TRUE)\nX$X5 &lt;- factor(X$X5, ordered = TRUE)\n\n# Split data into test and train sets\ntest_set_pct &lt;- 0.2\nn_test &lt;- round(test_set_pct * n)\nn_train &lt;- n - n_test\n\ntest_inds &lt;- sort(sample(1:n, n_test, replace = FALSE))\ntrain_inds &lt;- setdiff(1:n, test_inds)\n\nX_test &lt;- X[test_inds, ]\nX_train &lt;- X[train_inds, ]\npi_test &lt;- pi_x[test_inds]\npi_train &lt;- pi_x[train_inds]\nZ_test &lt;- Z[test_inds]\nZ_train &lt;- Z[train_inds]\ny_test &lt;- y[test_inds]\ny_train &lt;- y[train_inds]\nmu_test &lt;- mu_x[test_inds]\nmu_train &lt;- mu_x[train_inds]\ntau_test &lt;- tau_x[test_inds]\ntau_train &lt;- tau_x[train_inds]\n\nTo sample using {stochtree} we can run the following code:\n\nlibrary(stochtree)\nnum_gfr &lt;- 10\nnum_burnin &lt;- 0\nnum_mcmc &lt;- 1000\nnum_samples &lt;- num_gfr + num_burnin + num_mcmc\nbcf_model_warmstart &lt;- bcf(\n  X_train = X_train,\n  Z_train = Z_train,\n  y_train = y_train,\n  pi_train = pi_train,\n  X_test = X_test,\n  Z_test = Z_test,\n  pi_test = pi_test,\n  num_gfr = num_gfr,\n  num_burnin = num_burnin,\n  num_mcmc = num_mcmc,\n  sample_sigma_leaf_mu = F,\n  sample_sigma_leaf_tau = F\n)\n\nAfter fitting the model, it’s crucial to assess convergence. One way to do this is by examining the traceplot for \\(\\sigma^2\\):\n\nlibrary(ggplot2)\ndf &lt;- tibble::tibble(\n  sample = 1:length(bcf_model_warmstart$sigma2_samples),\n  sigma2_samples = bcf_model_warmstart$sigma2_samples\n)\n\n\n# Create the plot\nggplot(df, aes(x = sample, y = sigma2_samples)) +\n  geom_line() +\n  labs(\n    x = \"Sample\",\n    y = expression(sigma^2),\n    title = \"Global Variance Parameter\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe traceplot shows no obvious trends, suggesting that the MCMC chain has likely converged. Next, we can evaluate the model’s performance in predicting the prognostic function:\n\ndf &lt;- tibble::tibble(\n  predicted = rowMeans(bcf_model_warmstart$mu_hat_test),\n  actual = mu_test\n)\n\n# Create the plot\nggplot(df, aes(x = predicted, y = actual)) +\n  geom_point() +\n  geom_abline(\n    slope = 1,\n    intercept = 0,\n    color = \"red\",\n    linetype = \"dashed\",\n    linewidth = 1\n  ) +\n  labs(x = \"Predicted\",\n       y = \"Actual\",\n       title = \"Prognostic Function\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot shows a strong correlation between predicted and actual values of the prognostic function, indicating that the BCF model has captured the nonlinear relationships in the data well.\nGiven that we know the true data generating process, we can assess how well the model estimates the true treatment effects:\n\ndf &lt;- tibble::tibble(\n  predicted = rowMeans(bcf_model_warmstart$tau_hat_test),\n  actual = tau_test\n)\n\n# Calculate the limits for the axes\nlimits &lt;- range(c(df$predicted, df$actual))\n\n# Create the plot\nggplot(df, aes(x = predicted, y = actual)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  labs(\n    x = \"Predicted\",\n    y = \"Actual\",\n    title = \"Treatment Effect\"\n  ) +\n  coord_fixed(ratio = 1, xlim = limits, ylim = limits)  +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFinally, let’s check our coverage:\n\ntest_lb &lt;- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.025)\ntest_ub &lt;- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.975)\ncover &lt;- (\n    (test_lb &lt;= tau_x[test_inds]) & \n    (test_ub &gt;= tau_x[test_inds])\n)\n\ncat(\"95% Credible Interval Coverage Rate:\", round(mean(cover) * 100, 2), \"%\\n\")\n\n95% Credible Interval Coverage Rate: 76 %",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bayesian Causal Forest (BCF)</span>"
    ]
  },
  {
    "objectID": "bcf.html#conclusion",
    "href": "bcf.html#conclusion",
    "title": "17  Bayesian Causal Forest (BCF)",
    "section": "17.5 Conclusion",
    "text": "17.5 Conclusion\nBayesian Causal Forest represents a significant advancement in the application of tree-based methods to causal inference. By building upon the strengths of BART and addressing some of its limitations, BCF offers a powerful tool for estimating heterogeneous treatment effects. Its ability to handle strong confounding and targeted selection, coupled with its interpretability, makes it a valuable addition to the causal inference toolkit.\nHowever, like all methods, BCF has its limitations. It assumes that all relevant confounders are observed, and its performance can degrade in scenarios with limited overlap between treated and control units. As always in causal inference, careful consideration of the problem at hand and the assumptions of the method is crucial.\n\n\n\n\n\n\nLearn more\n\n\n\nHahn, Murray, and Carvalho (2020) Bayesian regression tree models for causal inference: Regularization, confounding, and heterogeneous effects (with discussion).\n\n\n\n\n\n\n\n\nHahn, P Richard, Jared S Murray, and Carlos M Carvalho. 2020. “Bayesian Regression Tree Models for Causal Inference: Regularization, Confounding, and Heterogeneous Effects (with Discussion).” Bayesian Analysis 15 (3): 965–1056. https://doi.org/10.1214/19-BA1195.\n\n\nHerren, Drew, Richard Hahn, Jared Murray, Carlos Carvalho, and Jingyu He. 2024. Stochtree: Stochastic Tree Ensembles (XBART and BART) for Supervised Learning and Causal Inference. https://stochastictree.github.io/stochtree-r/.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bayesian Causal Forest (BCF)</span>"
    ]
  },
  {
    "objectID": "longbet.html",
    "href": "longbet.html",
    "title": "18  LongBet: Heterogeneous Treatment Effects in Panel Data",
    "section": "",
    "text": "18.1 Introduction to LongBet\nAs businesses and policymakers increasingly rely on longitudinal data to make causal inferences, there’s a growing need for methods that can handle the complexities of panel data while estimating heterogeneous treatment effects. LongBet, introduced by Wang, Martinez, and Hahn (2024), fills this gap by extending the Bayesian Causal Forest (BCF) model to panel data settings.\nLongBet is particularly suited for short panel data with large cross-sectional samples and observed confounders. Unlike traditional difference-in-differences methods that often rely on the parallel trends assumption, LongBet leverages observed confounders to impute potential outcomes and identify treatment effects. LongBet models the data generating process as follows:\n\\[\nY_{it} = \\alpha\\mu(X_i, T=t) + \\beta_S\\nu(X_i, S_{it}, T=t) + \\epsilon_{it}\n\\]\nwhere:\nBoth \\(\\mu(\\cdot)\\) and \\(\\nu(\\cdot)\\) are modeled using XBART and considering splits on the time dimension.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LongBet: Heterogeneous Treatment Effects in Panel Data</span>"
    ]
  },
  {
    "objectID": "longbet.html#introduction-to-longbet",
    "href": "longbet.html#introduction-to-longbet",
    "title": "18  LongBet: Heterogeneous Treatment Effects in Panel Data",
    "section": "",
    "text": "\\(Y_{it}\\) is the outcome for individual \\(i\\) at time \\(t\\)\n\\(X_i\\) are time-invariant covariates\n\\(S_{it}\\) is the time elapsed since treatment adoption\n\\(T\\) is the time period\n\\(\\mu(\\cdot)\\) is the prognostic function\n\\(\\nu(\\cdot)\\) is the treatment effect function\n\\(\\beta_S\\) is a Gaussian process factor capturing the general trend of treatment effects\n\\(\\epsilon_{it}\\) is an independent Gaussian error term",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LongBet: Heterogeneous Treatment Effects in Panel Data</span>"
    ]
  },
  {
    "objectID": "longbet.html#key-features-of-longbet",
    "href": "longbet.html#key-features-of-longbet",
    "title": "18  LongBet: Heterogeneous Treatment Effects in Panel Data",
    "section": "18.2 Key Features of LongBet",
    "text": "18.2 Key Features of LongBet\n\nFlexibility in Trend Modeling: LongBet doesn’t require the parallel trends assumption, making it suitable for scenarios where this assumption may not hold.\nHandling of Staggered Adoption: The model can accommodate treatments adopted at different times across units.\nSeparation of Prognostic and Treatment Effects: Like BCF, LongBet separates the modeling of prognostic and treatment effects, allowing for different regularization strategies.\nTime-Varying Treatment Effects: The model captures how treatment effects may change over time since adoption.\nBayesian Uncertainty Quantification: As a Bayesian method, LongBet provides credible intervals for conditional treatment effects.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LongBet: Heterogeneous Treatment Effects in Panel Data</span>"
    ]
  },
  {
    "objectID": "longbet.html#comparison-with-traditional-panel-methods",
    "href": "longbet.html#comparison-with-traditional-panel-methods",
    "title": "18  LongBet: Heterogeneous Treatment Effects in Panel Data",
    "section": "18.3 Comparison with Traditional Panel Methods",
    "text": "18.3 Comparison with Traditional Panel Methods\nLongBet offers several advantages over traditional panel data methods:\n\nRelaxed Assumptions: Unlike difference-in-differences, LongBet doesn’t rely on the parallel trends assumption.\nHeterogeneous Effects: LongBet can capture complex, nonlinear heterogeneity in treatment effects.\nFlexible Functional Form: The use of tree-based models allows for flexible modeling of the relationship between outcomes, covariates, and time.\nUncertainty Quantification: The Bayesian framework provides natural uncertainty estimates for all quantities of interest.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LongBet: Heterogeneous Treatment Effects in Panel Data</span>"
    ]
  },
  {
    "objectID": "longbet.html#example-application",
    "href": "longbet.html#example-application",
    "title": "18  LongBet: Heterogeneous Treatment Effects in Panel Data",
    "section": "18.4 Example Application",
    "text": "18.4 Example Application\nTODO",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LongBet: Heterogeneous Treatment Effects in Panel Data</span>"
    ]
  },
  {
    "objectID": "longbet.html#conclusion",
    "href": "longbet.html#conclusion",
    "title": "18  LongBet: Heterogeneous Treatment Effects in Panel Data",
    "section": "18.5 Conclusion",
    "text": "18.5 Conclusion\nLongBet represents a significant advancement in causal inference for panel data. By combining the flexibility of tree-based methods with the ability to handle longitudinal data, it offers a powerful tool for estimating heterogeneous treatment effects in complex, real-world scenarios.\nHowever, like all methods, LongBet has its limitations. It assumes that all relevant confounders are observed and time-invariant. It may also struggle in scenarios with limited overlap between treated and control units across time. As always in causal inference, careful consideration of the problem at hand and the assumptions of the method is crucial.\n\n\n\n\n\n\nWang, Meijia, Ignacio Martinez, and P Richard Hahn. 2024. “LongBet: Heterogeneous Treatment Effect Estimation in Panel Data.” arXiv Preprint arXiv:2406.02530. https://doi.org/arXiv:2406.02530.",
    "crumbs": [
      "Stochastic Trees",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LongBet: Heterogeneous Treatment Effects in Panel Data</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "19  References",
    "section": "",
    "text": "Abadie, Alberto. 2021. “Using Synthetic Controls: Feasibility,\nData Requirements, and Methodological Aspects.” Journal of\nEconomic Literature 59 (2): 391–425.\n\n\nAbadie, Alberto, Alexis Diamond, and Jens Hainmueller. 2015.\n“Comparative Politics and the Synthetic Control Method.”\nAmerican Journal of Political Science 59 (2): 495–510.\n\n\nAbadie, Alberto, and Jaume Vives-i-Bastida. 2022. “Synthetic\nControls in Action.” https://arxiv.org/abs/2203.06279.\n\n\nAthey, Susan, Raj Chetty, Guido W Imbens, and Hyunseung Kang. 2019.\n“The Surrogate Index: Combining Short-Term Proxies to Estimate\nLong-Term Treatment Effects More Rapidly and Precisely.” National\nBureau of Economic Research.\n\n\nAthey, Susan, and Guido W Imbens. 2017. “The State of Applied\nEconometrics: Causality and Policy Evaluation.” Journal of\nEconomic Perspectives 31 (2): 3–32.\n\n\nBagby, Emilie, and Anu Rangarajan. 2023. Using\nRapid-Cycle Evaluation to Improve Program Design and\nDelivery. Oxford University Press. https://doi.org/10.1093/oxfordhb/9780190059668.013.7.\n\n\nBrodersen, Kay H, Fabian Gallusser, Jim Koehler, Nicolas Remy, and\nSteven L Scott. 2015. “Inferring Causal Impact Using Bayesian\nStructural Time-Series Models.” Annals of Applied\nStatistics 9: 247–74. https://doi.org/10.1214/14-AOAS788.\n\n\nBürkner, Paul-Christian. 2017. “brms:\nAn R Package for Bayesian Multilevel Models\nUsing Stan.” Journal of Statistical\nSoftware 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\nCarpenter, Bob, Andrew Gelman, Matthew D. Hoffman, Daniel Lee, Ben\nGoodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li,\nand Allen Riddell. 2017. “Stan: A Probabilistic Programming\nLanguage.” Journal of Statistical Software 76 (1): 1–32.\nhttps://doi.org/10.18637/jss.v076.i01.\n\n\nChandler, Jesse J, Ignacio Martinez, Mariel M Finucane, Jeffrey G\nTerziev, and Alexandra M Resch. 2020. “Speaking on Data’s Behalf:\nWhat Researchers Say and How Audiences Choose.” Evaluation\nReview 44 (4): 325–53.\n\n\nChen, Jiafeng, and David M Ritzwoller. 2023. “Semiparametric\nEstimation of Long-Term Treatment Effects.” Journal of\nEconometrics 237 (2): 105545.\n\n\nChernozhukov, Victor, Christian Hansen, Nathan Kallus, Martin Spindler,\nand Vasilis Syrgkanis. 2024. “Applied Causal Inference Powered by\nML and AI” 12 (1): 338. https://causalml-book.org/assets/chapters/CausalML_chap_2.pdf.\n\n\nChipman, Hugh A., Edward I. George, and Robert E. McCulloch. 2010.\n“BART: Bayesian additive regression\ntrees.” The Annals of Applied Statistics 4 (1):\n266–98. https://doi.org/10.1214/09-AOAS285.\n\n\nCunningham, Scott. 2021. “Potential Outcomes Causal Model.”\nIn Causal Inference: The Mixtape. Yale University Press. https://mixtape.scunning.com/04-potential_outcomes.\n\n\nDing, Peng, and Fan Li. 2018. “Causal Inference.”\nStatistical Science 33 (2): 214–37. https://projecteuclid.org/journals/statistical-science/volume-33/issue-2/Causal-Inference-A-Missing-Data-Perspective/10.1214/18-STS645.pdf.\n\n\nFinucane, Mariel McKenzie, Ignacio Martinez, and Scott Cody. 2018.\n“What Works for Whom? A Bayesian Approach to Channeling Big Data\nStreams for Public Program Evaluation.” American Journal of\nEvaluation 39 (1): 109–22. https://journals.sagepub.com/doi/abs/10.1177/1098214017737173.\n\n\nGelman, A., J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D.\nB. Rubin. 2013. “Parallel Experiments in Eight Schools.” In\nBayesian Data Analysis, Third Edition, 119. Chapman &\nHall/CRC Texts in Statistical Science. New York: Taylor & Francis.\nhttps://books.google.com/books?id=ZXL6AQAAQBAJ.\n\n\nGigerenzer, Gerd, Stefan Krauss, and Oliver Vitouch. 2004. “The\nNull Ritual.” The Sage Handbook of Quantitative Methodology\nfor the Social Sciences, 391–408.\n\n\nGoodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2020.\n“Rstanarm: Bayesian Applied Regression Modeling via\nStan.” https://mc-stan.org/rstanarm.\n\n\nHahn, P Richard, Jared S Murray, and Carlos M Carvalho. 2020.\n“Bayesian Regression Tree Models for Causal Inference:\nRegularization, Confounding, and Heterogeneous Effects (with\nDiscussion).” Bayesian Analysis 15 (3): 965–1056. https://doi.org/10.1214/19-BA1195.\n\n\nHerren, Drew, Richard Hahn, Jared Murray, Carlos Carvalho, and Jingyu\nHe. 2024. Stochtree: Stochastic Tree Ensembles (XBART and BART) for\nSupervised Learning and Causal Inference. https://stochastictree.github.io/stochtree-r/.\n\n\nHill, Jennifer L. 2011. “Bayesian Nonparametric Modeling for\nCausal Inference.” Journal of Computational and Graphical\nStatistics 20 (1): 217–40. https://doi.org/10.1198/jcgs.2010.08162.\n\n\nHoekstra, Rink, Richard D Morey, Jeffrey N Rouder, and Eric-Jan\nWagenmakers. 2014. “Robust Misinterpretation of Confidence\nIntervals.” Psychonomic Bulletin & Review 21:\n1157–64. https://doi.org/10.3758/s13423-013-0572-3.\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.”\nJournal of the American Statistical Association 81 (396):\n945–60. https://doi.org/10.2307/2289064.\n\n\nImbens, Guido, Nathan Kallus, Xiaojie Mao, and Yuhao Wang. 2022.\n“Long-Term Causal Inference Under Persistent Confounding via Data\nCombination.” arXiv Preprint arXiv:2202.07234.\n\n\nKassler, Daniel, Ira Nichols-Barrer, and Mariel Finucane. 2018.\n“Beyond ‘Treatment Versus Control’: How Bayesian\nAnalysis Makes Factorial Experiments Feasible in Education\nResearch.” https://doi.org/10.1177/0193841X18818903.\n\n\nKrantsevich, Nikolay, Jingyu He, and P. Richard Hahn. 2023.\n“Stochastic Tree Ensembles for Estimating Heterogeneous\nEffects.” In Proceedings of the 26th International Conference\non Artificial Intelligence and Statistics, edited by Francisco\nRuiz, Jennifer Dy, and Jan-Willem van de Meent, 206:6120–31. Proceedings\nof Machine Learning Research. PMLR. https://proceedings.mlr.press/v206/krantsevich23a.html.\n\n\nLi, Fan, Peng Ding, and Fabrizia Mealli. 2023. “Bayesian Causal\nInference: A Critical Review.” Philosophical Transactions of\nthe Royal Society A 381 (2247): 20220153. https://doi.org/10.1098/rsta.2022.0153.\n\n\nManski, Charles F. 2020. “The Lure of Incredible\nCertitude.” Economics & Philosophy 36 (2): 216–45.\nhttps://www.nber.org/system/files/working_papers/w24905/w24905.pdf.\n\n\nMartinez, Ignacio, and Jaume Vives-i-Bastida. 2023. “Bayesian and\nFrequentist Inference for Synthetic Controls.” https://arxiv.org/abs/2206.01779.\n\n\nNeyman, Jersey. 1923. “Sur Les Applications de La\nThéorie Des Probabilités Aux Experiences\nAgricoles: Essai Des Principes.” Roczniki Nauk\nRolniczych 10 (1): 1–51.\n\n\nRubin, Donald B. 1974. “Estimating Causal Effects of Treatments in\nRandomized and Nonrandomized Studies.” Journal of Educational\nPsychology 66 (5): 688. http://www.fsb.muohio.edu/lij14/420_paper_Rubin74.pdf.\n\n\n———. 1978. “Bayesian Inference for Causal Effects: The Role of\nRandomization.” The Annals of Statistics, 34–58. https://www.jstor.org/stable/2958688.\n\n\nThal, Dan RC, and Mariel M Finucane. 2023. “Causal Methods\nMadness: Lessons Learned from the 2022 ACIC Competition to Estimate\nHealth Policy Impacts.” Observational Studies 9 (3):\n3–27. https://doi.org/110.1353/obs.2023.0023.\n\n\nThaler, Richard H, and Cass R Sunstein. 2009. Nudge: Improving\nDecisions about Health, Wealth, and Happiness. Penguin.\n\n\n———. 2021. Nudge: The Final Edition. Yale University Press.\n\n\nWainer, Howard. 2007. “The Most Dangerous Equation.”\nAmerican Scientist 95 (3): 249.\n\n\nWang, Meijia, Ignacio Martinez, and P Richard Hahn. 2024.\n“LongBet: Heterogeneous Treatment Effect Estimation in Panel\nData.” arXiv Preprint arXiv:2406.02530. https://doi.org/arXiv:2406.02530.\n\n\nWasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA\nStatement on p-Values: Context, Process, and Purpose.” The\nAmerican Statistician. Taylor & Francis. https://www.tandfonline.com/doi/pdf/10.1080/00031305.2016.1154108.\n\n\nWWC. 2020. “What Works Clearinghouse Baseline Equivalence\nStandard.” U.S. Department of Education, Institute of Education\nSciences, National Center for Education Evaluation; Regional Assistance.\nhttps://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/WWC-Baseline-Brief-v6_508.pdf.\n\n\nZhang, Vickie, Michael Zhao, Anh Le, and Nathan Kallus. 2023.\n“Evaluating the Surrogate Index as a Decision-Making Tool Using\n200 a/b Tests at Netflix.” arXiv Preprint\narXiv:2311.11922.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>References</span>"
    ]
  }
]