{
  "hash": "a85d53ff878daaf7a6c906c8d203cc3a",
  "result": {
    "engine": "knitr",
    "markdown": "<style>\na.glossary {\n  color: purple;\n  text-decoration: underline;\n  cursor: help;\n  position: relative;\n}\n\n/* only needed for popup = \"click\" */\n/* popup-definition */\na.glossary .def {\n  display: none;\n  position: absolute;\n  z-index: 1;\n  width: 200px;\n  bottom: 100%;\n  left: 50%;\n  margin-left: -100px;\n  background-color: #333;\n  color: white;\n  padding: 5px;\n  border-radius: 6px;\n}\n/* show on click */\na.glossary:active .def {\n  display: inline-block;\n}\n/* triangle arrow */\na.glossary:active .def::after {\n  content: ' ';\n  position: absolute;\n  top: 100%;\n  left: 50%;\n  margin-left: -5px;\n  border-width: 5px;\n  border-style: solid;\n  border-color: #333 transparent transparent transparent;\n}\n</style>\n\n\n\n## A Decisions First Framework\n\nIn the realm of <a class='glossary' title='Describes decision-making that is informed by the analysis and interpretation of data.'>data-driven</a> decision-making, it's tempting to\nsuccumb to the allure of the \"null ritual.\" This ritual, as outlined in\nGigerenzer et al. (2004), involves a rigid adherence to Null Hypothesis\nSignificance Testing (NHST), a statistical method used to determine if a result\nis due to chance. Often, this is done without a clear understanding of its\nunderlying assumptions or limitations. It's a bit like blindly following a\nrecipe without tasting the ingredients – you might end up with a statistically\nsignificant result, but it may not be a meaningful or useful one.\n\nInstead of mindlessly performing this ritual, we should adopt a more thoughtful\nand purposeful approach. A <a class='glossary' title='A decision-making approach that prioritizes clearly defining the decision to be made before gathering and analyzing data.'>Decisions First Framework</a> offers a\nrefreshing alternative, putting the focus squarely on the decisions we need to\nmake and using data as a tool to illuminate the path forward. This framework\nhelps us avoid several pitfalls, including the\n<a class='glossary' title='The tendency to present research findings with unwarranted certainty, often driven by pressure to provide clear-cut answers.'>Lure of Incredible Certitude</a>, a concept described by\n@manski2020lure. This refers to the tendency to present research findings with\nunwarranted certainty, often driven by incentives to provide clear-cut answers\neven when the data is inconclusive or riddled with uncertainty.\n\nOne crucial reason for this shift is that NHST doesn't always align with the\npractical questions businesses need to answer. Furthermore, there's a widespread\nmisinterpretation of <a class='glossary' title='the probability, assuming a certain statistical model, that a statistical summary of the data (such as the sample mean difference between two groups) would be equal to or more extreme than its observed value. While a P-value indicates how “incompatible” the data are with a specified statistical model, it does not measure the probability that the hypothesis under study is true nor does it measure the probability that the data were produced by random chance alone.'>p-values</a>. The American\nStatistical Association wrote a rare statement in 2016 to raise the alarm over\nthis problem [see @wasserstein2016asa].\nThey noted that \"practices that reduce data analysis or scientific\ninference to mechanical 'bright-line' rules (such as 'p \\< 0.05') for justifying\nscientific claims or conclusions can lead to erroneous beliefs and poor decision\nmaking.\" They further clarified that \"a p-value does not measure the probability\nthat the studied hypothesis is true, or the probability that the data were\nproduced by random chance alone.\"\n\nThe \"Decisions First\" framework offers an antidote to the null ritual by\nembracing a Bayesian perspective. This approach treats learning from data as a\ncontinuous process, not a binary \"accept/reject\" decision based on a single\np-value. Rather than fixating on point estimates, p-values, or confidence\nintervals, it emphasizes estimating probabilities that are relevant to the\ndecisions at hand. It acknowledges the inherent uncertainty in data and seeks to\nquantify and integrate it into the decision-making process. In essence,\n<a class='glossary' title='A statistical approach that allows for updating beliefs or probabilities based on new evidence, rather than relying on fixed hypotheses.'>Bayesian Statistics</a> allows us to use data to reallocate\ncredibility across various possibilities.\n\n### The Framework  {.unnumbered}\n\n#### Define the Decision(s): The Cornerstone of Data-Driven Choices {.unnumbered}\n\nThe first, and arguably most crucial, step is to **clearly articulate the\ndecision(s) you need to make.** This might involve launching a new product,\nadjusting pricing strategies, or optimizing marketing campaigns. Avoid vague\nstatements and strive for specificity. For instance, instead of saying, \"We need\nto improve customer satisfaction,\" refine it to, \"Should we implement a chatbot\nto reduce customer wait times and boost satisfaction scores?\"\n\nA well-structured decision question can be surprisingly simple, often following\nthe format \"Does A do B among C compared to D?\"\n\n  - A: The intervention or action under evaluation (e.g., chatbot, new pricing).\n  - B: Your clear definition of success (e.g., reduce wait times, increase\n    sales).\n  - C: The target population (e.g., all customers, a specific segment).\n  - D: The alternative or baseline for comparison (e.g., no chatbot, current\n    pricing).\n\nHowever, sometimes you'll encounter more nuanced questions like \"What works for\nwhom?\" These situations involve evaluating multiple alternatives with the\nunderstanding that different options might be optimal for different groups\nwithin your population.\n\n#### Formulate Data-Driven Questions: Asking the Right Questions  {.unnumbered}\n\nWith your decision clearly defined, it's time to craft questions that data can\nanswer and that directly inform your decision. These questions should be\nfocused, actionable, and revolve around\n<a class='glossary' title='A predetermined level of effect or change that is considered meaningful for decision-making purposes.'>meaningful threshold</a>,\nrather than fixating solely on the null hypothesis (zero effect).\n\nFor instance, in the chatbot scenario, you might ask:\n\n  + \"Will a chatbot reduce average customer wait time by at least 15%?\" Here,\n    the threshold of interest isn't whether there's any reduction in wait time,\n    but whether the reduction is substantial enough (15% or more) to justify\n    implementing the chatbot.\n\n  + \"Will a chatbot increase customer satisfaction scores by at least 10\n    points?\" Similarly, the focus is on a meaningful increase in satisfaction,\n    not just any statistically significant difference from the current baseline.\n\nBy establishing these thresholds, we align our data analysis with the real-world\nimpact of our decisions. A 5% reduction in wait times, even if statistically\nsignificant, might not be worth the cost of implementing a chatbot.\n\n#### Design the Study: Tailoring Research to Your Decision  {.unnumbered}\n\nThis stage involves selecting the appropriate research methodology to answer\nyour questions. Crucially, **the study design must be tailored to the specific\ndecision you're facing.** Factors to consider include data availability,\nexperimental design, and potential biases. **Additionally, ethical\nconsiderations should be at the forefront of your design, ensuring the study\ndoes no harm and respects the rights of participants.**\n\n  - **Chatbot Example:** If you're exploring whether to offer a chatbot as an\n    option, an A/B test where some customers are offered the chatbot while\n    others follow the standard process might be suitable. However, if you're\n    considering making the chatbot the only option, your study design needs to\n    reflect this forced-choice scenario.\n  - **Event Invitation Example:** If you want to understand the value of\n    inviting people to an event, randomizing invitations and analyzing\n    attendance rates is a valid approach. But if you want to understand the\n    value of actually attending the event, you'd focus on outcomes for\n    attendees, even if the data comes from the same experiment.\n\nThe key is to ensure your study design mirrors the real-world conditions of the\ndecision as closely as possible.\n\n\n#### Present Findings and Implications: Communicating Clearly and Transparently  {.unnumbered}\n\n\nAfter conducting your study, present the results clearly, concisely, and\naccessibly. Avoid jargon that could confuse your audience. Even a meticulously\ndesigned study can lead to misinformed decisions if the findings are poorly\ncommunicated. Be transparent about your learnings, acknowledge any limitations\nof the study, and highlight new questions that have arisen.\n\nIn discussing limitations, it's crucial to distinguish between\n<a class='glossary' title='The confidence that the observed effects in a study are truly caused by the factor being studied, rather than other confounding variables.'>Internal Validity</a> (the confidence that the observed effects \nare due to the factor you're studying) and <a class='glossary' title='The extent to which the results of a study can be generalized to other populations, settings, or situations.'>External Validity</a>\n(the extent to which the results can be generalized to other situations).\nEven with a flawless experimental design, questions of external validity might\nremain. For example, a chatbot study conducted with tech-savvy users might\nnot apply to an older demographic.\n\n#### Real-World Constraints and the Path Forward  {.unnumbered}\n\nReal-world constraints often prevent us from conducting the \"perfect\" study with\nboth impeccable internal and external validity. Therefore, it's essential to\nview evidence quality as a spectrum, not a binary. Learning is an ongoing\nprocess. Embrace uncertainty, acknowledging that no single study provides all\nthe answers. Instead of thinking in terms of \"success\" or \"failure,\" consider\nthe weight of evidence, the specific context, and the potential risks and\nrewards when making decisions.\n\nBy adopting the \"Decisions First\" framework and embracing a Bayesian approach,\nyou can transform data analysis from a ritualistic exercise into a powerful tool\nfor making informed, impactful decisions.\n\n\n\n",
    "supporting": [
      "chapter_02_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}