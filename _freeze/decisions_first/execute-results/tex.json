{
  "hash": "3823f364ca34ab406677f0061d20981d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"A Decisions First Framework\"\nshare:\n  permalink: \"https://book.martinez.fyi/decisions_first.html\"\n  description: \"Business Data Science: What Does it Mean to Be Data-Driven?\"\n  linkedin: true\n  email: true\n  mastodon: true\n---\n\n<style>\na.glossary {\n  color: purple;\n  text-decoration: underline;\n  cursor: help;\n  position: relative;\n}\n\n/* only needed for popup = \"click\" */\n/* popup-definition */\na.glossary .def {\n  display: none;\n  position: absolute;\n  z-index: 1;\n  width: 200px;\n  bottom: 100%;\n  left: 50%;\n  margin-left: -100px;\n  background-color: #333;\n  color: white;\n  padding: 5px;\n  border-radius: 6px;\n}\n/* show on click */\na.glossary:active .def {\n  display: inline-block;\n}\n/* triangle arrow */\na.glossary:active .def::after {\n  content: ' ';\n  position: absolute;\n  top: 100%;\n  left: 50%;\n  margin-left: -5px;\n  border-width: 5px;\n  border-style: solid;\n  border-color: #333 transparent transparent transparent;\n}\n</style>\n\n\n\n\nIn the world of <a class='glossary' title='Describes decision-making that is informed by the analysis and interpretation of data.'>data-driven</a> decision-making, it's all too easy to\nfall into the trap of the \"null ritual.\" This ritual, as @gigerenzer2004null\npoint out, involves a slavish adherence to Null Hypothesis Significance Testing\n(NHST) without truly grasping its foundations or limitations. It's akin to\nfollowing a recipe without tasting the ingredients - you might end up with a\nstatistically significant result, but one that lacks real-world meaning or\nutility.\n\nInstead of mindlessly performing this ritual, we need a more thoughtful,\npurposeful approach. Enter the r glossary(\"Decisions First Framework\"). This\napproach flips the script, putting the focus squarely on the decisions we need\nto make and using data as a tool to illuminate the path forward. It helps us\nsidestep several pitfalls, including what @manski2020lure calls the \n<a class='glossary' title='The tendency to present research findings with unwarranted certainty, often driven by pressure to provide clear-cut answers.'>Lure of Incredible Certitude</a>- the tendency to present research\nfindings with unwarranted certainty, often driven by pressure to provide\nclear-cut answers even when the data is ambiguous or uncertain.\n\nOne crucial reason for this shift is that NHST often misses the mark when it\ncomes to answering the practical questions businesses need to address. Moreover,\nthere's a widespread misunderstanding of <a class='glossary' title='the probability, assuming a certain statistical model, that a statistical summary of the data (such as the sample mean difference between two groups) would be equal to or more extreme than its observed value. While a P-value indicates how “incompatible” the data are with a specified statistical model, it does not measure the probability that the hypothesis under study is true nor does it measure the probability that the data were produced by random chance alone.'>p-values</a>.\nThe American Statistical Association took the unusual step of issuing a\nstatement in 2016 to sound the alarm on this issue [see @wasserstein2016asa].\nThey noted that \"practices that reduce data analysis or scientific inference to\nmechanical 'bright-line' rules (such as 'p \\< 0.05') for justifying scientific\nclaims or conclusions can lead to erroneous beliefs and poor decision making.\"\nThey further clarified that \"a p-value does not measure the probability that the\nstudied hypothesis is true, or the probability that the data were produced by\nrandom chance alone.\"\n\nThe \"Decisions First\" framework offers an antidote to the null ritual by\nembracing a Bayesian perspective. This approach treats learning from data as a\ncontinuous process, not a binary \"accept/reject\" decision based on a single\np-value. Rather than fixating on point estimates, p-values, or confidence\nintervals, it emphasizes estimating probabilities that are relevant to the\ndecisions at hand. It acknowledges the inherent uncertainty in data and seeks to\nquantify and integrate it into the decision-making process. In essence,\n<a class='glossary' title='A statistical approach that allows for updating beliefs or probabilities based on new evidence, rather than relying on fixed hypotheses.'>Bayesian Statistics</a> allows us to use data to reallocate\ncredibility across various possibilities.\n\n\n## Define the Decision(s): The Cornerstone of Data-Driven Choices\n\nThe first, and arguably most crucial, step is to **clearly articulate the\ndecision(s) you need to make**. This might involve launching a new product,\nadjusting pricing strategies, or optimizing marketing campaigns. Some decisions\nwill be binary, but that's not always the case. The key is that the optimal\ndecision should hinge on the information available. If no amount of evidence\nwill change your mind, there's little point in designing a study to collect it.\n\n## Formulate Data-Driven Questions: Asking the Right Questions\n\nWith your decision clearly defined, it's time to craft questions that data can\nanswer and that directly inform your decision. These questions should be\nfocused, actionable, and revolve around\n<a class='glossary' title='A predetermined level of effect or change that is considered meaningful for decision-making purposes.'>meaningful thresholds</a>, rather than\nfixating solely on the null hypothesis (zero effect).\n\nFor instance, in a scenario involving the implementation of a chatbot, you might\nask:\n\n  + \"Will a chatbot reduce average customer wait time by at least 15%?\" Here,\n    the threshold of interest isn't whether there's any reduction in wait time,\n    but whether the reduction is substantial enough (15% or more) to justify\n    implementing the chatbot.\n\n  + \"Will a chatbot increase customer satisfaction scores by at least 10\n    points?\" Similarly, the focus is on a meaningful increase in satisfaction,\n    not just any statistically significant difference from the current baseline.\n\nBy establishing these thresholds, we align our data analysis with the real-world\nimpact of our decisions. A 5% reduction in wait times, even if statistically\nsignificant, might not justify the cost of implementing a chatbot.\n\nIn many cases, a well-structured question can be surprisingly simple, often\nfollowing the format \"Does <span style=\"color:#EA4335;\">A</span> do\n<span style=\"color:#34A853;\">B</span> among\n<span style=\"color:#4285F4;\">C</span> compared to\n<span style=\"color:#FBBC04;\">D</span>?\"\n\n  - <span style=\"color:#EA4335;\">A</span>: The intervention or action under\n    evaluation (e.g., chatbot, new pricing).\n  - <span style=\"color:#34A853;\">B</span>: Your clear definition of success\n    (e.g., reduce wait times, increase sales).\n  - <span style=\"color:#4285F4;\">C</span>: The target population (e.g., all\n    customers, a specific segment).\n  - <span style=\"color:#FBBC04;\">D</span>: The alternative or baseline for\n    comparison (e.g., no chatbot, current pricing).\n\nHowever, sometimes you'll encounter more nuanced questions like \"What works for\nwhom?\" These situations involve evaluating multiple alternatives with the\nunderstanding that different options might be optimal for different groups\nwithin your population.\n\n\n## Design the Study: Tailoring Research to Your Decision  \n\nThis stage involves selecting the appropriate research methodology to answer\nyour questions. Crucially, **the study design must be tailored to the specific\ndecision you're facing.** Factors to consider include data availability,\nexperimental design, and potential biases. **Additionally, ethical\nconsiderations should be at the forefront of your design, ensuring the study\ndoes no harm and respects the rights of participants.**\n\n  - **Chatbot Example:** If you're exploring whether to offer a chatbot as an\n    option, an A/B test where some customers are offered the chatbot while\n    others follow the standard process might be suitable. However, if you're\n    considering making the chatbot the only option, your study design needs to\n    reflect this forced-choice scenario.\n  - **Event Invitation Example:** If you want to understand the value of\n    inviting people to an event, randomizing invitations and analyzing\n    attendance rates is a valid approach. But if you want to understand the\n    value of actually attending the event, you'd focus on outcomes for\n    attendees, even if the data comes from the same experiment.\n\nThe key is to ensure your study design mirrors the real-world conditions of the\ndecision as closely as possible.\n\n\n## Present Findings and Implications: Communicating Clearly and Transparently\n\nAfter conducting your study, present the results clearly, concisely, and\naccessibly. Avoid jargon that could confuse your audience. Even a meticulously\ndesigned study can lead to misinformed decisions if the findings are poorly\ncommunicated. Be transparent about your learnings, acknowledge any limitations\nof the study, and highlight new questions that have arisen.\n\nIn discussing limitations, it's crucial to distinguish between\n<a class='glossary' title='The confidence that the observed effects in a study are truly caused by the factor being studied, rather than other confounding variables.'>Internal Validity</a> (the confidence that the observed effects \nare due to the factor you're studying) and <a class='glossary' title='The extent to which the results of a study can be generalized to other populations, settings, or situations.'>External Validity</a>\n(the extent to which the results can be generalized to other situations).\nEven with a flawless experimental design, questions of external validity might\nremain. For example, a chatbot study conducted with tech-savvy users might\nnot apply to an older demographic.\n\n## Real-World Constraints and the Path Forward\n\nReal-world constraints often prevent us from conducting the \"perfect\" study with\nboth impeccable internal and external validity. Therefore, it's essential to\nview evidence quality as a spectrum, not a binary. Learning is an ongoing\nprocess. Embrace uncertainty, acknowledging that no single study provides all\nthe answers. Instead of thinking in terms of \"success\" or \"failure,\" consider\nthe weight of evidence, the specific context, and the potential risks and\nrewards when making decisions.\n\nBy adopting the \"Decisions First\" framework and embracing a Bayesian approach,\nyou can transform data analysis from a ritualistic exercise into a powerful tool\nfor making informed, impactful decisions. This approach not only aligns your\nresearch with your business objectives but also acknowledges the complexities\nand uncertainties inherent in real-world decision-making.\n\n\n::: {.callout-tip}\n## Learn more\n@duke2019thinking Thinking in bets: Making smarter decisions when you don't have\nall the facts.\n::: \n",
    "supporting": [
      "decisions_first_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}