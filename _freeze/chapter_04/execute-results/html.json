{
  "hash": "62d693e3309e9afbe3b648697fc76c96",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"The Gold Standard\"\n---\n\n\nRandomized controlled trials (RCTs) have a rich history, dating back to\nthe 1920s when researchers used them to compare crop yields under different\nconditions. In medicine, RCTs gained prominence in the mid-20th century, thanks\nto pioneers like Austin Bradford Hill, who demonstrated their power in\nevaluating the effectiveness of treatments like streptomycin for tuberculosis.\nToday, RCTs are considered the gold standard for establishing causality in many\nfields, from healthcare to social policy.\n\nIn the tech sector, a simplified version of the RCT – the A/B test – has become\nubiquitous. Companies like Google, Amazon, and Facebook routinely run A/B tests\nto evaluate new features, website designs, and marketing campaigns. The allure\nof A/B testing lies in its simplicity: randomly assign users to different\ngroups, expose them to different versions of a product or experience, and\nmeasure the outcomes. This allows for a clean comparison, isolating the effect\nof the change from other factors that might influence user behavior.\n\n## The Power of Randomization  \n\nIt is crucial to recognize that randomization isn't magic. When a sufficiently\nlarge population is randomly divided into two groups, these groups will be\nremarkably similar in both observable and unobservable characteristics. By\nassigning one group to a treatment and leaving the other as a control, any\ndifference in the outcome of interest can be confidently attributed to the\ntreatment. As we discussed in Chapter 1, causal inference can be conceptualized\nas a missing data problem. In a randomized experiment, this problem is\nsimplified because we effectively have data missing at random, allowing us to\nmake unbiased estimates of causal effects.\n\n### The Importance of SUTVA: The Cornerstone of Valid Inference \n\nHowever, it's crucial to remember that the success of both RCTs and A/B tests\nhinges on a fundamental assumption: the Stable Unit Treatment Value Assumption\n(SUTVA). SUTVA has two main components:\n\n  - **No Interference:** The treatment applied to one unit (individual, user,\n    etc.) should not affect the outcome of another unit.\n\n  - **Consistency:** The potential outcome of a unit under a treatment should be\n    the same regardless of how other units are assigned.\n\nIn simpler terms, SUTVA ensures that the effect of the treatment is solely due\nto the treatment itself and not influenced by other factors or interactions\nbetween units.\n\n#### SUTVA Violations: When the Ideal Meets Reality   {.unnumbered}\n\nWhile SUTVA is often assumed, it can be easily violated:\n\n  - **Network Effects:** Consider an A/B test of a new social media feature. If\n    users in the treatment group interact with users in the control group, the\n    feature's impact might spread beyond the intended group, violating SUTVA.\n\n  - **Market Competition:** Testing a new pricing strategy might trigger\n    competitor reactions, indirectly affecting the outcome even for users not\n    exposed to the new price.\n\n  - **Spillover Effects:** In advertising, a targeted campaign for one product\n    might unintentionally increase awareness or sales of related products.\n\n#### Mitigating SUTVA Violations   {.unnumbered}\n\nSometimes, the solution to a SUTVA violation can be as simple as **changing the\nunit of randomization.** For instance, running geo-experiments in geographically\nisolated markets can minimize interaction between groups. In other cases,\nsolutions require more intricate study designs. When complete elimination isn't\nfeasible, it's crucial to **acknowledge and mitigate** the potential impact of\nSUTVA violations on your conclusions.\n\n::: {.callout-note title=\"Key Takeaway:\"}\nUnderstanding and addressing SUTVA is essential for designing experiments and\ndrawing valid conclusions. By carefully considering the potential for\ninterference and inconsistency, researchers and practitioners can design more\nrobust experiments and make more informed decisions based on their findings.\n:::\n\n### Baseline equivalence\n\nBaseline equivalence is a critical, particularly in experiments with a small\nsample size or observational studies. It refers to the similarity of groups\n(e.g., treatment and control groups) on key characteristics before an\nintervention or treatment is introduced. Without baseline equivalence, observed\ndifferences in outcomes could be due to pre-existing differences between the\ngroups, rather than the intervention itself. For instance, if a company is\ntesting a new algorithm to increase user engagement on their platform, and the\ntreatment group (those exposed to the new algorithm) already had higher baseline\nengagement levels than the control group, any observed increase in engagement in\nthe treatment group might be due to their pre-existing behavior, not necessarily\nthe new algorithm.\n\n#### Establishing Baseline Equivalence  {.unnumbered}\n\nIn RCTs, random assignment of participants to treatment and control groups is\nshould warranty baseline equivalence as long as the group is large enough.\nHowever, even in large RCTs, factors like attrition (loss of participants) or\nreassignment (participants switching groups) can compromise baseline\nequivalence. In such cases, you will need employ statistical techniques to\nadjust for any remaining differences between the groups.\n\nIn non-randomized studies (quasi-experimental designs), establishing baseline\nequivalence is more challenging. You can use techniques to match treated units\nto units that are untreated but look similar in observable characteristics.\nHowever, it's important to note that these methods cannot guarantee equivalence\non unobserved characteristics, which could still bias the results.\n\n#### Assessing Baseline Equivalence  {.unnumbered}\n\nTo assess baseline equivalence, we can examine pre-intervention outcomes and\nother relevant observable characteristics. A common approach is to calculate the\neffect size, defined as the difference in means between the treatment and\ncontrol groups divided by the pooled standard deviation.\n\nIt is generally accepted that if the absolute value of the effect size is\ngreater than 0.25, baseline equivalence is not established, and statistical\nadjustments are unlikely to adequately correct for the differences. When the\nabsolute value of the effect size falls between 0.05 and 0.25, statistical\nadjustments are necessary to achieve baseline equivalence. An absolute value\nless than 0.05 indicates strong evidence of baseline equivalence.\n\n### An example using code\n\nLet's delve into the concept of baseline equivalence with synthetic data.\nSuppose we're analyzing a population of 10,000 individuals, each characterized\nby two traits: x1, observable to us, and e1, which remains hidden. Given the\nlarge sample size, if we randomly split this population into two groups, we can\nexpect these groups to be statistically similar. Let's see this in action:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(941998)\nN <- 10000  # Our synthetic dataset comprises 10,000 observations\n\nfake_data <- tibble::tibble(\n  x1 = rnorm(n = N, mean = 5000, sd = 500),\n  e1  = rnorm(n = N, mean = 7000, sd = 900),\n  t = sample(\n    x = c(TRUE, FALSE),\n    size = N,\n    replace = TRUE,\n    prob = c(0.5, 0.5)\n  )\n)\n```\n:::\n\n\nWe can examine the mean values for the two groups:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeans <- fake_data |>\n  dplyr::group_by(t) |>\n  dplyr::summarise(mean_x1 = mean(x1),\n                   mean_e1 = mean(e1))\n\n# Calculate the differences between means for x1 and e1\nmean_diff_x1 <- means$mean_x1[2] - means$mean_x1[1]\nmean_diff_e1 <- means$mean_e1[2] - means$mean_e1[1]\n\ncat(\"Difference in mean_x1 between t = TRUE and t = FALSE:\", mean_diff_x1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDifference in mean_x1 between t = TRUE and t = FALSE: -0.4563229 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Difference in mean_e1 between t = TRUE and t = FALSE:\", mean_diff_e1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDifference in mean_e1 between t = TRUE and t = FALSE: -3.910241 \n```\n\n\n:::\n:::\n\n\nHowever, simply observing the difference in means might not provide a clear picture of the magnitude. Let's assess the effect size:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npooled_sd_x1 <- sd(fake_data$x1)\npooled_sd_e1 <- sd(fake_data$e1)\n\nabs_effect_size_x1 <- abs(mean_diff_x1 / pooled_sd_x1)\nabs_effect_size_e1 <- abs(mean_diff_e1 / pooled_sd_e1)\n\ncat(\"Absolute value of the effect size for x1 is:\", abs_effect_size_x1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAbsolute value of the effect size for x1 is: 0.0009064671 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Absolute value of the effect size for e1 is:\", abs_effect_size_e1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAbsolute value of the effect size for e1 is: 0.00434118 \n```\n\n\n:::\n:::\n\n\nHere, we can observe that despite randomization, the two groups are statistically identical. However, what if our sample size is much smaller?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(941998)\nN <- 10  # Our synthetic dataset comprises 10,000 observations\n\nfake_data2 <- tibble::tibble(\n  x1 = rnorm(n = N, mean = 5000, sd = 500),\n  e1  = rnorm(n = N, mean = 7000, sd = 900),\n  t = sample(\n    x = c(TRUE, FALSE),\n    size = N,\n    replace = TRUE,\n    prob = c(0.5, 0.5)\n  )\n)\n\nmeans <- fake_data2 |>\n  dplyr::group_by(t) |>\n  dplyr::summarise(mean_x1 = mean(x1),\n                   mean_e1 = mean(e1))\n\n# Calculate the differences between means for x1 and e1\nmean_diff_x1 <- means$mean_x1[2] - means$mean_x1[1]\nmean_diff_e1 <- means$mean_e1[2] - means$mean_e1[1]\n\npooled_sd_x1 <- sd(fake_data2$x1)\npooled_sd_e1 <- sd(fake_data2$e1)\n\nabs_effect_size_x1 <- abs(mean_diff_x1 / pooled_sd_x1)\nabs_effect_size_e1 <- abs(mean_diff_e1 / pooled_sd_e1)\n\ncat(\"Absolute value of the effect size for x1 is:\", abs_effect_size_x1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAbsolute value of the effect size for x1 is: 0.3726052 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Absolute value of the effect size for e1 is:\", abs_effect_size_e1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAbsolute value of the effect size for e1 is: 1.276676 \n```\n\n\n:::\n:::\n\n\nWith a smaller sample size, we see significant differences between the two\ngroups, even with randomization. The disparity in the unobservable\ncharacteristic (e1) is particularly large. If we were to compare outcomes for\nthese groups, which are influenced by this unobservable characteristic, we might\nmistakenly attribute any differences to the intervention being studied.\n\nThe {im} package provides a convenient way to check baseline equivalence using\nthe `im::checkBaseline` function and visualize the results with\n`im::balancePlot`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbalance <-\n  im::checkBaseline(data = fake_data,\n                    variables = c(\"x1\", \"e1\"),\n                    treatment = \"t\")\nbalance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  variables  std_diff balance      \n  <chr>         <dbl> <fct>        \n1 x1        -0.000906 Not Concerned\n2 e1        -0.00434  Not Concerned\n```\n\n\n:::\n\n```{.r .cell-code}\nim::balancePlot(data = balance)\n```\n\n::: {.cell-output-display}\n![](chapter_04_files/figure-html/baseline-1.png){width=672}\n:::\n:::\n\n\nYou can also randomize data using `im::randomize`. This function iteratively\nre-randomizes until achieving a specified level of baseline equivalence or\nreaching a maximum number of attempts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_data <- tibble::tibble(\n  x1 = rnorm(10000), \n  x2 = rnorm(10000)\n)\n\n# Randomize\nrandomized <- im::randomizer$new(\n  data = my_data, \n  seed = 12345, \n  max_attempts = 1000,\n  variables = c(\"x1\", \"x2\"), \n  standard = \"Not Concerned\"\n)\n\n# Get Randomized Data\nrandomized$data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10,000 × 3\n       x1      x2 treated\n    <dbl>   <dbl> <lgl>  \n 1 -0.206 -1.06   TRUE   \n 2 -0.559  1.66   TRUE   \n 3  0.465  1.25   TRUE   \n 4  0.400  0.808  TRUE   \n 5 -0.964 -0.290  FALSE  \n 6  0.642 -0.741  FALSE  \n 7  0.632 -0.0584 FALSE  \n 8 -1.27  -0.767  TRUE   \n 9 -0.945  0.355  TRUE   \n10 -1.13   0.332  TRUE   \n# ℹ 9,990 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get Balance Summary\nrandomized$balance_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  variables  std_diff balance      \n  <chr>         <dbl> <fct>        \n1 x1        -0.000358 Not Concerned\n2 x2        -0.0161   Not Concerned\n```\n\n\n:::\n\n```{.r .cell-code}\n# Generate Balance Plot\nrandomized$balance_plot\n```\n\n::: {.cell-output-display}\n![](chapter_04_files/figure-html/im--randomizer-1.png){width=672}\n:::\n:::\n\n\n::: {.callout-tip}\n## Learn more\n@van2024experiments [Applied Causal Inference Powered by ML and AI. Chapter 2.](https://causalml-book.org/)\n\n@wwc_baseline[What Works Clearinghouse Baseline Equivalence Standard.](https://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/WWC-Baseline-Brief-v6_508.pdf)\n:::\n\n## Stratified Randomization\n\nWhile random assignment is effective, unforeseen factors can sometimes lead to\nimbalanced groups. Stratified randomization addresses this by dividing users\ninto subgroups (strata) based on relevant characteristics that are believed to\ninfluence the outcome metric. Randomization is then performed within each\nstratum, ensuring that both treatment and control groups have a similar\nproportion of users from each subgroup.\n\nThis approach strengthens experiments by creating balanced groups. For instance,\nif user location is expected to affect the outcome metric, users can be\nstratified by location (e.g., urban vs. rural), followed by randomization within\neach location. This ensures a similar distribution of user attributes across\ntreatment and control groups, controlling for confounding factors—user traits\nthat impact both exposure to the new feature and the desired outcome. With\nbalanced groups, any observed differences in the outcome metric are more likely\ndue to the new feature itself, leading to more precise and reliable results.\n\n### Examples   {.unnumbered}\n\n  - **Targeting Mobile App Engagement:** In an RCT to evaluate a new in-app\n    notification, user location (urban vs. rural) is suspected to influence user\n    response. Stratification by location, followed by randomization within each\n    stratum, can control for this factor.\n\n  - **Personalizing a Recommendation Engine:** When A/B testing a revamped\n    recommendation engine, past purchase history is hypothesized to influence\n    user response. Stratification by purchase history categories (e.g., frequent\n    buyers of clothing vs. electronics), followed by randomization within each\n    category, can account for this.\n\n### Advantages:  {.unnumbered}\n\n  - **Reduced bias:** Stratification helps isolate the true effect of the new\n    feature by controlling for the influence of confounding factors. This leads\n    to more reliable conclusions about the feature's impact on user behavior.\n\n  - **Improved decision-making:** By pinpointing the feature's effect on\n    specific user groups (e.g., urban vs. rural in the notification example),\n    stratified randomization can inform decisions about targeted rollouts or\n    further iterations based on subgroup performance.\n\n### Disadvantages:  {.unnumbered}\n\n  - **Increased complexity:** Designing and implementing stratified randomization\nrequires careful planning to choose the right stratification factors and ensure\nenough users within each stratum for valid analysis.\n\n  - **Need for larger sample sizes:** Maintaining balance across strata might\nnecessitate a larger overall sample size compared to simple random assignment.\n\n### Example with code\n\nLet's illustrate the concept of stratified randomization with a practical\nexample. Consider a scenario where we have data on 10,000 individuals, each\ndescribed by two continuous variables (x1 and x2) and two categorical variables\n(x3 and x4). We suspect that variable x3 might be a confounding factor\ninfluencing the outcome of our experiment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_data <- tibble::tibble(\n  x1 = rnorm(10000), \n  x2 = rnorm(10000),\n  x3 = rep(c(\"A\", \"B\"), 5000),\n  x4 = rep(c(\"C\", \"D\"), 5000)\n)\n\n# Create a Randomizer Object\nrandomized <- im::randomizer$new(\n  data = my_data, \n  seed = 12345, \n  max_attempts = 1000,\n  variables = c(\"x1\", \"x2\"), \n  standard = \"Not Concerned\",\n  group_by = \"x3\"\n)\n\n# Generate Balance Plot\nrandomized$balance_plot\n```\n\n::: {.cell-output-display}\n![](chapter_04_files/figure-html/stratified-1.png){width=672}\n:::\n:::\n\n\n\nIn this code, we're using the im::randomizer function to create an object that\nwill help us perform stratified randomization. We specify x3 as the variable to\nstratify by, ensuring that the treatment and control groups have a balanced\ndistribution of individuals from both categories of x3.\n\nBy incorporating stratified randomization into our experimental design, we can\neffectively control for the influence of variable x3, enhancing the internal\nvalidity of our study and allowing for more precise estimates of causal effects.\n\nIn conclusion, stratified randomization offers a powerful way to enhance the\nrigor and precision of experiments, particularly when dealing with potential\nconfounding factors. While it may introduce some additional complexity and\npotentially require larger sample sizes, the benefits in terms of internal\nvalidity and the ability to draw more nuanced conclusions often outweigh these\ndrawbacks. The thoughtful use of stratified randomization can be a valuable\nasset in the causal inference toolkit.\n\n## Adaptive designs\n\nIn the realm of randomized trials, the concept of \"adaptive design\" introduces a dynamic element. Unlike traditional designs where the course of the experiment is fixed from the outset, adaptive designs allow for modifications during the trial based on the accumulating data. This flexibility can be harnessed to enhance the efficiency and effectiveness of the experiment.\n\nOne particularly powerful approach to adaptive design is the Bayesian adaptive design. This method leverages Bayesian statistics, which allows for incorporating prior knowledge and the continuous updating of beliefs as new data become available. In the context of randomized trials, this means that the allocation of participants to different treatment arms can be adjusted in real time based on the observed outcomes.\n\nFor instance, if early data suggest that a particular treatment arm is showing promising results, the Bayesian adaptive design might allocate more participants to that arm, increasing our ability to distinguish signal from noise. Conversely, if a treatment arm appears to be ineffective or even harmful, the design might reduce or even stop the allocation of participants to that arm, thus protecting them from unnecessary exposure.\n\n### Advantages:  {.unnumbered}\n\n  - **Increased Efficiency:** By focusing resources on the most promising treatment arms, Bayesian adaptive designs can potentially reduce the sample size needed to detect a significant effect, saving time and costs.\n\n  - **Ethical Considerations:** The ability to adapt the trial based on emerging data can help protect participants from ineffective or harmful treatments.\n\n  - **Improved Decision-Making:** The continuous updating of beliefs based on real-time data can lead to more informed decisions about the allocation of resources and the selection of the most effective interventions.\n\n### Challenges:  {.unnumbered}\n\n  - **Complexity:** Designing and implementing Bayesian adaptive designs can be more complex than traditional fixed designs, requiring expertise in Bayesian statistics and careful planning.\n\n  - **Statistical Considerations:** The adaptive nature of these designs can introduce statistical challenges, such as the need to adjust for multiple comparisons and the potential for bias if the adaptation process is not carefully controlled.\n\n::: {.callout-tip}\n## Learn more\n@finucane2018works [What works for whom? A Bayesian approach to channeling big data streams for public program evaluation.](https://journals.sagepub.com/doi/abs/10.1177/1098214017737173)\n:::\n\n### Example with code\n\nTODO\n\n## Factorial designs\n\nIn many scenarios, your goal is to use data to design the optimal treatment. This is where factorial designs\ncome into play. A factorial design is an experimental setup where multiple factors are\nmanipulated or varied simultaneously. Each factor can have multiple levels, and\nthe experiment involves testing all possible combinations of these factor\nlevels. This allows us to not only assess the main effects of each factor but\nalso to investigate the interactions between factors.\n\nBayesian analysis offers a powerful approach to factorial designs, especially\nwhen dealing with complex experiments with many factors and levels. By\nincorporating prior knowledge and using hierarchical models, Bayesian methods\ncan improve the precision of estimates and control the risk of false positives\nfrom multiple comparisons.\n\nIn a Bayesian factorial design, we start with prior distributions for the\neffects of each factor level and the interactions between them. These priors\ncan be based on previous research, expert opinion, or simply reflect our\nuncertainty about the effects. As we collect data from the experiment, we update\nthese priors using Bayes' theorem, resulting in posterior distributions that\nreflect our updated beliefs about the effects.\n\nOne of the key advantages of Bayesian factorial designs is the ability to\n\"borrow strength\" across different factor levels and interactions. This means\nthat if the data for one factor level are limited, the model can use information\nfrom other factor levels to improve the estimate for that level. This is\nparticularly useful in complex experiments where some factor combinations might\nhave smaller sample sizes.\n\n::: {.callout-tip}\n## Learn more\n@kassler2018beyond [Kassler, D. Nichols-Barrer, I. and Finucane, M. Beyond “Treatment versus Control”: How Bayesian Analysis Makes Factorial Experiments Feasible in Education Research.](https://journals.sagepub.com/doi/abs/10.1177/0193841X18818903)\n:::\n\n### Example with code\n\nTODO\n",
    "supporting": [
      "chapter_04_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}