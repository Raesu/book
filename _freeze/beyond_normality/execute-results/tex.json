{
  "hash": "6e19c89ae6b740506d76b3a57c459538",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Beyond Normality\"\nshare:\n  permalink: \"https://book.martinez.fyi/beyond_normality.html\"\n  description: \"Business Data Science: What Does it Mean to Be Data-Driven?\"\n  linkedin: true\n  email: true\n  mastodon: true\n---\n\n\n\nIn the preceding chapters, we basked in the warm glow of normality, a\nsimplifying assumption that allowed us to model outcomes as if they arose from\nthe familiar bell curve. Yet, the real world often serves up data generated by\nfar more diverse processes. Outcomes might be binary choices (to adopt or\nreject), rankings on an ordinal scale (like satisfaction surveys), or simple\ncounts (such as the number of subscribers). In this chapter, we embark on an\nexpedition beyond the comfortable confines of normality, charting the terrain of\nthese alternative data types and the specialized tools we need to analyze them.\n\n## Binary Outcomes: The Coin Flips of Data\n\nBinary outcomes are the coin flips of the data world â€“ two sides, two\npossibilities. Think success/failure, yes/no, or the ever-important adopt/reject\ndecision. To model these, we turn to the trusty logistic regression. While\nlinear probability models are also used, logistic regression has the distinct\nadvantage of keeping our predictions bounded between the sensible limits of 0\nand 1.\n\nThis workhorse of a model is a type of generalized linear model, employing a\nlogit link function:\n\n$$\n\\text{BernoulliLogit}(y|\\theta) = \\text{Bernoulli}(y|\\text{logit}^{-1}(\\theta))\n$$\n\nwhere\n\n$$\n\\text{logit}^{-1}(\\theta) = \\frac{1}{1 + \\exp(-\\theta)}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(dplyr)\n\ninv_logit <- function(theta) {\n  return(1 / (1 + exp(-theta)))\n}\n\nggplot2::ggplot() +\n  ggplot2::stat_function(fun = inv_logit) +\n  ggplot2::theme_minimal() +\n  ggplot2::xlim(-5, 5) +\n  ggplot2::xlab(expression(theta))\n```\n\n::: {.cell-output-display}\n![](beyond_normality_files/figure-pdf/inverse_logit-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\nTo illustrate, let's cook up some fake data:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fake data\nN <- 2000\nK <- 2\nset.seed(1982)\nfake_data <- tibble::tibble(\n  x1 = rnorm(N, mean = 0, sd = 1),\n  x2 = rnorm(N, mean = 0, sd = 1),\n  treat = sample(\n    x = c(TRUE, FALSE), size = N, replace = TRUE,\n    prob = c(0.5, 0.5)\n  ),\n  r = runif(n = N, min = 0, max = 1)\n) %>%\n  dplyr::mutate(\n    p0 = inv_logit(theta = -3 + 0.1 * x1 + 0.25 * x2),\n    p1 = inv_logit(theta = -3 + 0.1 * x1 + 0.25 * x2 + 0.2),\n    y0 = dplyr::case_when(p0 > r ~ 1, TRUE ~ 0),\n    y1 = dplyr::case_when(p1 > r ~ 1, TRUE ~ 0),\n    y = dplyr::case_when(\n      treat ~ as.logical(y1),\n      TRUE ~ as.logical(y0)\n    )\n  )\ndplyr::glimpse(fake_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 2,000\nColumns: 9\n$ x1    <dbl> 0.685092067, -0.005550195, -0.777641329, 1.875702830, -0.3771291~\n$ x2    <dbl> -0.665502269, -1.256150229, -0.230714338, 0.743955915, -0.630752~\n$ treat <lgl> FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE,~\n$ r     <dbl> 0.45401970, 0.20720548, 0.15142911, 0.31274073, 0.88143116, 0.97~\n$ p0    <dbl> 0.04319535, 0.03507396, 0.04166872, 0.06745600, 0.03933915, 0.03~\n$ p1    <dbl> 0.05225914, 0.04250932, 0.05042906, 0.08117855, 0.04763407, 0.04~\n$ y0    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\n$ y1    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\n$ y     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F~\n```\n\n\n:::\n\n```{.r .cell-code}\nmean_y0 <- mean(fake_data$y0)\nmean_y1 <- mean(fake_data$y1)\nimpact <- round((mean(fake_data$y1) - mean(fake_data$y0)) * 100, 2)\n```\n:::\n\n\n\nIn this fabricated dataset, we've engineered a scenario where, without\nintervention, only 4% would have adopted the feature.\nYet, with the intervention applied universally, adoption would have jumped to\n5%. The true impact of this intervention is a hefty\n1 percentage points.\n\n### Prior predictive checking\n\nAs Bayesians, we're not just number crunchers; we're storytellers. We weave\nnarratives about data, and our priors are the opening chapters. So, before we\nunleash our model, let's ponder what our priors imply.\n\nOur model takes this form:\n\n$$\n\\begin{aligned}\n\\theta &=  \\alpha + X \\beta + \\tau  T \\\\\n\\alpha &\\sim N(\\mu_{\\alpha}, sd_{\\alpha}) \\\\\n\\beta_j &\\sim N(\\mu_{\\beta_j}, sd_{\\beta_j}) \\\\\n\\tau &\\sim N(\\mu_{\\tau}, sd_{\\tau})\n\\end{aligned}\n$$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogit <- im::logit$new(\n  data = fake_data,\n  y = \"y\", # this will not be used\n  treatment = \"treat\",\n  x = c(\"x1\", \"x2\"),\n  mean_alpha = -3,\n  sd_alpha = 2,\n  mean_beta = c(0, 0),\n  sd_beta = c(1, 1),\n  tau_mean = 0.05,\n  tau_sd = 0.5,\n  fit = FALSE # we will not be fitting the model\n)\n\nlogit$plotPrior()\n```\n\n::: {.cell-output-display}\n![](beyond_normality_files/figure-pdf/priors_check-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n\n### Fitting the Model: Where Theory Meets Data\n\nSatisfied with our priors, we're ready to fit the model to the data:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogit <- im::logit$new(\n  data = fake_data,\n  y = \"y\",\n  treatment = \"treat\",\n  x = c(\"x1\", \"x2\"),\n  mean_alpha = -3,\n  sd_alpha = 2,\n  mean_beta = c(0, 0),\n  sd_beta = c(1, 1),\n  tau_mean = 0.05,\n  tau_sd = 0.5,\n  fit = TRUE\n)\n```\n:::\n\n\n\nLet's glance at the trace plot of tau to ensure our chains mixed well and converged:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogit$tracePlot()\n```\n\n::: {.cell-output-display}\n![](beyond_normality_files/figure-pdf/tracePlot-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n\nTo sum up our findings, we have a few handy methods at our disposal:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogit$pointEstimate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.35\n```\n\n\n:::\n\n```{.r .cell-code}\nlogit$credibleInterval(width = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGiven the data, we estimate that there is a 95% probability that the effect is between -1 and 4 percentage points.\n```\n\n\n:::\n\n```{.r .cell-code}\nlogit$calcProb(a = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGiven the data, we estimate  that the probability that the effect is more than 0 percentage points is 89%.\n```\n\n\n:::\n:::\n\n\n\nWe can also use the prediction function to predict new data and compare the\ndifferences between groups. The `predict` function takes the `new_data` and\n`name` argument to name the group. For example, here we will predict the data as\nif all units are treated, then make another prediction as if all units are not\ntreated and summarize the two groups.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfake_treated_data <- fake_data %>% mutate(treat = TRUE)\nfake_control_data <- fake_data %>% mutate(treat = FALSE)\nlogit$predict(\n  new_data = fake_treated_data,\n  name = \"y1\"\n)\nlogit$predict(\n  new_data = fake_control_data,\n  name = \"y0\"\n)\nlogit$predSummary(name = \"y1\", width = 0.95, a = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Given the data, we estimate that for group: y1, the point estimate of the group average is 5%. With 95% probability, the point estimate is between 4 and 7 percentage points. Given the data, we estimate  that the probability that the group average is more than 0 percentage points is 100%.\"\n```\n\n\n:::\n\n```{.r .cell-code}\nlogit$predSummary(name = \"y0\", width = 0.95, a = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Given the data, we estimate that for group: y0, the point estimate of the group average is 4%. With 95% probability, the point estimate is between 3 and 6 percentage points. Given the data, we estimate  that the probability that the group average is more than 0 percentage points is 100%.\"\n```\n\n\n:::\n:::\n\n\n\nWe can also compare the differences between two groups of predictions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit$predCompare(name1 = \"y1\", name2 = \"y0\", width = 0.95, a = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Given the data, we estimate that the point estimate of the group difference is 1%. With 95% probability, the point estimate is between -1 and 3 percentage points. Given the data, we estimate  that the probability that the group difference is more than 0 percentage points is 89%.\"\n```\n\n\n:::\n:::\n\n\n\nWe can also summarize and compare the predictions conditioning on subgroups.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit$predSummary(\n  name = \"y1\",\n  subgroup = (fake_data$x1 > 0),\n  width = 0.95, a = 0\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Given the data, we estimate that for group: y1, the point estimate of the group average is 5%. With 95% probability, the point estimate is between 3 and 8 percentage points. Given the data, we estimate  that the probability that the group average is more than 0 percentage points is 100%.\"\n```\n\n\n:::\n\n```{.r .cell-code}\nlogit$predCompare(\n  name1 = \"y1\",\n  name2 = \"y0\",\n  subgroup1 = (fake_treated_data$x1 > 0),\n  subgroup2 = (fake_control_data$x1 > 0),\n  width = 0.95, a = 0\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Given the data, we estimate that the point estimate of the group difference is 1%. With 95% probability, the point estimate is between -1 and 4 percentage points. Given the data, we estimate  that the probability that the group difference is more than 0 percentage points is 85%.\"\n```\n\n\n:::\n:::\n\n\n\nFinally, we can get the posterior predictive draws for advanced analysis.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred <- logit$getPred(name = \"y1\")\n```\n:::\n\n\n\n\n\n## Ordinal Outcomes\n\nOrdinal outcomes are a rather curious beast in the realm of causal inference.\nThey have a natural order - think of a survey respondent rating their\nsatisfaction on a scale from \"very dissatisfied\" to \"very satisfied\" - but the\nintervals between the values don't necessarily hold equal weight. The distance\nbetween \"dissatisfied\" and \"neutral\" may not be the same as that between\n\"satisfied\" and \"very satisfied.\" This lack of equal spacing is a challenge we\nmust address head-on when analyzing the impact of an intervention or treatment.\n\nThis nuance of ordinal outcomes is also present in other domains, such as user\nexperience ratings (e.g., poor, fair, good, excellent), or educational\nattainment levels (e.g., less than high school, high school diploma, some\ncollege, college degree). In each case, there's a clear order, but the spacing\nbetween levels is not uniform.\n\nThis lack of uniform spacing can complicate our analysis, particularly when\napplying traditional regression models designed for continuous outcomes. We need\na tailored approach that respects the ordinal nature of the data, while still\nallowing us to draw meaningful causal inferences.\n\nTODO\n\n## Count Outcomes\n\nIn this section, we're talking about events we can tallyâ€”website visits,\ncustomer complaints, product sales, you name it. These outcomes are inherently\nnon-negative integers, reflecting the discrete nature of the events we're\ncounting.\n\nThe go-to tool for analyzing count outcomes is often Poisson regression.\nHowever, real-world data frequently throws us a curveball in the form of\noverdispersion, where the variance of the count data outstrips its mean. This is\nwhere negative binomial regression swoops in to save the day. It's a souped-up\nversion of Poisson regression that accounts for overdispersion by adding an\nextra parameter to the model.\n\nCount data pops up in all sorts of scenarios. We might be interested in the\neffect of a new marketing campaign on app downloads or the impact of a software\nupdate on user logins. With count outcomes, the possibilities are as endless as\nthe events we can count.\n\n### An Example with Fake Data: Video Views Galore\n\nLet's say we're interested in the number of views a video receives in a given\nperiod. This is a perfect opportunity to use the negative binomial distribution\nto model the underlying data-generating process. Here's how we can express this:\n\n$$\n\\begin{aligned}\ny_i  & \\sim \\text{NB}(\\mu_i, \\phi) \\\\\nlog(\\mu_i)  & = \\alpha + X\\beta\n\\end{aligned}\n$$\n\nIn this model, $\\mu$ represents the mean (which must be positive, hence the log\nlink function), and the inverse of $\\phi$ controls the overdispersion. A small\n$\\phi$ means the negative binomial distribution significantly deviates from a\nPoisson distribution, while a large $\\phi$ brings it closer to a Poisson. This\nbecomes clear when we look at the variance:\n\n\n$$\nVar(y_i)  \\sim \\mu_i + \\frac{\\mu_i^2}{\\phi}\n$$\n\nLet's whip up some fake data to illustrate:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(9782)\nlibrary(dplyr)\nlibrary(ggplot2)\nN <- 1000\n\nfake_data <-\n  tibble::tibble(x1 = runif(N, 2, 9), x2 = rnorm(N, 0, 1)) %>%\n  dplyr::mutate(\n    mu = exp(0.5 + 1.7 * x1 + 4.2 * x2),\n    y0 = rnbinom(N, size = 2, mu = mu), # here size is phi\n    y1 = y0 * 1.05,\n    t = sample(c(TRUE, FALSE), size = n(), replace = TRUE, prob = c(0.5, 0.5)),\n    y = case_when(t ~ as.integer(y1),\n      .default = as.integer(y0)\n    )\n  ) %>%\n  filter(y > 0)\n\n# Plotting the histogram using ggplot2\nggplot(fake_data, aes(x = y)) +\n  geom_histogram(\n    bins = 100,\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  facet_wrap(~t) +\n  labs(title = \"Histogram of y\", x = \"y\", y = \"Frequency\") +\n  xlim(0, 1e4) +\n  ylim(0, 20)\n```\n\n::: {.cell-output-display}\n![](beyond_normality_files/figure-pdf/unnamed-chunk-1-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n### OLS: A Common, But Flawed, Approach\n\nA typical way to estimate the lift of an intervention in this scenario is to run\nordinary least squares (OLS) on the natural logarithm of the outcome. Then,\nfolks often look at the point estimate and 95% confidence interval for the\ntreatment effect. Doing this with our fake data might lead you to conclude that\nthe intervention is ineffective, as the 95% confidence interval includes zero.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(data = fake_data, log(y) ~ x1 + x2 + t) %>% broom::tidy(conf.int = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 7\n  term        estimate std.error statistic    p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>    <dbl>     <dbl>\n1 (Intercept)   0.422     0.0861     4.89  0.00000116   0.253      0.591\n2 x1            1.68      0.0137   122.    0            1.65       1.70 \n3 x2            4.15      0.0293   142.    0            4.09       4.20 \n4 tTRUE         0.0254    0.0547     0.464 0.643       -0.0819     0.133\n```\n\n\n:::\n:::\n\n\n\n### Taming Overdispersion: The Bayesian Negative Binomial Advantage\n\nEnter the Bayesian negative binomial model, easily implemented using the {im}\npackage in R. This approach has two key advantages. First, it better captures\nthe true data-generating process. Second, being Bayesian, it lets us incorporate\nprior information and express our findings in a way that's more directly\nrelevant to business decisions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(im)\nnb <- negativeBinomial$new(\n  data = fake_data, y = \"y\", x = c(\"x1\", \"x2\"),\n  treatment = \"t\", tau_mean = 0.0, tau_sd = 0.025\n)\n```\n:::\n\n\n\nA quick check of the trace plot is always a good idea:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb$tracePlot()\n```\n\n::: {.cell-output-display}\n![](beyond_normality_files/figure-pdf/tp-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nNow for the payoff. We can readily obtain a point estimate for the impact using\n`nb$pointEstimate()` (4%) and a 95%\ncredible interval using `nb$credibleInterval(width = 0.95, round = 2)`\n(Given the data, we estimate that there is a 95% probability that the effect is between 0 and 0.08.). But what if we want to know\nthe probability that the impact is at least 1% (or any other threshold)? Easy\npeasy! We use `nb$posteriorProb(threshold = 0.01)`\n(Given the data, we estimate that the probability that the effect is more than 0.01 is 93%.). Finally, to visualize the evolution of\nour understanding from prior to posterior, we employ `nb$vizdraws()`.\n\n::: {.content-visible when-format=\"html\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb$vizdraws(display_mode_name = TRUE, breaks = 0.01,\n            break_names = c(\"< 0.01\", \"> 0.01\"))\n```\n:::\n\n\n::: ",
    "supporting": [
      "beyond_normality_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}