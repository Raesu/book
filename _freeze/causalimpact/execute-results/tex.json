{
  "hash": "025b79a86bf0ba2bbedb6a63f607d2e4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian Structural Time Series\"\nshare:\n  permalink: \"https://book.martinez.fyi/causalimpact.html\"\n  description: \"Business Data Science: What Does it Mean to Be Data-Driven?\"\n  linkedin: true\n  email: true\n  mastodon: true\nauthor:\n  - name: Eray Turkel\n  - name: Ignacio Martinez\n---\n\n\n\nIn the fast-paced digital landscape, businesses continuously seek ways to\nmeasure the impact of their interventions, such as marketing campaigns, pricing\nadjustments, or product launches. The difficulty arises in discerning the\nintervention's effect from the natural fluctuations of business metrics.\nBayesian structural time series emerges as a tool that, when employed\njudiciously, can leverage data to steer your decisions. However, if used without\ngrasping the underlying assumptions, it can lead to misguided choices while\ncreating a false sense of data-driven decision-making.\n\nFundamentally, this method constructs a counterfactual scenarioâ€”what would have\ntranspired if the intervention hadn't taken place. It accomplishes this by\nlearning patterns from the pre-intervention period and from analogous time\nseries not subject to the intervention. The divergence between this\ncounterfactual and the actual outcome is then ascribed to the intervention.\n\nThe [{causalImpact}](https://github.com/google/CausalImpact) R package is a\nwidely used implementation of this methodology. Let's illustrate its application\nwith a simple example from the package\n[vignette](https://google.github.io/CausalImpact/CausalImpact.html):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(CausalImpact)  \nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Generate synthetic data\nset.seed(1)  # Set the random seed for reproducibility\nx1 <- 100 + arima.sim(model = list(ar = 0.999), n = 100)  # Simulated covariate\ny <- 1.2 * x1 + rnorm(100)  # Simulated outcome with a linear relationship to x1\ny[71:100] <- y[71:100] + 10  # Introduce an intervention effect in the latter part of the series\ndata <- cbind(y, x1)  # Combine outcome and covariate into a data matrix\n\n# Define pre- and post-intervention periods\npre.period <- c(1, 70)\npost.period <- c(71, 100)\n\n# Estimate causal impact\nimpact <- CausalImpact(data, pre.period, post.period)  # Main function to estimate the impact\n\n# Visualize results\nplot(impact)  # Default plot with three panels\n```\n\n::: {.cell-output-display}\n![](causalimpact_files/figure-pdf/casualimapct-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nThe default plot produced by this code consists of three panels. The first panel\ndisplays the data alongside a counterfactual prediction for the post-treatment\nperiod. The second panel shows the discrepancy between the observed data and the\ncounterfactual predictions, representing the estimated pointwise causal effect.\nIn the third panel, these pointwise contributions are summed, resulting in a\nplot of the intervention's cumulative effect.\n\n## Caveats and Assumptions\n\nWhen using this tool, we must tread carefully. The allure of a method that\npromises to quantify causal effects is strong, but like any statistical\ntechnique, it comes with caveats that are crucial to understand.\n\nAt the heart of this method lie two critical assumptions:\n\n1.  **Stability and Generalizability:** The model assumes that the relationship\n    between the covariates and the treated time series, which it learns from the\n    pre-intervention period, remains stable and generalizable to the\n    post-intervention period. In essence, it's betting that the past is a\n    reliable guide to the future. This is akin to assuming that if you've\n    observed how interest rates affect inflation for the past decade, that\n    relationship will hold true for the next year, regardless of any policy\n    changes.\n2.  **Unaffected Covariates:** The model's ability to construct a reliable\n    counterfactual hinges on the assumption that the intervention does not\n    affect the covariates used in the analysis. It's like assuming that when the\n    Federal Reserve changes interest rates, it doesn't influence other economic\n    indicators we're using to predict inflation.\n\nThese assumptions are not mere technicalities - they're the foundation upon\nwhich the entire inferential structure is built. If they crumble, so does our\nability to draw causal conclusions.\n\n::: {.callout-important}\nIt's paramount to remember that causality is not a property that can be\nmagically extracted from data through algorithmic means. No matter how\nsophisticated the method, whether Bayesian structural time series or any other\ncausal inference algorithm, it cannot definitively establish causality on its\nown. Causality emerges from our understanding of the world, our theories about\nhow things operate, and the assumptions we are willing to embrace.\n:::\n\n## Cautionary Tale: Stability\n\nMisinterpreting the findings of a {causalImpact} study can occur in several\nways.\n\nIn practice, it may be relatively straightforward to find exogenous covariates\nfor predicting the outcome that are not directly affected by the treatment\nthemselves.\n\nA critical threat, even in this setting, is the assumption of stability and\ngeneralizability of the relationship between covariates and the treated time\nseries.\n\nHow can this assumption lead us astray? Let's consider a few illustrative\nscenario\n\n\n### Simple Setup\n\nImagine we are conducting an intervention in one geographic region, and we\nleverage data from other regions to predict the outcome for the treated region.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerate_baseline_data<-function(\n    n_cities,n_weeks,\n    base_min, base_max,\n    noise_sd){\n\n    #City ID's\n    cities <- paste(\"City\", 1:n_cities)\n\n    # Base values\n    base_values <- runif(\n      n_cities,\n      min = base_min,\n      max = base_max)  # Random base value for each city\n\n    sim_data <- data.frame(\n      week = rep(1:n_weeks, times = n_cities)) %>%\n      mutate(\n        city = rep(cities, each = n_weeks),\n        base_value = rep(runif(n_cities, base_min, base_max), each = n_weeks),\n        random_noise = rnorm(n(), sd = noise_sd),\n        value = base_value + random_noise\n      )\n    return(sim_data)\n\n    }\n\n# Seed for reproducibility\nset.seed(42)\n\ndata<-generate_baseline_data(\n  n_cities=5,\n  n_weeks=52,\n  base_min=100,\n  base_max=200,\n  noise_sd=10)\n\nggplot(data, aes(x = week, y = value, color = city)) +\n  geom_line() +\n  labs(x = \"Week\", y = \"Value\", title = \"Simulated Weekly Time Series\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](causalimpact_files/figure-pdf/baseline-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nCausalImpact performs reasonably well in a stable environment. In this example,\nwe have 5 cities with a stable trend.\n\nLet's suppose our campaign is run in city 3, starting in October. The campaign\nhas a small effect.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadd_constant_treatment_effect<-function(\n    data,\n    target_city_id,\n    start_week,\n    end_week,\n    effect_size\n){\n  data %>%\n  mutate(\n    treatment = ifelse( # Treatment indicator\n      city == target_city_id & week >= start_week & week <= end_week, 1, 0)\n  ) %>%\n  mutate(\n    treatment_effect = ifelse(treatment == 1, effect_size, 0),  # <1>\n    value = value + treatment_effect       # Add treatment effect to City 3's values\n  ) -> treated_data\n\n  return(treated_data)\n}\n\ntreated_data <- add_constant_treatment_effect(\n  data,\n  target_city_id = \"City 3\",\n  start_week=40,\n  end_week=52,\n  effect_size=15  # <1> \n)\n\nggplot(treated_data, aes(x = week, y = value, color = city)) +\n  geom_line() +\n  labs(x = \"Week\", y = \"Value\", title = \"Simulated Weekly Time Series\") +\n  geom_vline(xintercept = 39, color = \"black\")+\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](causalimpact_files/figure-pdf/treated_data-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n1.  Note that the true impact is 15 and constant.\n\nLet's see how CausalImpact fares in the face of this uncomplicated intervention.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data preparation for CausalImpact\nCI_data_prep<-function(data){\n\n  CI_input <- data %>%\n  select(week, city, value) %>%\n  tidyr::pivot_wider(\n  names_from = city,\n  values_from = value) %>%select(-week)\n\n  # Get the treated city:\n  data %>%\n    filter(treatment == 1) %>%\n    select(city) %>% pull() %>% unique() -> treated_city_id\n\n  # Rename columns\n  colnames(CI_input)[colnames(CI_input) == treated_city_id] <- \"Y\"\n  colnames(CI_input)[colnames(CI_input) != \"Y\"] <- paste0(\"X\", 1:(ncol(CI_input)-1))\n  # Relocate the treated city to be the first column for CI\n  CI_input <- relocate(CI_input, Y)\n\n  data %>%\n    filter(treatment == 1) %>%\n    select(week) %>% pull() %>% min() -> treatment_start\n  data %>%\n    filter(treatment == 1) %>%\n    select(week) %>% pull() %>% max() -> treatment_end\n  pre_period <- c(1, treatment_start)\n  post_period <- c(treatment_start+1, treatment_end)\n\n  return(list(\n    CI_input_matrix= as.matrix(CI_input),\n    pre_period = pre_period,\n    post_period = post_period\n  ))\n\n}\n\nCI_inputs<-CI_data_prep(treated_data)\n\nimpact <- CausalImpact(\n  CI_inputs$CI_input_matrix,\n  CI_inputs$pre_period,\n  CI_inputs$post_period)\n\n# Summary of results\nsummary(impact)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPosterior inference {CausalImpact}\n\n                         Average       Cumulative  \nActual                   125           1505        \nPrediction (s.d.)        112 (2.5)     1342 (29.7) \n95% CI                   [107, 116]    [1282, 1397]\n                                                   \nAbsolute effect (s.d.)   14 (2.5)      163 (29.7)  \n95% CI                   [8.9, 19]     [107.3, 223]\n                                                   \nRelative effect (s.d.)   12% (2.5%)    12% (2.5%)  \n95% CI                   [7.7%, 17%]   [7.7%, 17%] \n\nPosterior tail-area probability p:   0.00101\nPosterior prob. of a causal effect:  99.89919%\n\nFor more details, type: summary(impact, \"report\")\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the results\nplot(impact)\n```\n\n::: {.cell-output-display}\n![](causalimpact_files/figure-pdf/unnamed-chunk-1-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n::: {.callout-important}\nIf we look at the summary table we can get by running `summary(impact)` the\nfirst thing you will probably notice is that the model estimates a point\nestimate for the average treatment effect equal to\n14 which is very close to the truth\n(15). However, a common misinterpretation arises from the unfortunate line that\nreads \"Posterior prob. of a causal effect: 99%\". **This statement is\nunequivocally incorrect!** Neither this package nor any other can estimate the\nprobability of a causal effect. This number merely represents the posterior\nprobability that the estimated effect exceeds zero.\n:::\n\nWith this simple example out of the way, let's introduce some seasonality to see\nhow the model handles a bit more complexity.\n\n### Example with seasonality\n\nLet's suppose that towards the end of the year, there's a natural seasonal\nincrease for every city, coinciding with our treatment start. Assume the\nseasonality is identical across all cities.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadd_seasonal_effect<-function(\n    data,\n    seasonal_start,\n    seasonal_end,\n    magnitude\n){\n\n  n_seasonal_weeks <- floor((seasonal_end - seasonal_start)/2)\n\n  seasonal_effect <- c(\n  seq(0,magnitude,length.out = n_seasonal_weeks), # Seasonality Ramp Up\n  seq(magnitude,0,length.out = n_seasonal_weeks+1)) # Seasonality Ramp Down\n\n\n  seasonal_data <- data %>%\n    group_by(city) %>%\n    mutate(\n    seasonal =  ifelse(\n      week >= seasonal_start & week <= seasonal_end,\n      seasonal_effect,\n      0\n      )\n    ) %>%\n    ungroup() %>%\n    mutate(value = seasonal + value)\n\n  return(seasonal_data)\n\n}\n\n\n\n# Seasonal effect\nseasonal_data<-add_seasonal_effect(\n  treated_data,\n  seasonal_start=40,\n  seasonal_end=52,\n  magnitude=50)\n\n\n\nggplot(seasonal_data, aes(x = week, y = value, color = city)) +\n  geom_line() +\n  labs(x = \"Date\", y = \"Value\", title = \"Simulated Weekly Time Series\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](causalimpact_files/figure-pdf/seasonal-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data preparation for CausalImpact\nCI_inputs<-CI_data_prep(seasonal_data)\n\nimpact <- CausalImpact(\n  CI_inputs$CI_input_matrix,\n  CI_inputs$pre_period,\n  CI_inputs$post_period)\n\n# Summary of results\nsummary(impact)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPosterior inference {CausalImpact}\n\n                         Average      Cumulative  \nActual                   152          1830        \nPrediction (s.d.)        111 (4.1)    1326 (48.9) \n95% CI                   [100, 116]   [1196, 1396]\n                                                  \nAbsolute effect (s.d.)   42 (4.1)     504 (48.9)  \n95% CI                   [36, 53]     [434, 634]  \n                                                  \nRelative effect (s.d.)   38% (5.4%)   38% (5.4%)  \n95% CI                   [31%, 53%]   [31%, 53%]  \n\nPosterior tail-area probability p:   0.00101\nPosterior prob. of a causal effect:  99.89919%\n\nFor more details, type: summary(impact, \"report\")\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the results\nplot(impact)\n```\n\n::: {.cell-output-display}\n![](causalimpact_files/figure-pdf/sesonal_impact-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nNotice how the estimated effect is now much larger than the true effect of our\nintervention.\n\nThe plots reveal the underlying issue: Our model presumes a 'stable'\nrelationship based on what it learned in the pre-period, and extrapolates these\npatterns into the post-period. Any deviation from this extrapolated prediction\nis incorrectly attributed to our intervention.\n\nSince we lack historical observations with seasonality for the model to learn\nfrom, it conflates the seasonal increases with the effect of our intervention.\n\n### What if we had more data?\n\nYou might suspect that having enough data to learn seasonal patterns from prior\nyears would help. Let's extend our simulation to two years to test this.\n\nSuppose our intervention again has a modest effect in the second year. We can\nutilize the prior year's data to learn the pattern of seasonality.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Two years of baseline data\nset.seed(42)\ntwo_year_data<-generate_baseline_data(\n  n_cities=5,\n  n_weeks=104,\n  base_min=100,\n  base_max=200,\n  noise_sd=5)\n\n# Add simulated treatment effect to the second year\ntwo_year_treated_data <- add_constant_treatment_effect(\n  two_year_data,\n  target_city_id = \"City 3\",\n  start_week=92,\n  end_week=104,\n  effect_size=15 # True effect size.\n)\n\n# Add first year seasonality\ntwo_year_seasonal_data<-add_seasonal_effect(\n  two_year_treated_data,\n  seasonal_start=40,\n  seasonal_end=52,\n  magnitude=50)\n\n# Add second year seasonality\ntwo_year_seasonal_data<-add_seasonal_effect(\n  two_year_seasonal_data,\n  seasonal_start=92,\n  seasonal_end=104,\n  magnitude=50)\n\n\nggplot(two_year_seasonal_data, aes(x = week, y = value, color = city)) +\n  geom_line() +\n  labs(x = \"Week\", y = \"Value\", title = \"Simulated Weekly Time Series\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](causalimpact_files/figure-pdf/more_data-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data preparation for CausalImpact\nCI_inputs<-CI_data_prep(two_year_seasonal_data)\n\nimpact <- CausalImpact(\n  CI_inputs$CI_input_matrix,\n  CI_inputs$pre_period,\n  CI_inputs$post_period)\n\n# Summary of results\nsummary(impact)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPosterior inference {CausalImpact}\n\n                         Average       Cumulative  \nActual                   154           1852        \nPrediction (s.d.)        138 (2.2)     1653 (26.2) \n95% CI                   [133, 142]    [1602, 1701]\n                                                   \nAbsolute effect (s.d.)   17 (2.2)      199 (26.2)  \n95% CI                   [13, 21]      [151, 250]  \n                                                   \nRelative effect (s.d.)   12% (1.8%)    12% (1.8%)  \n95% CI                   [8.8%, 16%]   [8.8%, 16%] \n\nPosterior tail-area probability p:   0.001\nPosterior prob. of a causal effect:  99.9%\n\nFor more details, type: summary(impact, \"report\")\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the results\nplot(impact)\n```\n\n::: {.cell-output-display}\n![](causalimpact_files/figure-pdf/impact_more_data-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nIt appears that in this scenario, the model does manage to get closer to the\ntrue effect of our intervention.\n\nHowever, note that the pattern of seasonality is exactly the same from year 1 to\nyear 2. This level of stability is rarely encountered in the real world.\n\n### What if Seasonal Patterns Change?\n\nConsider the case where last year's seasonality was much lower than usual due to\na special event.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Change to smaller first year seasonality\ntwo_year_seasonal_data<-add_seasonal_effect(\n  two_year_treated_data,\n  seasonal_start=40,\n  seasonal_end=52,\n  magnitude=15)\n\n# Add larger second year seasonality\ntwo_year_seasonal_data<-add_seasonal_effect(\n  two_year_seasonal_data,\n  seasonal_start=92,\n  seasonal_end=104,\n  magnitude=50)\n\n\nggplot(two_year_seasonal_data, aes(x = week, y = value, color = city)) +\n  geom_line() +\n  labs(x = \"Week\", y = \"Value\", title = \"Simulated Weekly Time Series\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](causalimpact_files/figure-pdf/change-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data preparation for CausalImpact\nCI_inputs<-CI_data_prep(two_year_seasonal_data)\n\nimpact <- CausalImpact(\n  CI_inputs$CI_input_matrix,\n  CI_inputs$pre_period,\n  CI_inputs$post_period)\n\n# Summary of results\nsummary(impact)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPosterior inference {CausalImpact}\n\n                         Average      Cumulative  \nActual                   154          1852        \nPrediction (s.d.)        127 (2.8)    1521 (33.4) \n95% CI                   [122, 132]   [1462, 1588]\n                                                  \nAbsolute effect (s.d.)   28 (2.8)     331 (33.4)  \n95% CI                   [22, 33]     [264, 390]  \n                                                  \nRelative effect (s.d.)   22% (2.7%)   22% (2.7%)  \n95% CI                   [17%, 27%]   [17%, 27%]  \n\nPosterior tail-area probability p:   0.001\nPosterior prob. of a causal effect:  99.9%\n\nFor more details, type: summary(impact, \"report\")\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the results\nplot(impact)\n```\n\n::: {.cell-output-display}\n![](causalimpact_files/figure-pdf/impact_change-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nOnce again, the model struggles in the presence of changing seasonal patterns\nbecause it assumes stability. If last year's seasonal patterns differ from this\nyear's, any difference is erroneously attributed to the treatment, leading us to\nan incorrect conclusion.\n\n### Heterogeneous Seasonality\n\nSo far, we've explored scenarios with uniform seasonality across cities, even if\nit varied year over year.\n\nBut what if seasonal patterns are heterogeneous between units? Things become\neven trickier.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadd_random_seasonal_effect<-function(\n    data,\n    seasonal_start,\n    seasonal_end,\n    max_magnitude,\n    min_magnitude\n){\n\n  n_seasonal_weeks <- floor((seasonal_end - seasonal_start)/2)\n\n  seasonal_data <- data %>%\n    group_by(city) %>%\n    mutate(\n    seasonal_coef = runif(n=1, min=min_magnitude, max= max_magnitude),\n    seasonal =  ifelse(\n      week >= seasonal_start & week <= seasonal_end,\n      c(seq(0,seasonal_coef[1],length.out = n_seasonal_weeks),\n      seq(seasonal_coef[1],0,length.out = n_seasonal_weeks+1)),\n      0\n      )\n    ) %>%\n    ungroup() %>%\n    mutate(value = seasonal + value)\n\n  return(seasonal_data)\n\n}\n\n\nset.seed(42)\n\n# Smaller random first year seasonality\ntwo_year_seasonal_data<-add_random_seasonal_effect(\n  two_year_treated_data,\n  seasonal_start=40,\n  seasonal_end=52,\n  max_magnitude=50,\n  min_magnitude=0)\n\n# Larger random second year seasonality\ntwo_year_seasonal_data<-add_random_seasonal_effect(\n  two_year_seasonal_data,\n  seasonal_start=92,\n  seasonal_end=104,\n  max_magnitude=250,\n  min_magnitude=150)\n\n\nggplot(two_year_seasonal_data, aes(x = week, y = value, color = city)) +\n  geom_line() +\n  labs(x = \"Week\", y = \"Value\", title = \"Simulated Weekly Time Series\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](causalimpact_files/figure-pdf/heterogeneous_seasonality-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data preparation for CausalImpact\nCI_inputs<-CI_data_prep(two_year_seasonal_data)\n\nimpact <- CausalImpact(\n  CI_inputs$CI_input_matrix,\n  CI_inputs$pre_period,\n  CI_inputs$post_period)\n\n# Summary of results\nsummary(impact)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPosterior inference {CausalImpact}\n\n                         Average      Cumulative  \nActual                   216          2589        \nPrediction (s.d.)        151 (5.8)    1813 (69.1) \n95% CI                   [139, 162]   [1671, 1941]\n                                                  \nAbsolute effect (s.d.)   65 (5.8)     777 (69.1)  \n95% CI                   [54, 77]     [649, 919]  \n                                                  \nRelative effect (s.d.)   43% (5.5%)   43% (5.5%)  \n95% CI                   [33%, 55%]   [33%, 55%]  \n\nPosterior tail-area probability p:   0.00101\nPosterior prob. of a causal effect:  99.89868%\n\nFor more details, type: summary(impact, \"report\")\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the results\nplot(impact)\n```\n\n::: {.cell-output-display}\n![](causalimpact_files/figure-pdf/ht_impact-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nAs we can see, the model's performance deteriorates further when faced with\nheterogeneous seasonal patterns. The estimated effect is now nowhere near the\ntrue effect, underscoring the challenges inherent in causal inference when\nseasonality varies across units. In essence, the model is attempting to fit a\nsingle seasonal pattern to all cities, while each city exhibits its own unique\nseasonal fluctuations. This mismatch leads to a significant bias in the\nestimated treatment effect.\n\n## Cautionary Tale: Spillovers\n\nAnother key assumption of the CausalImpact model is that the covariates used to\npredict the outcome of interest are not themselves affected by the intervention.\n\nIn certain situations, this assumption is reasonable, while in others, it's less\nso. For example, we might be testing different interventions on our customers,\nwho then communicate with one another.\n\nOur units of analysis might interact strategically and compete over limited\nresources, where boosting outcomes for one unit could decrease outcomes for\nothers.\n\nConsider a scenario where we run a special promotion in one region to increase\nsales. This could lead to less inventory available in other regions, causing\nshortages.\n\n### Example: Competing over finite resources\n\nLet's simplify things by examining an example without seasonality.\n\nSuppose we implement an intervention that positively impacts the treated unit.\n\nNow, imagine our units are strategically interacting and competing, and\nincreasing the outcome for one unit inevitably decreases the outcome for others.\n\nThink of a retailer promoting new running shoes with limited inventory to meet\ndemand across all regions. In this case, increased sales in one region would\ninvariably come at the expense of other regions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadd_treatment_effect_with_spillovers<-function(\n    data,\n    target_city_id,\n    start_week,\n    end_week,\n    effect_size\n){\n\n  n_cities<-length(unique(data$city))\n  data %>%\n  mutate(\n    treatment = ifelse(\n      city == target_city_id & week >= start_week & week <= end_week, 1, 0)\n  ) %>%\n  mutate(\n    treatment_effect = case_when(\n      treatment == 1 & city == target_city_id ~ effect_size, # <1>\n       week >= start_week & week <= end_week & city != target_city_id ~ (-effect_size)/(n_cities-1), # <2>\n      TRUE ~ 0\n    ),\n    value = value + treatment_effect       # Add treatment effect to City 3's values\n  ) -> treated_data\n\n  return(treated_data)\n}\n\ntreated_data <- add_treatment_effect_with_spillovers(\n  data,\n  target_city_id = \"City 3\",\n  start_week=40,\n  end_week=52,\n  effect_size=15 # <3>\n)\n\nggplot(treated_data, aes(x = week, y = value, color = city)) +\n  geom_line() +\n  labs(x = \"Week\", y = \"Value\", title = \"Simulated Weekly Time Series\") +\n  geom_vline(xintercept = 39, color = \"black\")+\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](causalimpact_files/figure-pdf/unnamed-chunk-2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n1. Impact on the treated city.\n2. Spillover on the untreated cities.\n3. True impact in treated city.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data preparation for CausalImpact\nCI_inputs<-CI_data_prep(treated_data)\n\nimpact <- CausalImpact(\n  CI_inputs$CI_input_matrix,\n  CI_inputs$pre_period,\n  CI_inputs$post_period)\n\n# Summary of results\nsummary(impact)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPosterior inference {CausalImpact}\n\n                         Average       Cumulative  \nActual                   125           1505        \nPrediction (s.d.)        112 (2.5)     1343 (29.6) \n95% CI                   [107, 116]    [1284, 1396]\n                                                   \nAbsolute effect (s.d.)   13 (2.5)      161 (29.6)  \n95% CI                   [9.1, 18]     [109.0, 221]\n                                                   \nRelative effect (s.d.)   12% (2.5%)    12% (2.5%)  \n95% CI                   [7.8%, 17%]   [7.8%, 17%] \n\nPosterior tail-area probability p:   0.00102\nPosterior prob. of a causal effect:  99.89765%\n\nFor more details, type: summary(impact, \"report\")\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the results\nplot(impact)\n```\n\n::: {.cell-output-display}\n![](causalimpact_files/figure-pdf/unnamed-chunk-3-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nCausalImpact suggests the effect of our intervention is positive.\n\nLet's examine the true change in the total outcome across all cities.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntotal_outcome_treated_world <- treated_data %>% group_by(week) %>%\n  summarize(total_outcome = sum(value)) %>% mutate(Treatment = 'Treatment')\n\ntotal_outcome_untreated_world <- data %>% group_by(week) %>%\n  summarize(total_outcome = sum(value)) %>% mutate(Treatment = 'No Treatment')\n\n\ntotal_outcome <-\n  bind_rows(total_outcome_treated_world, total_outcome_untreated_world)\n\nggplot(total_outcome, aes(x = week, y = total_outcome, color = Treatment)) +\n  geom_line() +\n  labs(x = \"Week\", y = \"Value\", title = \"Total Outcome Over Time, with added intervention\") +\n  geom_vline(xintercept = 39, color = \"black\") +\n  theme_minimal() +\n  facet_wrap( ~ Treatment)\n```\n\n::: {.cell-output-display}\n![](causalimpact_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nThe treatment has a net total effect of zero, as we can see by comparing the\ntotal outcomes in the data with and without the treatment effect added.\n\nAll the gains observed in the treated unit come at the expense of other\nuntreated units.\n\nYet, CausalImpact has no way of dealing with this and will erroneously suggest\nour intervention had a positive effect.\n\n## Conclusion\n\nBayesian structural time series models, as implemented in the CausalImpact\npackage, offer a powerful tool for businesses seeking to understand the impact\nof their interventions. However, like all statistical methods, they come with\nimportant caveats and assumptions that must be thoroughly understood and\nvalidated.\n\nThe key to successful application lies in combining these sophisticated\nstatistical techniques with domain knowledge, careful data preparation, and a\nhealthy dose of skepticism. By doing so, businesses can gain valuable insights\ninto the effectiveness of their strategies and make more informed decisions in\nan increasingly complex and data-driven world.\n\nRemember, causality is not something that can be magically extracted from data\nthrough algorithmic means. It emerges from our understanding of the world, our\ntheories about how things operate, and the assumptions we are willing to\nembrace. Use these tools wisely, and they can illuminate the path forward. Use\nthem carelessly, and they may lead you astray.\n\n\n::: {.callout-tip}\n## Learn more\n@brodersen2015inferring Inferring causal impact using Bayesian structural time-series models.\n:::",
    "supporting": [
      "causalimpact_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}