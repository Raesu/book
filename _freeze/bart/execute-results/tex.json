{
  "hash": "a79495139ecb510236131cc9ea4acb2a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian Additive Regression Trees (BART)\"\n---\n\n\n\n## BART: Bayesian Additive Regression Trees\n\nBART is a Bayesian nonparametric, machine learning, ensemble predictive modeling\nmethod introduced by @chipman2010bart. It can be expressed as:\n\n$$\nY = \\sum_{j=1}^m g(X; T_j, M_j) + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2)\n$$\n\nwhere $g(X; T_j, M_j)$ represents the $j$-th regression tree with structure\n$T_j$ and leaf node parameters $M_j$. The model uses $m$ trees, typically set to\na large number (e.g., 200), with each tree acting as a weak learner. BART\nemploys a carefully designed prior that encourages each tree to play a small\nrole in the overall fit, resulting in a flexible yet robust model.\n\n@hill2011bayesian proposed using BART for causal inference, recognizing its\nunique advantages in this context. BART is particularly well-suited for causal\ninference due to its ability to:\n\n1.  Capture complex, nonlinear relationships without requiring explicit\n    specification\n2.  Automatically model interactions between variables\n3.  Handle high-dimensional data effectively\n4.  Provide uncertainty quantification through its Bayesian framework\n\nThese features make BART especially valuable in observational studies with many\ncovariates, where the true functional form of the relationship between variables\nis often unknown.\n\nThe key idea was to use BART to model the response surface:\n\n$$\nE[Y | X, Z] = f(X, Z)\n$$\n\nwhere $Y$ is the outcome, $X$ are the covariates, and $Z$ is the treatment\nindicator. The causal effect can then be estimated as:\n\n$$\n\\begin{aligned}\n\\tau(x) & = E[Y | X=x, Z=1] - E[Y | X=x, Z=0] \\\\\n & = f(x, 1) - f(x, 0)\n\\end{aligned}\n$$\n\nThis formulation leverages BART's inherent ability to automatically capture\nintricate interactions and non-linear relationships, making it a potent tool for\ncausal inference, especially in high-dimensional scenarios.\n\nThe effectiveness of BART in causal inference has been further validated in\nrecent competitions. @thal2023causal report on the 2022 American Causal\nInference Conference (ACIC) data challenge, where BART-based methods were among\nthe top performers, particularly for estimating heterogeneous treatment effects.\nThey found that BART's regularizing priors were especially effective in\ncontrolling error for subgroup estimates, even in small subgroups, and its\nflexibility in modeling confounding relationships was crucial for improved\ncausal inference in complex scenarios.\n\n## Example with a Single Covariate\n\nTo illustrate how BART can be used for estimating the impact of an intervention,\n@hill2011bayesian presents a simple example:\n\n\n$$\n\\begin{aligned}\nZ &\\sim \\mbox{Bernoulli}(0.5) \\\\\nX | Z = 1 &\\sim N(40,10^2) \\\\\nX | Z = 0 &\\sim N(20,10^2) \\\\\nY(0) | X &\\sim N(72 + 3\\sqrt{X},1) \\\\\nY(1) | X &\\sim N(90 + exp(0.06X),1) \n\\end{aligned}\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(broom) \n\n# 1. Define True Outcome Functions\nf_treated <- function(x) 90 + exp(0.06 * x)\nf_control <- function(x) 72 + 3 * sqrt(x)\n\n# 2. Visualize True Outcome Functions\nggplot(data.frame(x = 6:62), aes(x = x)) +  # Expanded x range for clarity\n  stat_function(fun = f_control, aes(color = \"Truth - Control\"), linewidth = 1) +  \n  stat_function(fun = f_treated, aes(color = \"Truth - Treatment\"), linewidth = 1) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  labs(title = \"True Outcome Functions\", x = \"X\", y = \"Y\", color = \"\") +\n  theme_bw() +   \n  theme(legend.position = \"bottom\")  \n```\n\n::: {.cell-output-display}\n![](bart_files/figure-pdf/truth-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nWe can generate a sample from that data generating process as follows:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nn_samples <- 120\n\nsimulated_data <- tibble(\n  treatment_group = sample(c(\"Treatment\", \"Control\"), size = n_samples, replace = TRUE),\n  is_treated = treatment_group == \"Treatment\",\n  X = if_else(is_treated, rnorm(n_samples, 40, 10), rnorm(n_samples, 20, 10)),\n  Y1 = rnorm(n_samples, mean = f_treated(X), sd = 1),  \n  Y0 = rnorm(n_samples, mean = f_control(X), sd = 1),  \n  Y = if_else(is_treated, Y1, Y0),\n  true_effect = Y1 - Y0\n)\n\n\n# 4. Visualize Simulated Data with True Functions\nggplot(simulated_data, aes(x = X, y = Y, color = treatment_group)) +\n  geom_point(size = 2) + \n  stat_function(fun = f_control, aes(color = \"Truth - Control\")) +\n  stat_function(fun = f_treated, aes(color = \"Truth - Treatment\")) +\n  scale_color_manual(values = c(\"Truth - Control\" = \"red\", \"Truth - Treatment\" = \"blue\")) + \n  labs(title = \"Simulated Data with True Outcome Functions\", \n       x = \"X\", y = \"Y\", color = \"\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\") \n```\n\n::: {.cell-output-display}\n![](bart_files/figure-pdf/sample-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ncat(glue::glue(\"The true Sample Average Treatment Effect is {round(mean(simulated_data$true_effect),2)}\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe true Sample Average Treatment Effect is 10.84\n```\n\n\n:::\n:::\n\n\n\nNotice that in our sample, there is not very good overlap for low and high\nvalues of X. This means that we will have to do a lot of extrapolation when\ndoing inference for those cases, which is a common challenge in causal\ninference. Now, suppose we just fit OLS to the model to try to estimate the\naverage treatment effect:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model <- lm(Y ~ X + is_treated, data = simulated_data)\nlm_fit <- broom::tidy(linear_model)\nlm_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 5\n  term           estimate std.error statistic  p.value\n  <chr>             <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)      70.1      1.29       54.6  4.71e-85\n2 X                 0.715    0.0486     14.7  2.24e-28\n3 is_treatedTRUE    4.74     1.30        3.64 4.14e- 4\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(glue::glue(\"A linear model finds an Average Treatment Effect equal to {round(lm_fit$estimate[lm_fit$term=='is_treatedTRUE'],2)}\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA linear model finds an Average Treatment Effect equal to 4.74\n```\n\n\n:::\n:::\n\n\n\nIt's important to note that the linear model is misspecified given the true\nnonlinear relationships, which contributes to its poor performance. Let's add\nthe findings from OLS to our plot:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_data <- expand.grid(\n  X = seq(min(simulated_data$X), max(simulated_data$X), length.out = 1000),\n  is_treated = c(TRUE, FALSE)\n) %>%\n  mutate(\n    treatment_group = if_else(is_treated, \"Treatment\", \"Control\"),\n    linear_prediction = predict(linear_model, newdata = .)\n  )\n\n# 6. Visualize Simulated Data, True Functions, and Linear Predictions\nggplot() +\n  geom_point(data = simulated_data, aes(x = X, y = Y, color = treatment_group), size = 2) +\n  stat_function(fun = f_control, aes(color = \"Truth - Control\")) +\n  stat_function(fun = f_treated, aes(color = \"Truth - Treatment\")) +\n  geom_line(data = prediction_data, aes(x = X, y = linear_prediction, \n                                        color = treatment_group), linetype = \"dashed\") +\n  scale_color_manual(values = c(\"Truth - Control\" = \"red\",  \n                                \"Truth - Treatment\" = \"blue\",\n                                \"Control\" = \"red\",           \n                                \"Treatment\" = \"blue\")) +    \n  labs(title = \"Simulated Data, True Functions, and Linear Model Predictions\",\n       x = \"X\", y = \"Y\", color = \"\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\") \n```\n\n::: {.cell-output-display}\n![](bart_files/figure-pdf/ols_plot-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nTo fit BART we can use the [{stochtree}](https://stochastictree.github.io/stochtree-r/) package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxstart <- 20-15\nlength <- 4\ntext_left <- 26-15\nyVals <- seq(110,130,by=4)\n\nX_train <- simulated_data %>% \n                    select(X, is_treated) %>% \n                    as.matrix()\n\nX_test <- prediction_data %>% \n                    select(X, is_treated) %>% \n                    as.matrix()\n\nbart_model <-\n  stochtree::bart(X_train = X_train,\n                  X_test = X_test,\n                  y_train = simulated_data$Y,\n                  num_burnin = 1000,\n                  num_mcmc = 2000)\n```\n:::\n\n\n\nAfter fitting, it is important to examine the traceplot of $\\sigma^2$ to assess\nif the model has converged. We can do this by running the following code:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrace_plot_data <-\n  tibble(\n    iteration = 1:length(bart_model$sigma2_samples),\n    sigma2_samples = bart_model$sigma2_samples\n  )\nggplot(aes(x = iteration, y = sigma2_samples), data = trace_plot_data) +\n  geom_line(color = \"blue\", alpha = 0.5) +\n  labs(title = \"Trace Plot for sigma^2\",\n       x = \"Iteration\",\n       y = \"sigma^2\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](bart_files/figure-pdf/traceplot-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nWhen assessing convergence using the trace plot, look for the following:\n\n1.  Stationarity: The values should fluctuate around a constant mean level.\n2.  No trends: There should be no obvious upward or downward trends.\n3.  Quick mixing: The chain should move rapidly through the parameter space.\n4.  No stuck periods: There should be no long periods where the chain stays at\n    the same value.\n\nIn this case, the trace plot suggests good convergence, as it exhibits these\ndesirable properties.\n\nNow we can plot the predictions from BART and compare them to the truth with the\nfollowing code:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_data <- prediction_data %>%\n  mutate(bart_pred = rowMeans(bart_model$y_hat_test)) \n\n\nggplot() +\n  geom_point(data = simulated_data,\n             aes(x = X, y = Y, color = treatment_group),\n             size = 2) +\n  stat_function(fun = f_control, aes(color = \"Truth - Control\")) +\n  stat_function(fun = f_treated, aes(color = \"Truth - Treatment\")) +\n  scale_color_manual(\n    values = c(\n      \"Truth - Control\" = \"red\",\n      \"Truth - Treatment\" = \"blue\",\n      \"Control\" = \"red\",\n      \"Treatment\" = \"blue\"\n    )\n  ) +\n  labs(\n    title = \"Simulated Data, True Functions, and BART Predictions\",\n    x = \"X\",\n    y = \"Y\",\n    color = \"\"\n  ) +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  geom_line(data = prediction_data,\n            aes(x = X, y = bart_pred, color = treatment_group),\n            linetype = \"dashed\") +\n   scale_color_manual(\"\", values = c(\"red\", \"blue\",\"red\", \"blue\", \"red\", \"blue\", \"red\", \"blue\")) +\n   annotate(geom = \"segment\", x = xstart, y = yVals[1], xend = xstart+length, yend = yVals[1], color = \"red\") +\n   annotate(geom = \"text\", x = text_left, y = yVals[1], label = c(\"truth - control\"), hjust = 0) +\n   annotate(geom = \"segment\", x = xstart, y = yVals[2], xend = xstart+length, yend = yVals[2], color = \"blue\") +\n   annotate(geom = \"text\", x = text_left, y = yVals[2], label = c(\"truth - treatment\"), hjust = 0) +\n   geom_point(aes(x=xstart+length/2, y=yVals[3]), color = c(\"red\")) +\n   annotate(geom = \"text\", x = text_left, y = yVals[3], label = c(\"simulated data - control\"), hjust = 0) +\n   geom_point(aes(x=xstart+length/2, y=yVals[4]), color = c(\"blue\")) +\n   annotate(geom = \"text\", x = text_left, y = yVals[4], label = c(\"simulated data - treatment\"), hjust = 0) +\n   annotate(geom = \"segment\", x = xstart, y = yVals[5], xend = xstart+length, yend = yVals[5], color = \"red\", linetype = \"dashed\") +\n   annotate(geom = \"text\", x = text_left, y = yVals[5], label = c(\"BART - control\"), hjust = 0) +\n   annotate(geom = \"segment\", x = xstart, y = yVals[6], xend = xstart+length, yend = yVals[6], color = \"blue\", linetype = \"dashed\") +\n   annotate(geom = \"text\", x = text_left, y = yVals[6], label = c(\"BART - treatment\"), hjust = 0) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](bart_files/figure-pdf/bart_plot-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nNotice that BART does a very good job when we have overlap between the treatment\nand control groups, but when extrapolating for high values of X, BART cannot get\nthe true control curve right because it has no data in that region. This\nhighlights the importance of understanding how any method works.\n\n\nOnce we have fitted the BART model, we can calculate the sample average\ntreatment effect by predicting the outcome for every individual in our sample\nunder both treatment and control conditions. The difference between these\npredictions gives us the posterior distribution of the treatment effect for each\nindividual. The sample average treatment effect is then the mean of this\nposterior distribution.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx0 <- simulated_data %>% mutate(is_treated=FALSE) %>% select(X,is_treated)\nx1 <- simulated_data %>% mutate(is_treated=TRUE) %>% select(X,is_treated)\n\npred0 <- predict(bart_model, as.matrix(x0))\n\npred1 <- predict(bart_model, as.matrix(x1))\n\ntau_draws <- pred1$y_hat - pred0$y_hat  \nsate_draws <- colMeans(tau_draws)\ncat(glue::glue(\"BART finds an Average Treatment Effect equal to {round(mean(sate_draws),2)}\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBART finds an Average Treatment Effect equal to 9.4\n```\n\n\n:::\n:::\n\n\n\nHowever, as we discussed before, point estimates are often not very useful for\ndecision-making. For instance, we might make different decisions if the impact\nof the intervention is more than 9, between 0 and 9, or less than 0. Calculating\nprobabilities for different effect sizes is more useful for decision-making than\npoint estimates alone because it provides a more nuanced understanding of the\npotential outcomes and the uncertainty associated with our estimates.\n::: {.content-visible when-format=\"pdf\"}\nWe can easily calculate these probabilities using the draws from the posterior\nprobability that we just calculated.\n::: \n::: {.content-visible when-format=\"html\"}\nWe can easily calculate these probabilities using the posterior draws and visualize\nthem:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvizdraws::vizdraws(\n  posterior = sate_draws,\n  breaks = c(0, 9),\n  break_names = c(\"Discontinue\", \"Continue\", \" Expand\")\n)\n```\n:::\n\n\n:::\n\n## More Covariates and Propensity Score\n\nLet's extend our exploration of BART by incorporating multiple covariates and a\npropensity score. This scenario more closely resembles real-world causal\ninference problems, where we often deal with multiple confounding variables and\ncomplex relationships between covariates, treatment, and outcomes.\n\nConsider a scenario with an outcome of interest, $y$, a binary treatment\nindicator, $z$, and three covariates, $x_1$, $x_2$, and $x_3$, related as\nfollows: \n\n$$\ny \\sim N(\\mu(x_1,x_2) + z \\tau(x_2,x_3), \\sigma)\n$$\n\nHere, $\\mu(x_1,x_2)$ represents the prognostic function (the expected outcome in\nthe absence of treatment), and $\\tau(x_2,x_3)$ represents the treatment effect\nfunction. This setup allows for heterogeneous treatment effects that depend on\n$x_2$ and $x_3$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(tidyr)\n\nset.seed(1982)\nn <- 1000 \n\nmy_df <- tibble(\n    x1 = rnorm(n), \n    x2 = rnorm(n), \n    x3 = rnorm(n)\n  ) %>%\n  mutate(\n    mu = if_else(x1 > x2, -0.9, 1.1),\n    pi = pnorm(mu),\n    z = rbinom(n, 1, pi),\n    tau = 1 / (1 + exp(-x3)) + x2 / 10, \n    y = rnorm(n, mean = mu + z * tau, sd = 0.4)\n  )\n\nglimpse(my_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,000\nColumns: 8\n$ x1  <dbl> 0.685092067, -0.005550195, -0.777641329, 1.875702830, -0.377129105~\n$ x2  <dbl> 0.7421946, -1.3523128, 1.0798799, -0.5236852, 0.5257462, 0.6854145~\n$ x3  <dbl> -0.665502269, -1.256150229, -0.230714338, 0.743955915, -0.63075276~\n$ mu  <dbl> 1.1, -0.9, 1.1, -0.9, 1.1, 1.1, 1.1, -0.9, 1.1, -0.9, 1.1, 1.1, -0~\n$ pi  <dbl> 0.8643339, 0.1840601, 0.8643339, 0.1840601, 0.8643339, 0.8643339, ~\n$ z   <int> 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, ~\n$ tau <dbl> 0.41372415, 0.08640604, 0.55056390, 0.62549178, 0.39991449, 0.3825~\n$ y   <dbl> 1.6803051, -1.1743631, 1.4208142, -1.3559658, 0.7012013, 1.8609010~\n```\n\n\n:::\n:::\n\n\n\nIn this data generating process, we've introduced confounding by making the\ntreatment assignment ($z$) depend on the prognostic function ($\\mu$). This\ncreates a challenge for causal inference, as differences in outcomes between\ntreated and control groups will be due to both the treatment effect and the\nconfounding.\n\nTo gain insights into the relationship between $x_3$ and the treatment effect,\n$\\tau$, let's visualize it:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = my_df, aes(x = x3, y = tau)) +\n  geom_point(size = 2.5, alpha = 0.7) +\n  geom_smooth(\n    method = \"loess\",\n    se = FALSE,\n    color = \"blue\",\n    linewidth = 1\n  ) +\n  xlab(expression(x[3])) +\n  ylab(expression(tau)) +\n  ggtitle(expression(\n    paste(\"Relationship between \", x[3], \" and Treatment Effect (\", tau, \")\")\n  )) +\n  theme_minimal() +\n  theme(\n    axis.title = element_text(size = 14, face = \"bold\"),\n    plot.title = element_text(size = 16, hjust = 0.5)\n  )\n```\n\n::: {.cell-output-display}\n![](bart_files/figure-pdf/plot2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n::: {.callout-note}\n\n## Tip\nWhen using BART for causal inference, it's important to include the propensity\nscore as an additional covariate. The propensity score represents the\nprobability of being in the treatment group given the covariates. Including it\ncan improve the accuracy of treatment effect estimation, especially in scenarios\nwith strong confounding.\n\nEmpirical research, such as that by @hahn2020bayesian, has shown that including\nthe propensity score helps mitigate a phenomenon called \"Regularization-Induced\nConfounding\" (RIC). RIC can occur when flexible machine learning methods like\nBART are applied to causal inference problems with strong confounding. By\nincluding the propensity score, we give the model an additional tool to\ndistinguish between the effects of confounding and the true treatment effect.\n::: \n\nWe can estimate the propensity score using a Bayesian generalized linear model\n(GLM):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit Bayesian GLM\nps_model <- arm::bayesglm(z ~ x1 + x2 + x3,\n                    family = binomial(),\n                    data = my_df)\n\n# Calculate Predicted Probabilities\nmy_df <- my_df %>%\n  mutate(ps = predict(ps_model, type = \"response\"))\n\n# Plot Histogram of Predicted Probabilities\nggplot(my_df, aes(x = ps, fill = factor(z))) +\n  geom_histogram(alpha = 0.6, position = \"identity\", bins = 30) +  \n  facet_wrap(~ z, ncol = 1, labeller = labeller(z = c(\"0\" = \"Control\", \"1\" = \"Treated\"))) + \n  labs(title = \"Distribution of Predicted Probabilities by Treatment Group\",\n       x = \"Predicted Probability\", \n       y = \"Frequency\",\n       fill = \"Treatment Group\") +\n  theme_bw() +\n  scale_fill_manual(values = c(\"0\" = \"blue\", \"1\" = \"red\"))\n```\n\n::: {.cell-output-display}\n![](bart_files/figure-pdf/unnamed-chunk-1-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nThis plot reveals the degree of overlap in propensity scores between the treated\nand control groups. Good overlap is crucial for reliable causal inference. Areas\nwhere there's little overlap between the treated and control groups may lead to\nless reliable estimates of treatment effects.\n\nNow we can fit our BART model, including the estimated propensity score\nalongside our original covariates:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX_train <- my_df %>% \n   select(x1, x2, x3, ps, z) %>%\n  as.matrix()\n\nbart_model <-\n  stochtree::bart(X_train = X_train,\n                  y_train = my_df$y,\n                  num_burnin = 1000,\n                  num_mcmc = 2000)\n```\n:::\n\n\n\nAfter fitting, let's examine the trace plot of $\\sigma^2$ to assess convergence:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the Data for Plotting\ndf_plot <- tibble(\n  iteration = seq_along(bart_model$sigma2_samples),    # Sequence of iteration numbers\n  sigma = bart_model$sigma2_samples                    \n)\n\n# Create the Traceplot\nggplot(data = df_plot, aes(x = iteration, y = sigma)) +\n  geom_line(color = \"blue\", alpha = 0.7) +  \n  labs(title = \"Traceplot for BART Model Sigma\",\n       x = \"Iteration\",\n       y = expression(sigma)) +              \n  theme_bw() +\n  geom_hline(yintercept = mean(df_plot$sigma), linetype = \"dashed\", color = \"red\") \n```\n\n::: {.cell-output-display}\n![](bart_files/figure-pdf/unnamed-chunk-2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThis trace plot shows the values of $\\sigma$ across MCMC iterations. We're\nlooking for a relatively stationary pattern without any clear trends, which\nwould indicate good convergence. The red dashed line represents the mean value\nof $\\sigma$.\n\nWe can also assess the effective sample size (ESS) to gauge the efficiency of\nthe sampling process:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(coda)\n\n# Calculate Effective Sample Size (ESS)\ness_sigma <- effectiveSize(df_plot$sigma)  \n\n# Display the Result (with formatting)\ncat(\"Effective Sample Size (ESS) for sigma:\", format(ess_sigma, big.mark = \",\"), \"\\n\") \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEffective Sample Size (ESS) for sigma: 32.76147 \n```\n\n\n:::\n:::\n\n\n\nThe ESS tells us how many independent samples our MCMC chain is equivalent to. A\nhigher ESS indicates more efficient sampling and more reliable posterior\nestimates.\n\nNow, let's calculate the estimated average treatment effect using BART:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx0 <- my_df %>% \n   select(x1, x2, x3, ps, z) %>% \n  mutate(z=0) %>% as.matrix()\nx1 <- my_df %>% \n   select(x1, x2, x3, ps, z) %>% \n  mutate(z=1) %>% as.matrix()\n\npred0 <- predict(bart_model, as.matrix(x0))\n\npred1 <- predict(bart_model, as.matrix(x1))\n\ntau_draws <- pred1$y_hat - pred0$y_hat  \nsate_draws <- colMeans(tau_draws)\ncat(glue::glue(\"BART finds an Average Treatment Effect equal to {round(mean(sate_draws),2)}\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBART finds an Average Treatment Effect equal to 0.63\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(glue::glue(\"The truth Average Treatment Effect is equal to {round(mean(my_df$tau),2)}\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe truth Average Treatment Effect is equal to 0.51\n```\n\n\n:::\n:::\n\n\n\n::: {.callout-note}\n## Tip\nA key advantage of BART is its ability to estimate treatment effects at the\nindividual level. While these individual estimates can be noisy, they can be\nuseful for exploring potential treatment effect heterogeneity. One way to do\nthis is by using a classification and regression tree (CART) model on the\nestimated individual treatment effects:\n::: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart)\nlibrary(rpart.plot)\n\nmy_df <- my_df %>% \n  mutate(tau_hat = rowMeans(tau_draws)) # point estimate of the individual's treatment effect\n\n\n\n# Fit the Tree Model (Using Cross-Validation)\ntree_model <- rpart(\n  tau_hat ~ x1 + x2 + x3, \n  data = my_df, \n  method = \"anova\",      \n  control = rpart.control(cp = 0.05, xval = 10)  # Cross-validation with 10 folds\n)\n\n# Find Optimal Complexity Parameter (CP)\noptimal_cp <- tree_model$cptable[which.min(tree_model$cptable[, \"xerror\"]), \"CP\"]\n\n# Prune the Tree for Better Generalization\npruned_tree <- prune(tree_model, cp = optimal_cp)\n\n# Plot the Pruned Tree\nrpart.plot(pruned_tree, \n           type = 4, extra = 101, \n           fallen.leaves = TRUE,\n           main = \"Tree Model for Individual Treatment Effects\", \n           cex = 0.8, \n           box.palette = \"GnBu\") \n```\n\n::: {.cell-output-display}\n![](bart_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Print the Pruned Tree Summary\nprintcp(pruned_tree)  # See the results of pruning\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression tree:\nrpart(formula = tau_hat ~ x1 + x2 + x3, data = my_df, method = \"anova\", \n    control = rpart.control(cp = 0.05, xval = 10))\n\nVariables actually used in tree construction:\n[1] x1 x2 x3\n\nRoot node error: 105.66/1000 = 0.10566\n\nn= 1000 \n\n        CP nsplit rel error  xerror     xstd\n1 0.118155      0   1.00000 1.00091 0.074453\n2 0.088701      1   0.88185 0.89653 0.073628\n3 0.055613      3   0.70444 0.73799 0.061733\n4 0.050000      5   0.59322 0.66921 0.055934\n```\n\n\n:::\n:::\n\n\nIn our analysis, the CART model successfully identified $x_3$ as the primary\ndriver of treatment effect heterogeneity, aligning with the way we generated our\ndata. However, it's crucial to exercise caution when interpreting these results:\n\n1.  CART models do not inherently account for uncertainty. The splits in the\n    tree are based on point estimates and don't reflect the uncertainty in our\n    treatment effect estimates.\n2.  The tree structure can be sensitive to small changes in the data. Different\n    samples might yield different trees, even if the underlying relationships\n    are the same.\n3.  While CART pinpointed $x_3$ as the key modifier, it also suggests that $x_1$\n    is a modifier, which we know is not correct in our simulated data. This\n    illustrates how CART can sometimes identify spurious relationships.\n\nTo gain a more nuanced understanding of treatment effect heterogeneity, we\nshould leverage the full posterior distribution of treatment effects obtained\nfrom BART. This allows us to quantify our uncertainty about heterogeneous\neffects and avoid overinterpreting potentially spurious findings from methods\nlike CART.\n\nFor example, we could examine how the posterior distribution of treatment\neffects varies across different levels of $x_3$:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create bins for x3\nmy_df <- my_df %>%\n  mutate(x3_bin = cut(x3, breaks = 4))\n\n# Calculate mean and credible intervals for each bin\ntau_summary <- my_df %>%\n  group_by(x3_bin) %>%\n  summarise(\n    mean_tau = mean(tau_hat),\n    lower_ci = quantile(tau_hat, 0.025),\n    upper_ci = quantile(tau_hat, 0.975)\n  )\n\n# Plot\nggplot(tau_summary, aes(x = x3_bin, y = mean_tau)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), width = 0.2) +\n  labs(title = \"Treatment Effect by x3 Bins\",\n       x = \"x3 Bins\",\n       y = \"Estimated Treatment Effect\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](bart_files/figure-pdf/byx3-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nThis plot gives us a more robust view of how the treatment effect varies with\n$x_3$, including our uncertainty about these effects. We can see that the\ntreatment effect generally increases with $x_3$, but there's considerable\nuncertainty\n\nIn conclusion, while BART provides powerful tools for estimating heterogeneous\ntreatment effects, it's crucial to combine these estimates with careful\nconsideration of uncertainty and potential confounding. By doing so, we can gain\nvaluable insights into how treatment effects vary across different subgroups,\nwhile avoiding overconfident or spurious conclusions.\n\n::: {.callout-tip}\n## Learn more\n@hill2011bayesian Bayesian nonparametric modeling for causal inference.\n::: \n\n## Accelerated BART (XBART)\n\nWhile BART has proven to be a powerful tool for causal inference, its\ncomputational demands can be significant, especially with large datasets. To\naddress this, @he2019xbart introduced Accelerated BART (XBART), a method that\nmaintains the flexibility and effectiveness of BART while substantially reducing\ncomputation time.\n\nXBART uses a stochastic tree-growing algorithm inspired by Bayesian updating,\nblending regularization strategies from Bayesian modeling with computationally\nefficient techniques from recursive partitioning. The key difference is that\nXBART regrows each tree from scratch at each iteration, rather than making small\nmodifications to existing trees as in standard BART. The XBART algorithm\nproceeds as follows:\n\n1.  At each node, calculate the probability of splitting at each possible\n    cutpoint based on the marginal likelihood.\n2.  Sample a split (or no split) according to these probabilities.\n3.  If a split is chosen, recursively apply steps 1-2 to the resulting child\n    nodes.\n4.  If no split is chosen or stopping conditions are met, sample the leaf\n    parameter from its posterior distribution.\n\nThis approach allows XBART to efficiently explore the space of possible tree\nstructures, leading to faster convergence and reduced computation time compared\nto standard BART.\n\n",
    "supporting": [
      "bart_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}