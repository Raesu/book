{
  "hash": "fba5af09450509f71538beb9408bf840",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"The Power of Randomization \"\n---\n\n\nIt is crucial to recognize that randomization isn't magic. When a sufficiently\nlarge population is randomly divided into two groups, these groups will be\nremarkably similar in both observable and unobservable characteristics. By\nassigning one group to a treatment and leaving the other as a control, any\ndifference in the outcome of interest can be confidently attributed to the\ntreatment. As we discussed in Chapter 1, causal inference can be conceptualized\nas a missing data problem. In a randomized experiment, this problem is\nsimplified because we effectively have data missing at random, allowing us to\nmake unbiased estimates of causal effects.\n\n### The Importance of SUTVA: The Cornerstone of Valid Inference \n\nHowever, it's crucial to remember that the success of both RCTs and A/B tests\nhinges on a fundamental assumption: the Stable Unit Treatment Value Assumption\n(SUTVA). SUTVA has two main components:\n\n  - **No Interference:** The treatment applied to one unit (individual, user,\n    etc.) should not affect the outcome of another unit.\n\n  - **Consistency:** The potential outcome of a unit under a treatment should be\n    the same regardless of how other units are assigned.\n\nIn simpler terms, SUTVA ensures that the effect of the treatment is solely due\nto the treatment itself and not influenced by other factors or interactions\nbetween units.\n\n#### SUTVA Violations: When the Ideal Meets Reality \n\nWhile SUTVA is often assumed, it can be easily violated:\n\n  - **Network Effects:** Consider an A/B test of a new social media feature. If\n    users in the treatment group interact with users in the control group, the\n    feature's impact might spread beyond the intended group, violating SUTVA.\n\n  - **Market Competition:** Testing a new pricing strategy might trigger\n    competitor reactions, indirectly affecting the outcome even for users not\n    exposed to the new price.\n\n  - **Spillover Effects:** In advertising, a targeted campaign for one product\n    might unintentionally increase awareness or sales of related products.\n\n#### Mitigating SUTVA Violations \n\nSometimes, the solution to a SUTVA violation can be as simple as **changing the\nunit of randomization.** For instance, running geo-experiments in geographically\nisolated markets can minimize interaction between groups. In other cases,\nsolutions require more intricate study designs. When complete elimination isn't\nfeasible, it's crucial to **acknowledge and mitigate** the potential impact of\nSUTVA violations on your conclusions.\n\n::: {.callout-note title=\"Key Takeaway:\"}\nUnderstanding and addressing SUTVA is essential for designing experiments and\ndrawing valid conclusions. By carefully considering the potential for\ninterference and inconsistency, researchers and practitioners can design more\nrobust experiments and make more informed decisions based on their findings.\n:::\n\n::: {.callout-tip}\n## Learn more\n  - @chernozhukov2024applied Applied causal inference powered by ML and AI.\n:::\n\n### Baseline equivalence  and Sample Size Considerations\n\nBaseline equivalence is a critical, particularly in experiments with a small\nsample size or observational studies. It refers to the similarity of groups\n(e.g., treatment and control groups) on key characteristics before an\nintervention or treatment is introduced. Without baseline equivalence, observed\ndifferences in outcomes could be due to pre-existing differences between the\ngroups, rather than the intervention itself. For instance, if a company is\ntesting a new algorithm to increase user engagement on their platform, and the\ntreatment group (those exposed to the new algorithm) already had higher baseline\nengagement levels than the control group, any observed increase in engagement in\nthe treatment group might be due to their pre-existing behavior, not necessarily\nthe new algorithm.\n\n#### The Risk of Small Sample Sizes\n\nIt's important to emphasize that achieving baseline equivalence is particularly\nchallenging with small sample sizes. As @wainer2007most points out in \"The Most\nDangerous Equation,\" small samples are inherently more variable, making it more\nlikely that random chance will lead to imbalances between groups, even with\nrandomization. This can lead to the erroneous conclusion that an effect exists\nwhen it truly doesn't (a Type I error), or vice-versa (a Type II error).\nTherefore, in smaller experiments, careful attention to baseline equivalence and\npotential statistical adjustments are paramount.\n\n#### Establishing Baseline Equivalence\n\nIn RCTs, random assignment of participants to treatment and control groups is\nshould warranty baseline equivalence as long as the group is large enough.\nHowever, even in large RCTs, factors like attrition (loss of participants) or\nreassignment (participants switching groups) can compromise baseline\nequivalence. In such cases, you will need employ statistical techniques to\nadjust for any remaining differences between the groups.\n\nIn non-randomized studies (quasi-experimental designs), establishing baseline\nequivalence is more challenging. You can use techniques to match treated units\nto units that are untreated but look similar in observable characteristics.\nHowever, it's important to note that these methods cannot guarantee equivalence\non unobserved characteristics, which could still bias the results.\n\n#### Assessing Baseline Equivalence\n\nTo assess baseline equivalence, we can examine pre-intervention outcomes and\nother relevant observable characteristics. A common approach is to calculate the\neffect size, defined as the difference in means between the treatment and\ncontrol groups divided by the pooled standard deviation.\n\nIt is generally accepted that if the absolute value of the effect size is\ngreater than 0.25, baseline equivalence is not established, and statistical\nadjustments are unlikely to adequately correct for the differences. When the\nabsolute value of the effect size falls between 0.05 and 0.25, statistical\nadjustments are necessary to achieve baseline equivalence. An absolute value\nless than 0.05 indicates strong evidence of baseline equivalence.\n\n::: {.callout-tip}\n## Learn more\n  - @wwc_baseline What Works Clearinghouse Baseline Equivalence Standard.\n:::\n\n### An example using code\n\nLet's delve into the concept of baseline equivalence with synthetic data.\nSuppose we're analyzing a population of 10,000 individuals, each characterized\nby two traits: x1, observable to us, and e1, which remains hidden. Given the\nlarge sample size, if we randomly split this population into two groups, we can\nexpect these groups to be statistically similar. Let's see this in action:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(941998)\nN <- 10000  # Our synthetic dataset comprises 10,000 observations\n\nfake_data <- tibble::tibble(\n  x1 = rnorm(n = N, mean = 5000, sd = 500),\n  e1  = rnorm(n = N, mean = 7000, sd = 900),\n  t = sample(\n    x = c(TRUE, FALSE),\n    size = N,\n    replace = TRUE,\n    prob = c(0.5, 0.5)\n  )\n)\n```\n:::\n\n\nWe can examine the mean values for the two groups:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeans <- fake_data |>\n  dplyr::group_by(t) |>\n  dplyr::summarise(mean_x1 = mean(x1),\n                   mean_e1 = mean(e1))\n\n# Calculate the differences between means for x1 and e1\nmean_diff_x1 <- means$mean_x1[2] - means$mean_x1[1]\nmean_diff_e1 <- means$mean_e1[2] - means$mean_e1[1]\n\ncat(\"Difference in mean_x1 between t = TRUE and t = FALSE:\", mean_diff_x1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDifference in mean_x1 between t = TRUE and t = FALSE: -0.4563229 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Difference in mean_e1 between t = TRUE and t = FALSE:\", mean_diff_e1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDifference in mean_e1 between t = TRUE and t = FALSE: -3.910241 \n```\n\n\n:::\n:::\n\n\nHowever, simply observing the difference in means might not provide a clear picture of the magnitude. Let's assess the effect size:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npooled_sd_x1 <- sd(fake_data$x1)\npooled_sd_e1 <- sd(fake_data$e1)\n\nabs_effect_size_x1 <- abs(mean_diff_x1 / pooled_sd_x1)\nabs_effect_size_e1 <- abs(mean_diff_e1 / pooled_sd_e1)\n\ncat(\"Absolute value of the effect size for x1 is:\", abs_effect_size_x1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAbsolute value of the effect size for x1 is: 0.0009064671 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Absolute value of the effect size for e1 is:\", abs_effect_size_e1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAbsolute value of the effect size for e1 is: 0.00434118 \n```\n\n\n:::\n:::\n\n\nHere, we can observe that despite randomization, the two groups are statistically identical. However, what if our sample size is much smaller?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(941998)\nN <- 10  # Our synthetic dataset comprises 10 observations\n\nfake_data2 <- tibble::tibble(\n  x1 = rnorm(n = N, mean = 5000, sd = 500),\n  e1  = rnorm(n = N, mean = 7000, sd = 900),\n  t = sample(\n    x = c(TRUE, FALSE),\n    size = N,\n    replace = TRUE,\n    prob = c(0.5, 0.5)\n  )\n)\n\nmeans <- fake_data2 |>\n  dplyr::group_by(t) |>\n  dplyr::summarise(mean_x1 = mean(x1),\n                   mean_e1 = mean(e1))\n\n# Calculate the differences between means for x1 and e1\nmean_diff_x1 <- means$mean_x1[2] - means$mean_x1[1]\nmean_diff_e1 <- means$mean_e1[2] - means$mean_e1[1]\n\npooled_sd_x1 <- sd(fake_data2$x1)\npooled_sd_e1 <- sd(fake_data2$e1)\n\nabs_effect_size_x1 <- abs(mean_diff_x1 / pooled_sd_x1)\nabs_effect_size_e1 <- abs(mean_diff_e1 / pooled_sd_e1)\n\ncat(\"Absolute value of the effect size for x1 is:\", abs_effect_size_x1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAbsolute value of the effect size for x1 is: 0.3726052 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Absolute value of the effect size for e1 is:\", abs_effect_size_e1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAbsolute value of the effect size for e1 is: 1.276676 \n```\n\n\n:::\n:::\n\n\nWith a smaller sample size, we see significant differences between the two\ngroups, even with randomization. The disparity in the unobservable\ncharacteristic (e1) is particularly large. If we were to compare outcomes for\nthese groups, which are influenced by this unobservable characteristic, we might\nmistakenly attribute any differences to the intervention being studied.\n\n### The {im} package\n\nThe {im} package provides a convenient way to check baseline equivalence using\nthe `im::checkBaseline` function and visualize the results with\n`im::balancePlot`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbalance <-\n  im::checkBaseline(data = fake_data,\n                    variables = c(\"x1\", \"e1\"),\n                    treatment = \"t\")\nbalance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  variables  std_diff balance      \n  <chr>         <dbl> <fct>        \n1 x1        -0.000906 Not Concerned\n2 e1        -0.00434  Not Concerned\n```\n\n\n:::\n\n```{.r .cell-code}\nim::balancePlot(data = balance)\n```\n\n::: {.cell-output-display}\n![](rct_basic_files/figure-html/baseline-1.png){width=672}\n:::\n:::\n\n\nYou can also randomize data using `im::randomize`. This function iteratively\nre-randomizes until achieving a specified level of baseline equivalence or\nreaching a maximum number of attempts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_data <- tibble::tibble(\n  x1 = rnorm(10000), \n  x2 = rnorm(10000)\n)\n\n# Randomize\nrandomized <- im::randomizer$new(\n  data = my_data, \n  seed = 12345, \n  max_attempts = 1000,\n  variables = c(\"x1\", \"x2\"), \n  standard = \"Not Concerned\"\n)\n\n# Get Randomized Data\nrandomized$data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10,000 × 3\n       x1      x2 treated\n    <dbl>   <dbl> <lgl>  \n 1 -0.206 -1.06   TRUE   \n 2 -0.559  1.66   TRUE   \n 3  0.465  1.25   TRUE   \n 4  0.400  0.808  TRUE   \n 5 -0.964 -0.290  FALSE  \n 6  0.642 -0.741  FALSE  \n 7  0.632 -0.0584 FALSE  \n 8 -1.27  -0.767  TRUE   \n 9 -0.945  0.355  TRUE   \n10 -1.13   0.332  TRUE   \n# ℹ 9,990 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get Balance Summary\nrandomized$balance_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  variables  std_diff balance      \n  <chr>         <dbl> <fct>        \n1 x1        -0.000358 Not Concerned\n2 x2        -0.0161   Not Concerned\n```\n\n\n:::\n\n```{.r .cell-code}\n# Generate Balance Plot\nrandomized$balance_plot\n```\n\n::: {.cell-output-display}\n![](rct_basic_files/figure-html/im--randomizer-1.png){width=672}\n:::\n:::\n\n\n\n## Stratified Randomization\n\nWhile random assignment is effective, unforeseen factors can sometimes lead to\nimbalanced groups. Stratified randomization addresses this by dividing users\ninto subgroups (strata) based on relevant characteristics that are believed to\ninfluence the outcome metric. Randomization is then performed within each\nstratum, ensuring that both treatment and control groups have a similar\nproportion of users from each subgroup.\n\nThis approach strengthens experiments by creating balanced groups. For instance,\nif user location is expected to affect the outcome metric, users can be\nstratified by location (e.g., urban vs. rural), followed by randomization within\neach location. This ensures a similar distribution of user attributes across\ntreatment and control groups, controlling for confounding factors—user traits\nthat impact both exposure to the new feature and the desired outcome. With\nbalanced groups, any observed differences in the outcome metric are more likely\ndue to the new feature itself, leading to more precise and reliable results.\n\n### Examples \n\n  - **Targeting Mobile App Engagement:** In an RCT to evaluate a new in-app\n    notification, user location (urban vs. rural) is suspected to influence user\n    response. Stratification by location, followed by randomization within each\n    stratum, can control for this factor.\n\n  - **Personalizing a Recommendation Engine:** When A/B testing a revamped\n    recommendation engine, past purchase history is hypothesized to influence\n    user response. Stratification by purchase history categories (e.g., frequent\n    buyers of clothing vs. electronics), followed by randomization within each\n    category, can account for this.\n\n### Advantages:\n\n  - **Reduced bias:** Stratification helps isolate the true effect of the new\n    feature by controlling for the influence of confounding factors. This leads\n    to more reliable conclusions about the feature's impact on user behavior.\n\n  - **Improved decision-making:** By pinpointing the feature's effect on\n    specific user groups (e.g., urban vs. rural in the notification example),\n    stratified randomization can inform decisions about targeted rollouts or\n    further iterations based on subgroup performance.\n\n### Disadvantages:\n\n  - **Increased complexity:** Designing and implementing stratified randomization\nrequires careful planning to choose the right stratification factors and ensure\nenough users within each stratum for valid analysis.\n\n  - **Need for larger sample sizes:** Maintaining balance across strata might\nnecessitate a larger overall sample size compared to simple random assignment.\n\n### Example with code\n\nLet's illustrate the concept of stratified randomization with a practical\nexample. Consider a scenario where we have data on 10,000 individuals, each\ndescribed by two continuous variables (x1 and x2) and two categorical variables\n(x3 and x4). We suspect that variable x3 might be a confounding factor\ninfluencing the outcome of our experiment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_data <- tibble::tibble(\n  x1 = rnorm(10000), \n  x2 = rnorm(10000),\n  x3 = rep(c(\"A\", \"B\"), 5000),\n  x4 = rep(c(\"C\", \"D\"), 5000)\n)\n\n# Create a Randomizer Object\nrandomized <- im::randomizer$new(\n  data = my_data, \n  seed = 12345, \n  max_attempts = 1000,\n  variables = c(\"x1\", \"x2\"), \n  standard = \"Not Concerned\",\n  group_by = \"x3\"\n)\n\n# Generate Balance Plot\nrandomized$balance_plot\n```\n\n::: {.cell-output-display}\n![](rct_basic_files/figure-html/stratified-1.png){width=672}\n:::\n:::\n\n\n\nIn this code, we're using the im::randomizer function to create an object that\nwill help us perform stratified randomization. We specify x3 as the variable to\nstratify by, ensuring that the treatment and control groups have a balanced\ndistribution of individuals from both categories of x3.\n\nBy incorporating stratified randomization into our experimental design, we can\neffectively control for the influence of variable x3, enhancing the internal\nvalidity of our study and allowing for more precise estimates of causal effects.\n\nIn conclusion, stratified randomization offers a powerful way to enhance the\nrigor and precision of experiments, particularly when dealing with potential\nconfounding factors. While it may introduce some additional complexity and\npotentially require larger sample sizes, the benefits in terms of internal\nvalidity and the ability to draw more nuanced conclusions often outweigh these\ndrawbacks. The thoughtful use of stratified randomization can be a valuable\nasset in the causal inference toolkit.",
    "supporting": [
      "rct_basic_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}