{
  "hash": "a4341bba51b9afc0095212c86d64413b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"The Power of Randomization\"\nshare:\n  permalink: \"https://book.martinez.fyi/rct_basic.html\"\n  description: \"Business Data Science: What Does it Mean to Be Data-Driven?\"\n  linkedin: true\n  email: true\n  mastodon: true\n---\n\n\n![Randomization](img/randomization.png){.lightbox}\n\nIt is crucial to recognize that randomization isn't magic. When a sufficiently\nlarge population is randomly divided into two groups, these groups will be\nremarkably similar in both observable and unobservable characteristics. By\nassigning one group to a treatment and leaving the other as a control, any\ndifference in the outcome of interest can be confidently attributed to the\ntreatment. As we discussed in Chapter 1, causal inference can be conceptualized\nas a missing data problem. In a randomized experiment, this problem is\nsimplified because we effectively have data missing at random, allowing us to\nmake unbiased estimates of causal effects.\n\n## The Importance of SUTVA: The Cornerstone of Valid Inference \n\nHowever, it's crucial to remember that the success of both RCTs and A/B tests\nhinges on a fundamental assumption: the Stable Unit Treatment Value Assumption\n(SUTVA). SUTVA has two main components:\n\n  - **No Interference (or No Spillover):** The treatment applied to one unit\n    should not affect the outcome of another unit. This means that the outcome\n    for any unit is unaffected by the treatments received by other units.\n\n  - **Treatment Variation Irrelevance (or Consistency):** The potential outcome\n    of a unit under a specific treatment should be the same regardless of how\n    that treatment is assigned. This implies that if a unit receives a\n    particular treatment, the outcome should only depend on that treatment, not\n    on how or why it was assigned.\n\nIn simpler terms, SUTVA ensures that the effect of the treatment is solely due\nto the treatment itself and not influenced by other factors or interactions\nbetween units.\n\n#### SUTVA Violations: When the Ideal Meets Reality \n\nWhile SUTVA is often assumed, it can be easily violated:\n\n  - **Network Effects:** Consider an A/B test of a new social media feature. If\n    users in the treatment group interact with users in the control group, the\n    feature's impact might spread beyond the intended group, violating SUTVA.\n\n  - **Market Competition:** Testing a new pricing strategy might trigger\n    competitor reactions, indirectly affecting the outcome even for users not\n    exposed to the new price.\n\n  - **Spillover Effects:** In advertising, a targeted campaign for one product\n    might unintentionally increase awareness or sales of related products.\n\n#### Mitigating SUTVA Violations \n\nSometimes, the solution to a SUTVA violation can be as simple as **changing the\nunit of randomization.** For instance, running geo-experiments in geographically\nisolated markets can minimize interaction between groups. In other cases,\nsolutions require more intricate study designs. When complete elimination isn't\nfeasible, it's crucial to **acknowledge and mitigate** the potential impact of\nSUTVA violations on your conclusions.\n\n::: {.callout-note title=\"Key Takeaway:\"}\nUnderstanding and addressing SUTVA is essential for designing experiments and\ndrawing valid conclusions. By carefully considering the potential for\ninterference and inconsistency, researchers and practitioners can design more\nrobust experiments and make more informed decisions based on their findings.\n:::\n\n## An example using code\n\nThe {im} package provides a convenient way to randomize while ensuring baseline\nequivalence. The `im::randomize` function iteratively re-randomizes until\nachieving a specified level of baseline equivalence (see @sec-baseline) or\nreaching a maximum number of attempts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_data <- tibble::tibble(\n  x1 = rnorm(10000), \n  x2 = rnorm(10000)\n)\n\n# Randomize\nrandomized <- im::randomizer$new(\n  data = my_data, \n  seed = 12345, \n  max_attempts = 1000,\n  variables = c(\"x1\", \"x2\"), \n  standard = \"Not Concerned\"\n)\n\n# Get Randomized Data\nrandomized$data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10,000 × 3\n        x1     x2 treated\n     <dbl>  <dbl> <lgl>  \n 1 -0.859   1.59  TRUE   \n 2  1.72   -1.60  TRUE   \n 3  1.07    0.680 TRUE   \n 4  0.742   1.62  TRUE   \n 5  0.0386 -0.563 FALSE  \n 6  0.213  -2.50  FALSE  \n 7 -0.233  -0.168 FALSE  \n 8  0.794   2.17  TRUE   \n 9 -0.316  -1.34  TRUE   \n10  1.98   -0.260 TRUE   \n# ℹ 9,990 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get Balance Summary\nrandomized$balance_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  variables std_diff balance      \n  <chr>        <dbl> <fct>        \n1 x1          0.0491 Not Concerned\n2 x2         -0.0103 Not Concerned\n```\n\n\n:::\n\n```{.r .cell-code}\n# Generate Balance Plot\nrandomized$balance_plot\n```\n\n::: {.cell-output-display}\n![](rct_basic_files/figure-html/im--randomizer-1.png){width=672}\n:::\n:::\n\n\n\n## Stratified Randomization\n\nWhile random assignment is effective, unforeseen factors can sometimes lead to\nimbalanced groups. Stratified randomization addresses this by dividing users\ninto subgroups (strata) based on relevant characteristics that are believed to\ninfluence the outcome metric. Randomization is then performed within each\nstratum, ensuring that both treatment and control groups have a similar\nproportion of users from each subgroup.\n\nThis approach strengthens experiments by creating balanced groups. For instance,\nif user location is expected to affect the outcome metric, users can be\nstratified by location (e.g., urban vs. rural), followed by randomization within\neach location. This ensures a similar distribution of user attributes across\ntreatment and control groups, controlling for confounding factors—user traits\nthat impact both exposure to the new feature and the desired outcome. With\nbalanced groups, any observed differences in the outcome metric are more likely\ndue to the new feature itself, leading to more precise and reliable results.\n\n### Examples \n\n  - **Targeting Mobile App Engagement:** In an RCT to evaluate a new in-app\n    notification, user location (urban vs. rural) is suspected to influence user\n    response. Stratification by location, followed by randomization within each\n    stratum, can control for this factor.\n\n  - **Personalizing a Recommendation Engine:** When A/B testing a revamped\n    recommendation engine, past purchase history is hypothesized to influence\n    user response. Stratification by purchase history categories (e.g., frequent\n    buyers of clothing vs. electronics), followed by randomization within each\n    category, can account for this.\n\n### Advantages:\n\n  - **Reduced bias:** Stratification helps isolate the true effect of the new\n    feature by controlling for the influence of confounding factors. This leads\n    to more reliable conclusions about the feature's impact on user behavior.\n\n  - **Improved decision-making:** By pinpointing the feature's effect on\n    specific user groups (e.g., urban vs. rural in the notification example),\n    stratified randomization can inform decisions about targeted rollouts or\n    further iterations based on subgroup performance.\n\n### Disadvantages:\n\n  - **Increased complexity:** Designing and implementing stratified randomization\nrequires careful planning to choose the right stratification factors and ensure\nenough users within each stratum for valid analysis.\n\n  - **Need for larger sample sizes:** Maintaining balance across strata might\nnecessitate a larger overall sample size compared to simple random assignment.\n\n### Example with code\n\nLet's illustrate the concept of stratified randomization with a practical\nexample. Consider a scenario where we have data on 10,000 individuals, each\ndescribed by two continuous variables (x1 and x2) and two categorical variables\n(x3 and x4). We suspect that variable x3 might be a confounding factor\ninfluencing the outcome of our experiment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_data <- tibble::tibble(\n  x1 = rnorm(10000), \n  x2 = rnorm(10000),\n  x3 = rep(c(\"A\", \"B\"), 5000),\n  x4 = rep(c(\"C\", \"D\"), 5000)\n)\n\n# Create a Randomizer Object\nrandomized <- im::randomizer$new(\n  data = my_data, \n  seed = 12345, \n  max_attempts = 1000,\n  variables = c(\"x1\", \"x2\"), \n  standard = \"Not Concerned\",\n  group_by = \"x3\"\n)\n\n# Generate Balance Plot\nrandomized$balance_plot\n```\n\n::: {.cell-output-display}\n![](rct_basic_files/figure-html/stratified-1.png){width=672}\n:::\n:::\n\n\n\nIn this code, we're using the im::randomizer function to create an object that\nwill help us perform stratified randomization. We specify x3 as the variable to\nstratify by, ensuring that the treatment and control groups have a balanced\ndistribution of individuals from both categories of x3.\n\nBy incorporating stratified randomization into our experimental design, we can\neffectively control for the influence of variable x3, enhancing the internal\nvalidity of our study and allowing for more precise estimates of causal effects.\n\nIn conclusion, stratified randomization offers a powerful way to enhance the\nrigor and precision of experiments, particularly when dealing with potential\nconfounding factors. While it may introduce some additional complexity and\npotentially require larger sample sizes, the benefits in terms of internal\nvalidity and the ability to draw more nuanced conclusions often outweigh these\ndrawbacks. The thoughtful use of stratified randomization can be a valuable\nasset in the causal inference toolkit.\n\n::: {.callout-tip}\n## Learn more\n  - @chernozhukov2024applied Applied causal inference powered by ML and AI.\n:::",
    "supporting": [
      "rct_basic_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}