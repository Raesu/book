{
  "hash": "037655358771427fb8d21c3fb27a4c93",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayes 101\"\nfilters:\n  - shinylive\n---\n\n\n![Bayes' Rule](img/bayes.webp){.lightbox}\nIn the bustling marketplace of ideas that is modern data science, Bayesian\nstatistics stands out as a powerful and intuitive approach to understanding\nuncertainty and making decisions. For business data scientists, it offers a\nflexible framework that naturally incorporates prior knowledge, updates beliefs\nbased on new evidence, and quantifies uncertainty in a way that's both\nmathematically rigorous and intuitively appealing.\n\nNow, let's set some expectations. To truly thrive as a business data scientist,\nyou'll need more than a passing familiarity with statistics. Think of it this\nway: an analyst knows how to use a calculator, while a data scientist knows how\nthat calculator works—and can even build one from scratch to tackle the specific\nproblem at hand. Let me be clear: this chapter isn't a course in calculator\nconstruction. We won't delve into the nuts and bolts of how it's built, or even\nall the inner workings. Instead, our aim is to convince you that this particular\ncalculator is worth learning more about. We want to spark your curiosity, to\nshow you why this tool deserves a prime spot in your data science toolbox.\n\nIn this chapter, we'll explore the basics of Bayesian statistics, delve into\nBayes' rule, and examine why this paradigm is particularly well-suited for\nbusiness applications.\n\n## The Essence of Bayesian Thinking\n\nAt its core, Bayesian statistics is about updating our beliefs in light of new\nevidence. This process mirrors how we often think about problems in business: we\nstart with some prior knowledge or assumptions, gather data, and then update our\nunderstanding based on what we've learned.\n\nAs @kruschke2018bayesian eloquently put it, \"The main idea of Bayesian analysis\nis simple and intuitive. There are some data to be explained, and we have a set\nof candidate explanations. Before knowing the new data, the candidate\nexplanations have some prior credibilities of being the best explanation. Then,\nwhen given the new data, we shift credibility toward the candidate explanations\nthat better account for the data, and we shift credibility away from the\ncandidate explanations that do not account well for the data.\"\n\nThis perspective highlights a fundamental principle of Bayesian analysis: it's a\nprocess of reallocating credibility across possibilities. In a business context,\nthese \"possibilities\" might be different strategies, market scenarios, or\nparameter values in a model. As we gather more data, we adjust our beliefs about\nwhich possibilities are more or less likely to be true.\n\nThe Bayesian approach contrasts with the more traditional frequentist statistics\nin a fundamental way. While frequentists treat parameters as fixed (but unknown)\nquantities and data as random, Bayesians view parameters as random variables and\ndata as fixed once observed. This shift in perspective leads to more intuitive\ninterpretations of statistical results and allows for the incorporation of prior\nknowledge into our analyses.\n\n## Bayes' Rule: The Heart of Bayesian Statistics\n\nThe cornerstone of Bayesian statistics is Bayes' rule, named after the Reverend\nThomas Bayes. This elegant formula shows us how to update probabilities when we\nreceive new information. In its simplest form, Bayes' rule is expressed as:\n\n$$\nP(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n$$\n\nWhere:\n\n  - $P(A|B)$ is the posterior probability of A given B\n  - $P(B|A)$ is the likelihood of B given A\n  - $P(A)$ is the prior probability of A\n  - $P(B)$ is the marginal likelihood of B\n\nIn the context of parameter estimation, which is often our goal in business data\nscience, we can rewrite Bayes' rule as:\n\n$$\nP(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}\n$$\n\nWhere:\n\n  - $\\theta$ represents our parameter(s) of interest\n  - $D$ represents our observed data\n  - $P(\\theta|D)$ is the posterior distribution of our parameter given the data\n  - $P(D|\\theta)$ is the likelihood of the data given the parameter\n  - $P(\\theta)$ is our prior distribution for the parameter\n  - $P(D)$ is the marginal likelihood of the data\n  \nThis formulation clearly illustrates the process of reallocating credibility. We\nstart with our prior beliefs about the parameters $P(\\theta)$, consider how\nlikely the data are given those parameters $P(D|\\theta)$, and end up with an\nupdated (posterior) belief about the parameters $P(\\theta|D)$.  \n\n## The Case of the Declining User Engagement\n\nEven if you haven't formally studied Bayesian statistics, your brain is already\nwired to think like a Bayesian. To illustrate how this intuitive approach can be\napplied to real-world business problems, let's consider a scenario that data\nscientists in the tech sector frequently encounter: investigating a sudden\ndecline in user engagement and its potential impact on revenue.\n\nImagine you're a business data scientist at a high-growth tech company that\noffers a subscription-based productivity app. You've noticed a concerning trend:\ndaily active users (DAU) have dropped by 15% over the past month, and this is\nstarting to affect revenue. Your task is to identify the most likely cause of\nthis engagement drop and recommend actions to reverse the trend.\n\nLet's say we have four main hypotheses for the cause of the declining\nengagement:\n\n1.  A recent feature update (Feature)\n2.  Increased competition in the market (Competition)\n3.  Seasonal variation (Seasonality)\n4.  Changes in marketing spend (Marketing)\n\nBefore diving into the data, you have some prior beliefs about the likelihood of\neach cause, based on your experience and industry knowledge:\n\n  - Feature: 35% (feature updates can sometimes negatively impact user\n    experience)\n  - Competition: 25% (the market is becoming more saturated)\n  - Seasonality: 20% (there's often a summer slowdown in productivity app usage)\n  - Marketing: 20% (marketing budgets have been fluctuating)\n\nThis is your prior distribution. Now, as you investigate, you gather evidence:\n\n1.  User feedback shows mixed reactions to the recent feature update, with some\n    users reporting confusion about the new interface.\n2.  Market research indicates that while a major competitor launched a new\n    product, it hasn't gained significant market share yet.\n3.  Historical data shows a similar dip in engagement during the same period\n    last year, though not as pronounced.\n4.  Marketing spend has remained consistent over the past quarter.\n\nAs you collect this evidence, you update your beliefs about the likelihood of\neach cause. This is where Bayesian reasoning comes into play, allowing you to\nreallocate credibility based on the new information.\n\nAfter considering the evidence, you might update your beliefs as follows:\n\n  - Feature: 60% (user feedback suggests this is a significant factor)\n  - Competition: 8% (less likely given the market research)\n  - Seasonality: 30% (historical data supports this as a contributing factor)\n  - Marketing: 2% (unlikely given consistent spend)\n\nThis is your posterior distribution. You've reallocated credibility based on the\nevidence, increasing your belief that the feature update and seasonality are the\nprimary causes of the engagement drop.\n\n## The Case of Karl Broman's Socks: A Bayesian Adventure in Tiny Data\n\n![Karl Broman's Socks](img/socks.png){fig-alt=\"[Karl Broman's Socks](https://x.com/kwbroman/status/523221976001679360)\" fig-align=\"center\" width=50%}\n\nTo illustrate the power of Bayesian thinking, even with limited data, let's\nconsider an intriguing example from Rasmus Bååth's Blog, which he calls \"Tiny\nData, Approximate Bayesian Computation and the Socks of Karl Broman\" (see\n[original blog\npost](https://www.sumsar.net/blog/2014/10/tiny-data-and-the-socks-of-karl-broman/)).\nThe problem is deceptively simple: Given that [Karl Broman has 11 unique socks\nin his laundry](https://x.com/kwbroman/status/523221976001679360), how many\nsocks does he have in total? This is a perfect example of what Bååth calls \"Tiny\nData\" - a situation where we have very limited information but still need to\nmake an inference. It's a scenario that business data scientists often face,\nwhere decisions need to be made with incomplete information.\n\n### The Bayesian Approach to the Sock Problem\n\nBååth tackles this problem using Approximate Bayesian Computation (ABC), an\nintuitive albeit computationally intensive method (see @rubin1984bayesianly).\n\nIn this Bayesian adventure, we begin with two crucial pieces of information,\nencoded as prior probability distributions. The number of socks is a count\nvariable, so we'll employ a negative binomial data generating process. (We delve\ninto this topic in more detail in @sec-negative-binomial.) For now, let's follow\nBååth's lead and set ($\\mu = 30$) and (size = 4).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n# Dataframe of possible sock counts\nsock_counts <- data.frame(n_socks = 0:100)\n\n# Calculate probabilities from the negative binomial distribution\nsock_counts$probability <- dnbinom(sock_counts$n_socks, mu = 30, size = 4)\n\n# Create the histogram\nggplot(sock_counts, aes(x = n_socks, y = probability)) +\n  geom_col(fill = \"skyblue\", color = \"black\") +\n  theme_minimal() +\n  labs(x = \"Total Number of Socks\", y = \"Probability\") +\n  ggtitle(\"Prior Distribution for Total Number of Socks\")\n```\n\n::: {.cell-output-display}\n![](bayes101_files/figure-html/prior_socks-1.png){width=672}\n:::\n:::\n\n\nNext, we need to specify our beliefs about the proportion of socks that have a\npair. For this, we can use a beta data generating process. To remain consistent\nwith the blog post, we'll set shape1 = 15 and shape2 = 2. This distribution is\nskewed towards higher values, suggesting a belief that most socks in a laundry\npile are likely to be paired. The parameters indicate an expectation\naround 0.88, reflecting the common experience that unmatched socks are less\nfrequent.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a sequence of proportions from 0 to 1\nproportions <- seq(0, 1, length.out = 100)\n\n# Calculate density values from the beta distribution\ndensity_values <- dbeta(proportions, shape1 = 15, shape2 = 2)\n\n# Create the density plot\nggplot(data.frame(proportion = proportions, density = density_values),\n       aes(x = proportion, y = density)) +\n  geom_line(color = \"darkgreen\") +\n  theme_minimal() +\n  labs(x = \"Proportion of Paired Socks\", y = \"Density\") +\n  ggtitle(\"Prior Distribution for Proportion of Paired Socks\")\n```\n\n::: {.cell-output-display}\n![](bayes101_files/figure-html/prior_prop-1.png){width=672}\n:::\n:::\n\n\nThese prior distributions encapsulate our initial beliefs before we observe any\ndata. The Bayesian approach elegantly allows us to update these beliefs based on\nthe evidence, leading to more informed posterior distributions.\n\nNow, let's craft the R code for simulating draws from this data generating\nprocess.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(furrr)\nlibrary(patchwork)\n\nset.seed(123)\n\n# Enable parallel processing with the number of cores available\nplan(multisession, workers = availableCores())\n\n# Define the number of socks picked\nn_picked <- 11\n\n# Improved simulation function\nsimulate_socks <- function(n_picked) {\n  # Generate total number of socks from prior\n  n_socks <- rnbinom(1, mu = 30, size = 4)\n  \n  # Generate proportion of paired socks from prior\n  prop_pairs <- rbeta(1, shape1 = 15, shape2 = 2)\n  \n  # Calculate number of pairs and odd socks\n  n_pairs <- round(floor(n_socks / 2) * prop_pairs)\n  n_odd <- n_socks - n_pairs * 2\n  \n  # Simulate picking socks\n  socks <- rep(seq_len(n_pairs + n_odd), rep(c(2, 1), c(n_pairs, n_odd)))\n  picked_socks <- sample(socks, size = min(n_picked, n_socks))\n  sock_counts <- table(picked_socks)\n  \n  # Return results\n  tibble(\n    unique = sum(sock_counts == 1),\n    pairs = sum(sock_counts == 2),\n    n_socks = n_socks,\n    n_pairs = n_pairs,\n    n_odd = n_odd,\n    prop_pairs = prop_pairs\n  )\n}\n\n# Run simulations\nn_sims <- 100000\nsock_sim <- future_map_dfr(1:n_sims, ~simulate_socks(n_picked))\n\n# Filter for matching simulations (11 unique socks, 0 pairs)\npost_samples <- sock_sim %>%\n  filter(unique == 11, pairs == 0)\n```\n:::\n\n\nThis code implements the ABC method, which is a perfect illustration of the\n\"reallocation of credibility across possibilities\" that Kruschke and Liddell\ndescribe:\n\n1.  We define prior distributions for the total number of socks (negative\n    binomial) and the proportion of paired socks (beta). These represent our\n    initial beliefs about the possibilities.\n2.  We create a generative model that simulates picking socks from a laundry\n    pile.\n3.  We run this simulation many times (100,000 in this case), each time\n    generating a possible scenario.\n4.  We keep only those simulations that match our observed data (11 unique\n    socks, 0 pairs). This step is where we reallocate credibility, focusing on\n    the possibilities that are consistent with our observation.\n5.  We analyze the results by calculating median values from the retained\n    samples, which represent our updated beliefs.\n\n### Visualizing the Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prepare data for plotting\nprior_data <- sock_sim %>%\n  count(n_socks) %>%\n  mutate(prop = n / sum(n),\n         type = \"Prior\")\n\nposterior_data <- post_samples %>%\n  count(n_socks) %>%\n  mutate(prop = n / sum(n),\n         type = \"Posterior\")\n\nplot_data <- bind_rows(prior_data, posterior_data)\n\n# Plot prior and posterior distributions\np1 <- ggplot(plot_data, aes(x = n_socks, y = prop, fill = type)) +\n  geom_col(position = \"dodge\", alpha = 0.7) +\n  scale_fill_manual(values = c(\"Prior\" = \"lightgreen\", \"Posterior\" = \"skyblue\")) +\n  labs(title = \"Prior and Posterior Distributions of Total Socks\",\n       x = \"Number of Socks\", y = \"\", fill = \"Distribution\") +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n# Plot the posterior distribution of pairs and odd socks\np2 <- ggplot(post_samples, aes(x = n_pairs, y = n_odd)) +\n  geom_hex(bins = 30) +\n  scale_fill_viridis_c() +\n  labs(title = \"Joint Posterior Distribution of Pairs and Odd Socks\",\n       x = \"Number of Pairs\", y = \"Number of Odd Socks\") +\n  theme_minimal()\n\n# Combine plots\np1 / p2\n```\n\n::: {.cell-output-display}\n![](bayes101_files/figure-html/viz-1.png){width=672}\n:::\n:::\n\n\n### Results and Interpretation\n\nAfter running this model, our best guess (median of the posterior) is that Karl\nBroman has approximately:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate summary statistics\nsummary_stats <- post_samples %>%\n  summarize(\n    median_socks = median(n_socks),\n    median_pairs = median(n_pairs),\n    median_odd = median(n_odd),\n    ci_lower_socks = quantile(n_socks, 0.025),\n    ci_upper_socks = quantile(n_socks, 0.975)\n  )\n```\n:::\n\n\n  - Total socks: 45\n  (95% CI: 24 - 85)\n  - Pairs of socks: 19\n  - Odd socks: 6\n\n\n\nRemarkably, when Karl later revealed the actual numbers, it turned out there\nwere 21 pairs and 3 odd socks, totaling 45 socks. The estimate is surprisingly\nclose, considering we only had one piece of information to work with! The\nvisualizations provide additional insights:\n\nThe first plot shows how our beliefs about the total number of socks changed\nfrom the prior (green) to the posterior (blue) distribution after incorporating\nthe data. This is a clear visualization of the reallocation of credibility\nacross possibilities. The second plot illustrates the joint posterior\ndistribution of pairs and odd socks, showing the range of plausible combinations\ngiven our model and data.\n\nThis example beautifully illustrates several key aspects of Bayesian thinking:\n\n1.  Incorporation of prior knowledge: The model uses reasonable priors based on\n    general knowledge about sock ownership.\n2.  Handling uncertainty: The posterior distribution provides a range of\n    plausible values, not just a point estimate.\n3.  Learning from limited data: Even with just one piece of information (11\n    unique socks), we can make a surprisingly accurate inference.\n4.  Flexibility: The ABC approach allows us to work with a complex model that\n    would be difficult to handle with traditional methods.\n5.  Reallocation of credibility: We start with a wide range of possibilities and\n    narrow down to those most consistent with our observation.\n\nIn business contexts, we often face similar situations - limited data combined\nwith domain expertise or prior experience. The sock example, while whimsical,\ndemonstrates how Bayesian methods can be powerful in such real-world scenarios.\nAs we progress through this book, we'll explore how these principles can be\napplied to more complex business problems.\n\nAs Kruschke and Liddell (2018) point out, one of the key advantages of Bayesian\nanalysis is that \"the posterior distribution can be directly examined to see\nwhich parameter values are most credible, and what range of parameter values\ncovers the most credible values.\" This direct interpretation is particularly\nvaluable in business settings, where we often need to communicate results to\nnon-technical stakeholders.\n\nFor instance, in our sock example, we can straightforwardly say that \n\"there's a 95% probability that the total number of socks is between\n24 and 85.\"\nThis statement is intuitive and directly addresses the uncertainty in our\nestimate, which is crucial for informed decision-making.\nAdditionally, we can quantify probabilities for specific scenarios, such as:\n\"The probability that there are at least 15 pairs\nis 79%.\"\n\nMoreover, Bayesian methods naturally handle the \"small data\" scenarios that are\ncommon in business. While big data gets a lot of attention, many important\nbusiness decisions are made with limited information. The Bayesian framework\nallows us to start with prior knowledge (perhaps based on industry benchmarks or\nprevious experience), update this with whatever data is available, and still\nproduce meaningful results.\n\n### A Note on Priors\n\nIt's important to remember that the less data you have, the more influential\nyour priors become. Transparency about your priors is essential, as is\ninvestigating the sensitivity of your findings to different prior choices.\nHowever, don't shy away from using informative priors when justified by data.\nThe interactive dashboard below utilizes {shinylive} (see @shinylive) to run\nthese simulations directly in your browser, allowing you to experiment with\nvarious priors and observe their impact.\n\n```{shinylive-r}\n#| standalone: true\n#| viewerHeight: 800\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(furrr)\nlibrary(patchwork)\nlibrary(shinybusy)\n\n# Enable parallel processing with the number of cores available\nplan(multisession, workers = availableCores())\n\n# Define the number of socks picked\nn_picked <- 11\n\n# Improved simulation function\nsimulate_socks <- function(n_picked, n_socks) {\n  # Generate total number of socks from prior\n  n_socks <- rnbinom(1, mu = 30, size = 4)\n  \n  # Generate proportion of paired socks from prior\n  prop_pairs <- rbeta(1, shape1 = 15, shape2 = 2)\n  \n  # Calculate number of pairs and odd socks\n  n_pairs <- round(floor(n_socks / 2) * prop_pairs)\n  n_odd <- n_socks - n_pairs * 2\n  \n  # Simulate picking socks\n  socks <- rep(seq_len(n_pairs + n_odd), rep(c(2, 1), c(n_pairs, n_odd)))\n  picked_socks <- sample(socks, size = min(n_picked, n_socks))\n  sock_counts <- table(picked_socks)\n  \n  # Return results\n  tibble(\n    unique = sum(sock_counts == 1),\n    pairs = sum(sock_counts == 2),\n    n_socks = n_socks,\n    n_pairs = n_pairs,\n    n_odd = n_odd,\n    prop_pairs = prop_pairs\n  )\n}\n\nui <- fluidPage(\n  titlePanel(\"The Socks of Karl Broman\"),\n  use_busy_spinner(spin = \"fading-circle\"),  \n  sidebarLayout(\n    sidebarPanel(\n      numericInput(\n        inputId = \"seed\",\n        label = \"Random Seed\",\n        value = 123,\n        min = 1,\n        step = 1\n      ),\n      numericInput(\n        inputId = \"n_sims\",\n        label = \"Number of Simulations\",\n        value = 50000,\n        min = 100,\n        step = 100\n      ),\n      sliderInput(\n        inputId = \"mu\",\n        label = \"mu\",\n        min = 15,\n        max = 60,\n        value = 40,\n        step = 1\n      ),\n      sliderInput(\n        inputId = \"size\",\n        label = \"Size\",\n        min = 3,\n        max = 10,\n        value = 4,\n        step = 1\n      ),\n      sliderInput(\n        inputId = \"shape1\",\n        label = \"Shape 1\",\n        min = 2,\n        max = 20,\n        value = 15,\n        step = 1\n      ),\n      sliderInput(\n        inputId = \"shape2\",\n        label = \"Shape 2\",\n        min = 2,\n        max = 8,\n        value = 2,\n        step = 1\n      ),\n      actionButton(\"run_sim\", \"Run Simulation\")\n    ),\n    mainPanel(\n      plotOutput(\"prior_socks\"),\n      plotOutput(\"prior_prop\"),\n      plotOutput(\"distribution_plot\"),\n      plotOutput(\"joint_plot\")\n    )\n  )\n)\n\nserver <- function(input, output) {\n  \n  sock_sim <- eventReactive(input$run_sim, {\n    show_modal_spinner()  \n    on.exit(remove_modal_spinner())\n    set.seed(input$seed)  \n    future_map_dfr(1:input$n_sims, ~simulate_socks(n_picked))\n  })\n  \n  # Filter for matching simulations (11 unique socks, 0 pairs)\n  post_samples <- reactive({\n    sock_sim() %>% \n      filter(unique == 11, pairs == 0)\n  })\n  \n  # Prepare data for plotting\n  prior_data <- reactive({\n    sock_sim() %>%\n      count(n_socks) %>%\n      mutate(prop = n / sum(n),\n             type = \"Prior\")\n  })\n  \n  posterior_data <- reactive({\n    if(nrow(post_samples()) == 0) {\n      return(tibble(n_socks = numeric(), prop = numeric(), type = character())) \n    }\n    post_samples() %>%\n      count(n_socks) %>%\n      mutate(prop = n / sum(n),\n             type = \"Posterior\")\n  })\n  \n  output$prior_socks <- renderPlot({\n    # Dataframe of possible sock counts\n    sock_counts <- data.frame(n_socks = 0:100)\n    \n    # Calculate probabilities from the negative binomial distribution\n    sock_counts$probability <- dnbinom(sock_counts$n_socks,\n                                       mu = input$mu,\n                                       size = input$size)\n    \n    # Create the histogram\n    ggplot(sock_counts, aes(x = n_socks, y = probability)) +\n      geom_col(fill = \"skyblue\", color = \"black\") +\n      theme_minimal() +\n      labs(x = \"Total Number of Socks\", y = \"Probability\") +\n      ggtitle(\"Prior Distribution for Total Number of Socks\")\n  })\n  \n  output$prior_prop <- renderPlot({\n    # Create a sequence of proportions from 0 to 1\n    proportions <- seq(0, 1, length.out = 100)\n    \n    # Calculate density values from the beta distribution\n    density_values <- dbeta(proportions, shape1 = input$shape1,\n                            shape2 = input$shape2)\n    \n    # Create the density plot\n    ggplot(data.frame(proportion = proportions, density = density_values),\n           aes(x = proportion, y = density)) +\n      geom_line(color = \"darkgreen\") +\n      theme_minimal() +\n      labs(x = \"Proportion of Paired Socks\", y = \"Density\") +\n      ggtitle(\"Prior Distribution for Proportion of Paired Socks\")\n  })\n  \n  output$distribution_plot <- renderPlot({\n    req(prior_data(), posterior_data())\n    plot_data <- bind_rows(prior_data(), posterior_data())\n    \n    ggplot(plot_data, aes(x = n_socks, y = prop, fill = type)) +\n      geom_col(position = \"dodge\", alpha = 0.7) +\n      scale_fill_manual(values = c(\"Prior\" = \"lightgreen\", \"Posterior\" = \"skyblue\")) +\n      labs(title = \"Prior and Posterior Distributions of Total Socks\",\n           x = \"Number of Socks\", y = \"Proportion\", fill = \"Distribution\") +\n      theme_minimal() +\n      theme(legend.position = \"top\")\n  })\n  \n  output$joint_plot <- renderPlot({\n    req(post_samples())\n    if(nrow(post_samples()) > 0) {\n      ggplot(post_samples(), aes(x = n_pairs, y = n_odd)) +\n        geom_hex(bins = 30) +\n        scale_fill_viridis_c() +\n        labs(title = \"Joint Posterior Distribution of Pairs and Odd Socks\",\n             x = \"Number of Pairs\", y = \"Number of Odd Socks\") +\n        theme_minimal()\n    } else {\n      ggplot() + \n        annotate(\"text\", x = 0.5, y = 0.5, label = \"No matching simulations found\") +\n        theme_void()\n    }\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\n```\n\n\n## Conclusion\n\nBayesian statistics offers a powerful and intuitive framework for business data\nscience, aligning closely with how businesses make decisions. By incorporating\nprior knowledge, we can update our beliefs based on evidence and quantify\nuncertainty in a natural way. The concept of reallocating credibility across\npossibilities provides an intuitive way to think about learning from data.\n\nFurthermore, Bayesian methods are particularly well-suited to the \"small data\"\nscenarios often encountered in business. While \"big data\" garners much\nattention, many crucial business decisions hinge on limited information. The\nBayesian framework allows us to begin with prior knowledge, update it with\navailable data, and then communicate our findings in plain language.\n\nIn essence, Bayesian statistics provides a flexible and powerful approach for\ntackling the complex and often uncertain world of business decision-making.\n\n::: {.callout-tip}\n## Learn more\n  - @kruschke2018bayesian Bayesian data analysis for newcomers.\n  - @mcelreath2018statistical Statistical Rethinking: A Bayesian Course with\n    Examples in R and Stan\n  - @gelman2013bayesian Bayesian Data Analysis.\n::: \n",
    "supporting": [
      "bayes101_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}