{
  "hash": "bcd0cfb0bcea7ee7a6dcd7334ed00c3b",
  "result": {
    "engine": "knitr",
    "markdown": "<style>\na.glossary {\n  color: purple;\n  text-decoration: underline;\n  cursor: help;\n  position: relative;\n}\n\n/* only needed for popup = \"click\" */\n/* popup-definition */\na.glossary .def {\n  display: none;\n  position: absolute;\n  z-index: 1;\n  width: 200px;\n  bottom: 100%;\n  left: 50%;\n  margin-left: -100px;\n  background-color: #333;\n  color: white;\n  padding: 5px;\n  border-radius: 6px;\n}\n/* show on click */\na.glossary:active .def {\n  display: inline-block;\n}\n/* triangle arrow */\na.glossary:active .def::after {\n  content: ' ';\n  position: absolute;\n  top: 100%;\n  left: 50%;\n  margin-left: -5px;\n  border-width: 5px;\n  border-style: solid;\n  border-color: #333 transparent transparent transparent;\n}\n</style>\n\n\n## What Does it Mean to Be Data-Driven?\n\nIn today's tech-driven world, data is king. Every click, swipe, and search\ngenerates a breadcrumb of information. Alas, although most decision-makers want\nto be <a class='glossary' title='Describes decision-making that is informed by the analysis and interpretation of data.'>data-driven</a>, data does not speak for itself.\n\nSo, what does it mean to be <a class='glossary' title='Describes decision-making that is informed by the analysis and interpretation of data.'>data-driven</a>? At its core, it's about\nusing data to inform decisions, not just describe them. It's about moving beyond\n<a class='glossary' title='A statistical relationship between two variables, meaning they tend to change together. Correlation does not necessarily imply causation.'>correlation</a> – the \"what goes with what\" – and\nunderstanding causation, the \"why\" behind the patterns we see. To be truly\n<a class='glossary' title='Describes decision-making that is informed by the analysis and interpretation of data.'>data-driven</a>, there must be some level of evidence that the data\ncan provide that would make you choose a different path than the one you would\nhave otherwise taken.\n\nThis is where <a class='glossary' title='The process of drawing conclusions about cause-and-effect relationships from data. It goes beyond simple correlations to help us understand why patterns occur.'>causal inference</a> steps in.\n<a class='glossary' title='The process of drawing conclusions about cause-and-effect relationships from data. It goes beyond simple correlations to help us understand why patterns occur.'>Causal inference</a> is the science of drawing cause-and-effect\nconclusions from data. It allows us to answer questions like:\n\n  - Did a new marketing campaign actually drive sales, or was it launched during\n    a time when sales naturally increase?\n  - Will a new app feature increase user engagement, or will it just annoy\n    users?\n  - Is a chatbot the best way to reduce wait time and increase customer\n    satisfaction?\n\n<a class='glossary' title='The process of drawing conclusions about cause-and-effect relationships from data. It goes beyond simple correlations to help us understand why patterns occur.'>Causal inference</a> is the missing piece of the\n<a class='glossary' title='Describes decision-making that is informed by the analysis and interpretation of data.'>data-driven</a> puzzle. It lets us move beyond\n<a class='glossary' title='A statistical relationship between two variables, meaning they tend to change together. Correlation does not necessarily imply causation.'>correlation</a> and identify the true drivers of\nbusiness outcomes. <a class='glossary' title='The process of quantifying the causal effect of a particular program, policy, or business decision.'>Impact evaluation</a> builds on this, putting\nnumbers to the effects of a program, policy, or intervention. Think of it as\nmeasuring the impact of a specific business decision.\n\nData can also be used to improve the ongoing operations and effectiveness of a\nprogram, a process known as\n<a class='glossary' title='The process of using data to continuously assess, refine, and optimize a program’s operations and effectiveness.'>program improvement</a>. This involves\ncontinuously collecting data on how the program is running, identifying any\nbottlenecks or areas for enhancement, and making adjustments as needed. Think of\nprogram improvement as an ongoing feedback loop, constantly refining and\noptimizing the program based on real-world data.\n\nNow, let's delve a bit deeper. Imagine you're a decision-maker at a social media\ncompany pondering a new feature. You have data showing that users who engage\nwith the feature spend more time on the platform. This is a\n<a class='glossary' title='A statistical relationship between two variables, meaning they tend to change together. Correlation does not necessarily imply causation.'>correlation</a>, but it doesn't tell the whole story.\nWhat if those users were already naturally the most engaged?\n\nThis is where the concept of the counterfactual becomes crucial. The\ncounterfactual is what would have happened if we hadn't implemented the new\nfeature – it's the <a class='glossary' title='The result that would have happened to a unit (a person, company, etc.) under a particular treatment or condition. In r glossary(&quot;causal inference&quot;), we are often interested in the difference between the potential outcome under treatment and the potential outcome under control.'>potential outcome</a> had we not made the change.\nWhile Jerzy Neyman hinted at this idea in 1923 [see @neyman1923applications],\nDonald Rubin fully developed the concept in the 1970s [see\n@rubin1974estimating; also @rubin1978bayesian] . Given that we can only observe\none potential outcome for each unit, the counterfactual is inherently missing\ndata. Hence, causal inference can be viewed as a missing data problem. For a\nreview of variety of causal inference methods from this perspective see\n@ding2018causal.\n\nChoosing the right counterfactual is critical for drawing valid causal\nconclusions. The wrong counterfactual can lead to misleading results and\npotentially disastrous business decisions. We'll explore these challenges and\ndifferent approaches to constructing counterfactuals in the coming chapters.\n\nBy understanding <a class='glossary' title='The process of drawing conclusions about cause-and-effect relationships from data. It goes beyond simple correlations to help us understand why patterns occur.'>causal inference</a> and the importance of\ncounterfactuals, you'll be well on your way to leveraging the true power of data\nto make informed decisions for your business. However, choosing the wrong\ncounterfactual can have serious consequences. Here are some classic examples:\n\n  - *Before-and-After Studies:* Imagine evaluating a job training program by\n    comparing participants' income before and after participation. What if the\n    economy was improving during that time, and their income would have\n    increased anyway? A simple before-and-after comparison can't account for\n    these external factors.\n\n  - *Self-Selection Bias:* Suppose you want to assess the effect of a new\n    exercise app. You compare those who chose to use the app to those who\n    didn't. What if people who downloaded the app were already more motivated to\n    exercise? This self-selection bias can skew the results, making the app look\n    more effective than it truly is.\n\nRemember, in order to design a good study to inform decisions, we need to know\nwhich decisions we are trying to inform. This clarity about the decision at hand\nallows us to choose the right counterfactual scenario for comparison. By\ncarefully considering <a class='glossary' title='The result that would have happened to a unit (a person, company, etc.) under a particular treatment or condition. In r glossary(&quot;causal inference&quot;), we are often interested in the difference between the potential outcome under treatment and the potential outcome under control.'>potential outcomes</a>\nand constructing strong counterfactuals, we can leverage the power of data to\nmake informed choices and drive better business results.\n\n::: {.callout-tip}\n## Learn more\n@li2023bayesian Bayesian causal inference: a critical review.\n:::\n\n\n\n",
    "supporting": [
      "chapter_01_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}