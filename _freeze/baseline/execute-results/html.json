{
  "hash": "e9cebf5270bb7e7e58a9323d4a232697",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Baseline Equivalence\"\nshare:\n  permalink: \"https://book.martinez.fyi/baseline.html\"\n  description: \"Business Data Science: What Does it Mean to Be Data-Driven?\"\n  linkedin: true\n  email: true\n  mastodon: true\n---\n\n\n\n\n<img src=\"img/baseline.jpg\" align=\"right\" height=\"280\" alt=\"Baseline Equivalence\" />\n\nIn the quest to discern the true impact of an intervention, we must first\nestablish a level playing field. The concept of **baseline equivalence** serves\nthis purpose, ensuring that the groups under comparison are similar enough in\nkey observed characteristics before the intervention takes place. Any\ndiscrepancies at baseline could muddy the waters, making it difficult to isolate\nthe intervention's effect from pre-existing differences.\n\nBaseline equivalence is particularly crucial in scenarios where sample sizes are\nsmall or when we're dealing with observational studies. Let's say a company\nwants to evaluate a new algorithm designed to boost user engagement. If the\ngroup exposed to this new algorithm (the treatment group) already exhibited\nhigher engagement levels than the control group prior to the experiment, any\nobserved increase could simply be a continuation of their existing behavior, not\nnecessarily a testament to the algorithm's effectiveness.\n\n## Gauging Baseline Equivalence {#sec-baseline}\n\nTo ascertain baseline equivalence, we turn to pre-intervention outcomes and\nother relevant observables. A common approach is to calculate the **effect\nsize**, a standardized measure of the magnitude of an effect.\n\n\nFor **continuous variables**, Hedges' g statistic is a popular choice\n(@hedges1981distribution):\n\n\n$$\ng = \\frac{\\omega(y_t-y_c)}{\\sqrt{\\frac{(n_t - 1) s_t^2 + (n_c - 1) s_c^2}{n_t+n_c - 2}}}\n$$\nwhere \n\n- $y_t$ is the mean for the treatment group\n- $y_c$ is the mean for the comparison group\n- $n_t$ is the sample size for the treatment group\n- $n_c$ is the sample size for the comparison group\n- $s_t$ is the standard deviation for the treatment group\n- $s_c$ is the standard deviation for the comparison group\n- $\\omega := 1 - \\frac{3}{4(n_t+n_c)-9}$ is the small sample size correction.\n\nFor **binary outcomes**, Cox's index comes into play (see @cox1972regression):\n\n$$\nd = \\omega \\left[ \\ln\\left(\\frac{p_t}{1-p_t}\\right) - \\ln\\left(\\frac{p_c}{1-p_c}\\right) \\right]\n$$\nwhere:\n\n- $p_t$ is the the mean of the outcome in the intervention group\n- $p_c$ is the mean of the outcome in the comparison group\n- $\\omega := 1 - \\frac{3}{4(n_t+n_c)-9}$ is the small sample size correction.\n\nThe general rule of thumb is that an absolute effect size greater than 0.25\nsignals a lack of baseline equivalence, and statistical adjustments are unlikely\nto fully remedy the situation. If the absolute effect size lies between 0.05\nand 0.25, statistical adjustments become necessary. An absolute effect size\nbelow 0.05 indicates strong evidence of baseline equivalence.\n\n## Linking Baseline Equivalence to Potential Outcomes\n\nThe concept of baseline equivalence is intimately connected to the potential\noutcomes framework we discussed in @sec-potential. Baseline equivalence supports\nthe crucial ignorability assumption in the potential outcomes framework, which\nstates that treatment assignment is independent of the potential outcomes given\nobserved covariates. When groups are equivalent at baseline, it's more plausible\nthat any differences in outcomes are due to the treatment rather than unobserved\nconfounders.\n\nBy striving for baseline equivalence, we're essentially attempting to create\nconditions that allow us to more accurately estimate the causal effects defined\nin the potential outcomes framework. This connection underscores the importance\nof assessing and establishing baseline equivalence in any causal inference\nstudy, whether experimental or observational.\n\n## The {im} Package in R\n\nThe R package {im} package provides a convenient way to check baseline\nequivalence using the `im::checkBaseline` function and visualize the results\nwith `im::balancePlot`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\ndata <- data.frame(\n  time_spent_in_app = rnorm(1000, mean = 60, sd = 15),\n  # Continuous\n  premium_subscriber = rbinom(1000, 1, 0.2),\n  # Binary\n  device_type = factor(sample(\n    c(\"iOS\", \"Android\", \"other\"), 1000, replace = TRUE\n  )),\n  # Factor\n  treatment = factor(sample(c(\n    \"control\", \"treatment\"\n  ), 1000, replace = TRUE))\n)\n\n# Check baseline equivalence\nbaseline_results <- im::checkBaseline(\n  data,\n  variables = c(\"time_spent_in_app\", \"premium_subscriber\", \"device_type\"),\n  treatment = \"treatment\"\n)\n\nim::balancePlot(data = baseline_results)\n```\n\n::: {.cell-output-display}\n![](baseline_files/figure-html/baseline-1.png){width=672}\n:::\n:::\n\n\n\n\n## The Importance of Baseline Equivalence\n\nWhether your study design is experimental or observational, the principle of\nbaseline equivalence should always be top of mind. It's a fundamental building\nblock for drawing valid causal inferences.\n\nRemember, we can only assess baseline equivalence for the characteristics we can\nmeasure. It's crucial to consider the possibility of unobservable factors that\nmight differ between groups at baseline and potentially bias our findings. By\nacknowledging and addressing these potential confounders, we strengthen the\nrigor and reliability of our causal analyses.\n\n\n::: {.callout-tip}\n## Learn more\n  - @wwc_baseline What Works Clearinghouse Baseline Equivalence Standard.\n  - @anderson2018baseline Baseline Equivalence: What it is and Why it is Needed.\n:::\n",
    "supporting": [
      "baseline_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}