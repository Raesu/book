{
  "hash": "b64f0da9e49a69adf49481abe1e096ef",
  "result": {
    "engine": "knitr",
    "markdown": "## Bayesian Linear Regression\n\nWhile Ordinary Least Squares (OLS) is a popular frequentist method for linear\nregression, the Bayesian approach is arguably better suited for informing\nbusiness decisions.\n\nOLS aims to find the line that minimizes the sum of squared differences between\nobserved and predicted values. It treats model parameters as fixed but unknown\nquantities, estimating them by minimizing residuals. Inference relies on\nhypothesis testing and p-values to assess the significance of relationships.\nHowever, this approach can lead to a rigid focus on statistical significance\nrather than practical relevance.\n\nA Bayesian Linear Model, while similar in structure to OLS, views parameters as\nrandom variables with probability distributions reflecting uncertainty. Prior\ndistributions incorporate existing knowledge or assumptions, and Bayes' theorem\ncombines this prior information with observed data to estimate the posterior\ndistribution of parameters. Inference focuses on posterior probabilities to\nquantify uncertainty and interpret the strength of evidence.\n\nThe Bayesian approach offers several advantages for business decision-making:\n\n  - **Incorporating Prior Knowledge:** Bayesian models allow you to explicitly\n    include prior knowledge or beliefs about the parameters, which can be\n    valuable in business contexts where historical data or expert opinions\n    exist.\n\n  - **Learning from New Data:** The Bayesian framework naturally shows how new\n    data updates and refines your understanding of the relationships between\n    variables.\n\n  - **Thinking in Bets:** Instead of relying on the binary and often arbitrary\n    concept of statistical significance, Bayesian analysis encourages thinking\n    in terms of probabilities and bets. This aligns well with business\n    decisions, where you often need to weigh potential risks and rewards.\n\n  - **Practical Significance:** While anything can be statistically significant\n    with a large enough sample size, Bayesian analysis focuses on the magnitude\n    and probability of effects that are practically meaningful for your business\n    goals. Even if a result isn't statistically significant, it could still be a\n    good bet if the posterior probability of a meaningful impact is sufficiently\n    high.\n\nThe Bayesian approach embraces the inherent uncertainty in data analysis,\nproviding a richer and more nuanced understanding of the relationships between\nvariables, ultimately leading to more informed and effective business decisions.\n\n\n### An example with synthetic data: \n\nImagine that you are faced with a decision: should you discontinue a product?\nYou would like to keep the product if, and only if, its impact on your outcome\nof interest is at least 0.1. To help you make this decision, you've conducted a\nwell-designed experiment. Let's illustrate this with some synthetic data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nset.seed(9782)\nN  <- 200\nfake_data <- tibble::tibble(\n  x = rnorm(n = N, mean = 0, sd = 1),\n  t = sample(x = c(T,F), size = N, replace = T, prob = c(0.5,0.5)),\n  e = rnorm(n = N, mean = 0, sd = 0.4)\n  ) %>% \n  mutate(y = 7.1 + 0.6*x + 0.02*t + e) # <1>\n```\n:::\n\n\n1.  Note that the true impact is 0.02, suggesting that the correct decision\n    would be to not discontinue the product. However, what happens if you\n    analyze this data using a traditional frequentist approach?\n    \n#### Frequentist approach: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(broom)\n\nlm1 <- lm(data = fake_data, formula = y ~ x + t) %>% \n  tidy(., conf.int=T, conf.level=0.95) %>% \n  filter(term==\"tTRUE\") \n\nplot <- ggplot(data = lm1, aes(y=estimate, x= term)) +\n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +\n  geom_hline(yintercept = 0, linetype = \"dotted\", color = \"blue\") + \n  scale_y_continuous(breaks = seq(-0.2, 0.2, by = 0.02)) +\n  theme_bw(base_size = 18) +\n  xlab(\"\") + \n  ylab(\"Impact\") + \n  theme(\n  axis.text.x = element_blank(),\n  axis.ticks.x = element_blank())\n\nplot\n```\n\n::: {.cell-output-display}\n![](blm_files/figure-pdf/OLS-1.png){fig-pos='H'}\n:::\n:::\n\n\nIn this case, the point estimate is 0.08, the p-value\n0.17 is greater than 0.05, and the 95% confidence interval\nranges from -0.04 to 0.19. How would a\ndecision-maker typically use this information? Unfortunately, many might decide\nto discontinue the product, misinterpreting the results [see\n@chandler2020speaking].\n\n::: {.callout-important title=\"The null ritual, @gigerenzer2004null:\"}\n1.  Set up a statistical null hypothesis of \"no mean difference\" or \"zero\n    correlation.\" Don't specify the predictions of your research hypothesis or\n    of any alternative substantive hypotheses.\n2.  Use 5% as a convention for rejecting the null. If significant, accept your\n    research hypothesis.\n3.  Always performing this procedure.\n:::\n\nThis problem was so widespread that in 2016, the American Statistical\nAssociation issued a statement cautioning against this practice [see\n@wasserstein2016asa]. Confidence intervals are also frequently misinterpreted\n[see @hoekstra2014robust].\n\n::: {.callout-important title=\"Incorrect interpretations:\"}\n1.  The probability that the true mean is greater than 0 is at least 95%.\n2.  The probability that the true mean equals 0 is smaller than 5%.\n3.  The “null hypothesis” that the true mean equals 0 is likely to be incorrect.\n4.  There is a 95% probability that the true mean lies between\n    -0.04 and 0.19.\n5.  We can be 95% confident that the true mean lies between\n    -0.04 and 0.19.\n6.  If we were to repeat the experiment over and over, then 95% of the time the\n    true mean falls between 0.1 and 0.19.\n:::\n\n::: {.callout-tip title=\"Correct interpretations:\"}\nA particular procedure, when used repeatedly across a series of hyptothetical\ndata sets, yields intervals that contain the true parameter value 95% of the\ncases. The key is that the CIs do not provide a statement about the parameter as\nit relates to the particular sample at hand.\n:::\n\nThis example starkly illustrates the disconnect between what decision-makers\nwant to say and what a frequentist approach allows them to say. The good news?\nBayesian methods offer a way to answer business questions directly and in plain\nlanguage.\n\n\n#### Bayesian approach: \n\nThe Bayesian approach to linear regression fundamentally shifts how we interpret\nand utilize data in decision-making. Rather than relying on point estimates and\np-values, it focuses on understanding the probability distributions of\nparameters, providing a richer, more nuanced picture.\n\nIn a Bayesian Linear Model, parameters are viewed as random variables with their\nown probability distributions. This perspective allows us to incorporate prior\nknowledge into the model: prior distributions reflect existing knowledge or\nbeliefs about parameters before observing the current data, which can be based\non historical data, expert opinions, or theoretical considerations. The\nlikelihood represents the probability of the observed data given the parameters,\nsimilar to the frequentist approach. Posterior distributions combine the prior\ndistribution and the likelihood using Bayes' theorem, reflecting updated beliefs\nabout the parameters after observing the data. The beauty of the Bayesian\napproach lies in its flexibility and adaptability. As new data becomes\navailable, the posterior distribution from one analysis can serve as the prior\nfor the next, continually refining our understanding.\n\nBusiness decisions often leverage historical data and expert judgment, and\nBayesian models explicitly incorporate this information, leading to more\ninformed and credible inferences. Bayesian analysis naturally adapts to new\ninformation. As fresh data is collected, the model updates its estimates,\nproviding a dynamic and current understanding of the business environment.\nInstead of fixating on binary outcomes (significant vs. non-significant),\nBayesian analysis assesses probabilities, aligning perfectly with the real-world\ndecision-making process, which is inherently probabilistic and involves weighing\nrisks and rewards. Bayesian models emphasize the magnitude and probability of\neffects that matter in practice. This focus is crucial in business, where even\nsmall but reliable improvements can have substantial impacts.\n\n\nThe {im} package fits a Bayesian linear model using [weakly informative\npriors](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations) for\nthe covariates and allows the user to set more informative priors for the impact\nof the intervention. If $y$ is the outcome of interest, the model is specified\nas follows:\n\n$$\n\\begin{aligned}\ny & \\sim N(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + X^\\star\\beta + \\color{red}{\\eta} t\n\\end{aligned}\n$$\n\nWe standardize the data as follows:\n\n$$\n\\begin{aligned}\ny^\\star & = \\frac{y - \\mu_y}{\\sigma_y} \\\\\n & \\sim N(\\mu^\\star, \\sigma^\\star) \\\\\n\\mu^\\star & = \\alpha^\\star + \\frac{X - \\mu_X}{\\sigma_X} \\beta^\\star +\n \\eta^\\star t \\\\\n \\alpha^\\star & \\sim N(0,1) \\\\\n \\beta^\\star & \\sim N(0,1) \\\\\n \\color{red}{\\eta^\\star} & \\color{red}{\\sim N(\\mu_\\eta, \\sigma_\\eta)} \\\\\n \\sigma^\\star & \\sim N^+(0,1) \\\\\n\\end{aligned}\n$$\n\n\nTherefore\n\n$$\n\\begin{aligned}\n\\frac{y - \\mu_y}{\\sigma_y} & = \\alpha^\\star +\n\\frac{X - \\mu_X}{\\sigma_X} \\beta^\\star + \\eta^\\star t \\\\\ny & = (\\alpha^\\star +\n\\frac{X - \\mu_X}{\\sigma_X} \\beta^\\star + \\eta^\\star t) \\sigma_y + \\mu_y \\\\\n\\color{red}\\eta = \\eta^\\star \\sigma_y\n\\end{aligned}\n$$\n\n\nNotice that if you have better priors, you should use them. To use this simple\nmodel, you just need to run the following code:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(im)\n\nfitted_blm <- blm$new(\n  y = \"y\", \n  x = c(\"x\"),\n  treatment = \"t\", \n  data = fake_data, \n  eta_mean = 0,\n  eta_sd = 0.5\n)\n```\n:::\n\n\n\nIt is always a good idea to look at the traceplot. A traceplot is a diagnostic\n tool used to visualize the \"path\" that a Markov Chain Monte Carlo (MCMC)\n sampler takes as it explores the parameter space. It helps assess the\n convergence and mixing of the chains, which is crucial for ensuring\n reliable inference from the model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted_blm$tracePlot()\n```\n\n::: {.cell-output-display}\n![](blm_files/figure-pdf/tracePlot-1.png){fig-pos='H'}\n:::\n:::\n\n\n\n1. **Assessing Convergence:**\n\nA well-converged chain should exhibit a \"hairy caterpillar\" pattern, \n where the trace fluctuates around a stable value without any trends or drifts. \n This indicates that the sampler has adequately explored the parameter space \n and reached a stationary distribution.\n\nConversely, non-converging chains might show trends, jumps, or slow mixing, \n suggesting that the sampler is stuck in a local region or hasn't adequately \n explored the posterior distribution. Inferences drawn from such chains can be \n unreliable and misleading.\n\n2. **Diagnosing Mixing:**\n\nGood mixing implies that the chains effectively explore the entire parameter \n space and don't get stuck in local regions. This is visually represented by\n well-intertwined lines from different chains on the traceplot.\n\nPoorly mixed chains show distinct separation among lines, indicating they\n haven't adequately explored the entire posterior distribution.\n This can lead to biased and inaccurate estimates of the parameters and\n their uncertainty.\n\n3. **Identifying Issues:**\n\nTraceplots can reveal potential issues in the model specification, priors, \n or MCMC settings. For example, highly correlated parameters might exhibit \n synchronized movement in the traceplot, suggesting a dependence relationship \n that needs further investigation.\n \nOverall, examining traceplots is a valuable diagnostic step in Bayesian\n statistical analysis. They provide valuable insights into the convergence\n  and mixing of MCMC chains, aiding in the valid and reliable interpretation\n   of the model results.\n\nIt is prudent to verify that our model's data generating process is compatible\n with the data used to fit the model. To do this, we can compare the kernel\n density of draws from the posterior distribution to the density of our data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted_blm$ppcDensOverlay(n = 50)\n```\n\n::: {.cell-output-display}\n![](blm_files/figure-pdf/ppcDensOverlay-1.png){fig-pos='H'}\n:::\n:::\n\n\n\nThe next step is to use the fitted Bayesian model to answer our business\nquestion directly. In this example, we want to determine the likelihood that the\nproduct's impact is at least $0.01$. We can calculate this probability with a\nsingle line of code:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted_blm$posteriorProb(threshold = 0.01)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGiven the data, we estimate that the probability that the effect is more than 0.01 is 89%.\n```\n\n\n:::\n:::\n\n\n\nWith this information, we can make a much more informed decision about whether\nto keep the product than if we were merely assessing the rejection of a null\nhypothesis. Moreover, we may care about multiple thresholds for this decision.\nFor instance, if the impact exceeds $0.2$, we might consider doubling our\ninvestment. \n\n::: {.content-visible when-format=\"html\"}\n\nThe {im} package enables the creation of interactive visualizations that\neffectively demonstrate our data insights and summarize the risks associated\nwith various decisions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted_blm$vizdraws(breaks = c(0.01, 0.2),\n                    break_names = c(\"Discontinue\", \"Keep\", \"Double down\"),\n                    display_mode_name = TRUE)\n```\n:::\n\n\n\nThe plot generated by this code not only answers our business question directly\nbut also illustrates how much we have learned from the data and how our initial\npriors have evolved. This comprehensive view is crucial for making better\nbusiness decisions.\n\n:::\n\nBayesian analysis provides probabilities directly aligned with decision-making\nneeds. For example, if the probability that the product's impact exceeds $0.01$\nis low, we can confidently discontinue it. Conversely, if there's a reasonable\nprobability of a positive impact, we might decide to retain the product,\npotentially conducting further investigations or collecting more data.\n\nIn conclusion, the Bayesian approach offers a powerful, flexible, and intuitive\nframework for business decision-making. By focusing on probabilities and\nincorporating prior knowledge, it provides a clearer and more practical basis\nfor making informed decisions in an uncertain world. This methodology enhances\nour ability to navigate uncertainty, ultimately leading to more effective and\nstrategic business outcomes.",
    "supporting": [
      "blm_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}