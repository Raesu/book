---
title: "Bayesian Additive Regression Trees (BART)"
---

## BART: Bayesian Additive Regression Trees

BART is a Bayesian nonparametric, machine learning, ensemble predictive modeling
method introduced by @chipman2010bart. It can be expressed as:

$$
Y = \sum_{j=1}^m g(X; T_j, M_j) + \epsilon, \quad \epsilon \sim N(0, \sigma^2)
$$

where $g(X; T_j, M_j)$ represents the $j$-th regression tree with structure
$T_j$ and leaf node parameters $M_j$. The model uses $m$ trees, typically set to
a large number (e.g., 200), with each tree acting as a weak learner. BART
employs a carefully designed prior that encourages each tree to play a small
role in the overall fit, resulting in a flexible yet robust model.

@hill2011bayesian proposed using BART for causal inference, recognizing that its
flexibility in modeling complex response surfaces made it well-suited for
estimating causal effects, particularly in observational studies with many
covariates. The key idea was to use BART to model the response surface:

$$
E[Y | X, Z] = f(X, Z)
$$

where $Y$ is the outcome, $X$ are the covariates, and $Z$ is the treatment
indicator. The causal effect can then be estimated as:

$$
\begin{aligned}
\tau(x) & = E[Y | X=x, Z=1] - E[Y | X=x, Z=0] \\
 & = f(x, 1) - f(x, 0)
\end{aligned}
$$

This formulation leverages BART's inherent ability to automatically capture
intricate interactions and non-linear relationships, making it a potent tool for
causal inference, especially in high-dimensional scenarios.

The effectiveness of BART in causal inference has been further validated in
recent competitions. @thal2023causal report on the 2022 American Causal
Inference Conference (ACIC) data challenge, where BART-based methods were among
the top performers, particularly for estimating heterogeneous treatment effects.
They found that BART's regularizing priors were especially effective in
controlling error for subgroup estimates, even in small subgroups, and its
flexibility in modeling confounding relationships was crucial for improved
causal inference in complex scenarios.

## Example with a Single Covariate

To illustrate how BART can be used for estimating the impact of an intervention,
@hill2011bayesian presents a simple example:


$$
\begin{aligned}
Z &\sim \mbox{Bernoulli}(0.5) \\
X | Z = 1 &\sim N(40,10^2) \\
X | Z = 0 &\sim N(20,10^2) \\
Y(0) | X &\sim N(72 + 3\sqrt{X},1) \\
Y(1) | X &\sim N(90 + exp(0.06X),1) 
\end{aligned}
$$

```{r truth, message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
library(broom) 

# 1. Define True Outcome Functions
f_treated <- function(x) 90 + exp(0.06 * x)
f_control <- function(x) 72 + 3 * sqrt(x)

# 2. Visualize True Outcome Functions
ggplot(data.frame(x = 6:62), aes(x = x)) +  # Expanded x range for clarity
  stat_function(fun = f_control, aes(color = "Truth - Control"), linewidth = 1) +  
  stat_function(fun = f_treated, aes(color = "Truth - Treatment"), linewidth = 1) +
  scale_color_manual(values = c("red", "blue")) +
  labs(title = "True Outcome Functions", x = "X", y = "Y", color = "") +
  theme_bw() +   
  theme(legend.position = "bottom")  
```
We can generate a sample from that data generating process as follows:

```{r sample, warning=FALSE, message=FALSE}
set.seed(123)
n_samples <- 120

simulated_data <- tibble(
  treatment_group = sample(c("Treatment", "Control"), size = n_samples, replace = TRUE),
  is_treated = treatment_group == "Treatment",
  X = if_else(is_treated, rnorm(n_samples, 40, 10), rnorm(n_samples, 20, 10)),
  Y1 = rnorm(n_samples, mean = f_treated(X), sd = 1),  
  Y0 = rnorm(n_samples, mean = f_control(X), sd = 1),  
  Y = if_else(is_treated, Y1, Y0),
  true_effect = Y1 - Y0
)


# 4. Visualize Simulated Data with True Functions
ggplot(simulated_data, aes(x = X, y = Y, color = treatment_group)) +
  geom_point(size = 2) + 
  stat_function(fun = f_control, aes(color = "Truth - Control")) +
  stat_function(fun = f_treated, aes(color = "Truth - Treatment")) +
  scale_color_manual(values = c("Truth - Control" = "red", "Truth - Treatment" = "blue")) +  # Mapping colors correctly
  labs(title = "Simulated Data with True Outcome Functions", 
       x = "X", y = "Y", color = "") +
  theme_bw() +
  theme(legend.position = "bottom") 

cat(glue::glue("The true Sample Average Treatment Effect is {round(mean(simulated_data$true_effect),2)}"))
```

Notice that in our sample, there is not very good overlap for low and high
values of X. This means that we will have to do a lot of extrapolation when
doing inference for those cases, which is a common challenge in causal
inference. Now, suppose we just fit OLS to the model to try to estimate the
average treatment effect:

```{r ols}
linear_model <- lm(Y ~ X + is_treated, data = simulated_data)
lm_fit <- broom::tidy(linear_model)
lm_fit
cat(glue::glue("A linear model finds an Average Treatment Effect equal to {round(lm_fit$estimate[lm_fit$term=='is_treatedTRUE'],2)}"))
```

It's important to note that the linear model is misspecified given the true
nonlinear relationships, which contributes to its poor performance. Let's add
the findings from OLS to our plot:

```{r ols_plot, warning=FALSE}
prediction_data <- expand.grid(
  X = seq(min(simulated_data$X), max(simulated_data$X), length.out = 1000),
  is_treated = c(TRUE, FALSE)
) %>%
  mutate(
    treatment_group = if_else(is_treated, "Treatment", "Control"),
    linear_prediction = predict(linear_model, newdata = .)
  )

# 6. Visualize Simulated Data, True Functions, and Linear Predictions
ggplot() +
  geom_point(data = simulated_data, aes(x = X, y = Y, color = treatment_group), size = 2) +
  stat_function(fun = f_control, aes(color = "Truth - Control")) +
  stat_function(fun = f_treated, aes(color = "Truth - Treatment")) +
  geom_line(data = prediction_data, aes(x = X, y = linear_prediction, 
                                        color = treatment_group), linetype = "dashed") +
  scale_color_manual(values = c("Truth - Control" = "red",  
                                "Truth - Treatment" = "blue",
                                "Control" = "red",           
                                "Treatment" = "blue")) +    
  labs(title = "Simulated Data, True Functions, and Linear Model Predictions",
       x = "X", y = "Y", color = "") +
  theme_bw() +
  theme(legend.position = "bottom") 
```
To fit BART we can use the [{stochtree}](https://stochastictree.github.io/stochtree-r/) package.

```{r fit_bart, warning=FALSE, message=FALSE}
xstart <- 20-15
length <- 4
text_left <- 26-15
yVals <- seq(110,130,by=4)

X_train <- simulated_data %>% 
                    select(X, is_treated) %>% 
                    as.matrix()

X_test <- prediction_data %>% 
                    select(X, is_treated) %>% 
                    as.matrix()

bart_model <-
  stochtree::bart(X_train = X_train,
                  X_test = X_test,
                  y_train = simulated_data$Y,
                  num_burnin = 1000,
                  num_mcmc = 2000)

```

After fitting, it is important to examine the traceplot of $\sigma^2$ to assess
if the model has converged. We can do this by running the following code:

```{r traceplot}
trace_plot_data <-
  tibble(
    iteration = 1:length(bart_model$sigma2_samples),
    sigma2_samples = bart_model$sigma2_samples
  )
ggplot(aes(x = iteration, y = sigma2_samples), data = trace_plot_data) +
  geom_line(color = "blue", alpha = 0.5) +
  labs(title = "Trace Plot for sigma^2",
       x = "Iteration",
       y = "sigma^2") +
  theme_bw()

```

Now we can plot the predictions from BART and compare them to the truth with the
following code:

```{r bart_plot}
prediction_data <- prediction_data %>%
  mutate(bart_pred = rowMeans(bart_model$y_hat_test)) 


ggplot() +
  geom_point(data = simulated_data,
             aes(x = X, y = Y, color = treatment_group),
             size = 2) +
  stat_function(fun = f_control, aes(color = "Truth - Control")) +
  stat_function(fun = f_treated, aes(color = "Truth - Treatment")) +
  scale_color_manual(
    values = c(
      "Truth - Control" = "red",
      "Truth - Treatment" = "blue",
      "Control" = "red",
      "Treatment" = "blue"
    )
  ) +
  labs(
    title = "Simulated Data, True Functions, and BART Predictions",
    x = "X",
    y = "Y",
    color = ""
  ) +
  theme_bw() +
  theme(legend.position = "none") +
  geom_line(data = prediction_data,
            aes(x = X, y = bart_pred, color = treatment_group),
            linetype = "dashed") +
   scale_color_manual("", values = c("red", "blue","red", "blue", "red", "blue", "red", "blue")) +
   annotate(geom = "segment", x = xstart, y = yVals[1], xend = xstart+length, yend = yVals[1], color = "red") +
   annotate(geom = "text", x = text_left, y = yVals[1], label = c("truth - control"), hjust = 0) +
   annotate(geom = "segment", x = xstart, y = yVals[2], xend = xstart+length, yend = yVals[2], color = "blue") +
   annotate(geom = "text", x = text_left, y = yVals[2], label = c("truth - treatment"), hjust = 0) +
   geom_point(aes(x=xstart+length/2, y=yVals[3]), color = c("red")) +
   annotate(geom = "text", x = text_left, y = yVals[3], label = c("simulated data - control"), hjust = 0) +
   geom_point(aes(x=xstart+length/2, y=yVals[4]), color = c("blue")) +
   annotate(geom = "text", x = text_left, y = yVals[4], label = c("simulated data - treatment"), hjust = 0) +
   annotate(geom = "segment", x = xstart, y = yVals[5], xend = xstart+length, yend = yVals[5], color = "red", linetype = "dashed") +
   annotate(geom = "text", x = text_left, y = yVals[5], label = c("BART - control"), hjust = 0) +
   annotate(geom = "segment", x = xstart, y = yVals[6], xend = xstart+length, yend = yVals[6], color = "blue", linetype = "dashed") +
   annotate(geom = "text", x = text_left, y = yVals[6], label = c("BART - treatment"), hjust = 0) 
```

Notice that BART does a very good job when we have overlap between the treatment
and control groups, but when extrapolating for high values of X, BART cannot get
the true control curve right because it has no data in that region. This
highlights the importance of understanding how any method works.


Once we have fitted the BART model, we can calculate the sample average
treatment effect by predicting the outcome for every individual in our sample
under both treatment and control conditions. The difference between these
predictions gives us the posterior distribution of the treatment effect for each
individual. The sample average treatment effect is then the mean of this
posterior distribution.

```{r bart_sate}
x0 <- simulated_data %>% mutate(is_treated=FALSE) %>% select(X,is_treated)
x1 <- simulated_data %>% mutate(is_treated=TRUE) %>% select(X,is_treated)

pred0 <- predict(bart_model, as.matrix(x0))

pred1 <- predict(bart_model, as.matrix(x1))

tau_draws <- pred1$y_hat - pred0$y_hat  
sate_draws <- colMeans(tau_draws)
cat(glue::glue("BART finds an Average Treatment Effect equal to {round(mean(sate_draws),2)}"))
```

However, as we discussed before, point estimates are often not very useful for
decision-making. For instance, we might make different decisions if the impact
of the intervention is more than 9, between 0 and 9, or less than 0. Calculating
probabilities for different effect sizes is more useful for decision-making than
point estimates alone because it provides a more nuanced understanding of the
potential outcomes and the uncertainty associated with our estimates.
::: {.content-visible when-format="pdf"}
We can easily calculate these probabilities using the draws from the posterior
probability that we just calculated.
::: 
::: {.content-visible when-format="html"}
We can easily calculate these probabilities using the posterior draws and visualize
them:

```{r bart_viz}
#| eval: !expr knitr::is_html_output()
vizdraws::vizdraws(
  posterior = sate_draws,
  breaks = c(0, 9),
  break_names = c("Discontinue", "Continue", " Expand")
)
```
:::

## More Covariates and Propensity Score


Let's extend our exploration of BART by incorporating multiple covariates and a
propensity score. Consider a scenario with an outcome of interest, $y$, a binary
treatment indicator, $z$, and three covariates, $x_1$, $x_2$, and $x_3$, related
as follows:

$$
y \sim N(\mu(x_1,x_2) + z \tau(x_2,x_3) ,\sigma )
$$


```{r fake_data2}
library(dplyr)
library(tidyr)

set.seed(1982)
n <- 1000 

my_df <- tibble(
    x1 = rnorm(n), 
    x2 = rnorm(n), 
    x3 = rnorm(n)
  ) %>%
  mutate(
    mu = if_else(x1 > x2, -0.9, 1.1),
    pi = pnorm(mu),
    z = rbinom(n, 1, pi),
    tau = 1 / (1 + exp(-x3)) + x2 / 10, 
    y = rnorm(n, mean = mu + z * tau, sd = 0.4)
  )

glimpse(my_df)

```

To gain insights into the relationship between $x_3$ and the treatment effect,
$\tau$, let's visualize it:

```{r plot2, message=FALSE, warning=FALSE}

ggplot(data = my_df, aes(x = x3, y = tau)) +
  geom_point(size = 2.5, alpha = 0.7) +
  geom_smooth(
    method = "loess",
    se = FALSE,
    color = "blue",
    linewidth = 1
  ) +
  xlab(expression(x[3])) +
  ylab(expression(tau)) +
  ggtitle(expression(
    paste("Relationship between ", x[3], " and Treatment Effect (", tau, ")")
  )) +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 14, face = "bold"),
    plot.title = element_text(size = 16, hjust = 0.5)
  )
```

::: {.callout-note}

## Tip
One important recommendation when using BART for causal inference is to include
the propensity score, which represents the probability of being in the treatment
group given the covariates, as an additional factor. Empirical research has
shown that doing this can improve the accuracy of treatment effect estimation.
We can estimate the propensity score using a Bayesian generalized linear model
(GLM):

::: 


```{r}

# Fit Bayesian GLM
ps_model <- arm::bayesglm(z ~ x1 + x2 + x3,
                    family = binomial(),
                    data = my_df)

# Calculate Predicted Probabilities
my_df <- my_df %>%
  mutate(ps = predict(ps_model, type = "response"))

# Plot Histogram of Predicted Probabilities
ggplot(my_df, aes(x = ps, fill = factor(z))) +
  geom_histogram(alpha = 0.6, position = "identity", bins = 30) +  # More transparent, overlaid histograms
  facet_wrap(~ z, ncol = 1, labeller = labeller(z = c("0" = "Control", "1" = "Treated"))) + 
  labs(title = "Distribution of Predicted Probabilities by Treatment Group",
       x = "Predicted Probability", 
       y = "Frequency",
       fill = "Treatment Group") +
  theme_bw() +
  scale_fill_manual(values = c("0" = "blue", "1" = "red"))
  

```

Now we can fit our BART model, including the estimated propensity score
alongside our original covariates:


```{r fit_bart2}

X_train <- my_df %>% 
   select(x1, x2, x3, ps, z) %>%
  as.matrix()

bart_model <-
  stochtree::bart(X_train = X_train,
                  y_train = my_df$y,
                  num_burnin = 1000,
                  num_mcmc = 2000)
```

After fitting, let's examine the trace plot of $\sigma^2$ to assess convergence:

```{r}
# Create the Data for Plotting
df_plot <- tibble(
  iteration = seq_along(bart_model$sigma2_samples),    # Sequence of iteration numbers
  sigma = bart_model$sigma2_samples                    
)

# Create the Traceplot
ggplot(data = df_plot, aes(x = iteration, y = sigma)) +
  geom_line(color = "blue", alpha = 0.7) +    # Add transparency for better visual
  labs(title = "Traceplot for BART Model Sigma",
       x = "Iteration",
       y = expression(sigma)) +               # Use expression() for sigma label
  theme_bw() +
  geom_hline(yintercept = mean(df_plot$sigma), linetype = "dashed", color = "red") 
```

We can also assess the effective sample size (ESS) to gauge the efficiency of
the sampling process:

```{r}
library(coda)  # Make sure you have this package installed and loaded

# Calculate Effective Sample Size (ESS)
ess_sigma <- effectiveSize(df_plot$sigma)  # Use the sigma values from your data frame

# Display the Result (with formatting)
cat("Effective Sample Size (ESS) for sigma:", format(ess_sigma, big.mark = ","), "\n") 

```

Now, let's calculate the estimated average treatment effect using BART:


```{r}
x0 <- my_df %>% 
   select(x1, x2, x3, ps, z) %>% 
  mutate(z=0) %>% as.matrix()
x1 <- my_df %>% 
   select(x1, x2, x3, ps, z) %>% 
  mutate(z=1) %>% as.matrix()

pred0 <- predict(bart_model, as.matrix(x0))

pred1 <- predict(bart_model, as.matrix(x1))

tau_draws <- pred1$y_hat - pred0$y_hat  
sate_draws <- colMeans(tau_draws)
cat(glue::glue("BART finds an Average Treatment Effect equal to {round(mean(sate_draws),2)}"))
cat(glue::glue("The truth Average Treatment Effect is equal to {round(mean(my_df$tau),2)}"))
```

::: {.callout-note}
## Tip
A helpful feature of BART is its ability to estimate treatment effects at the
individual level. While these estimates are to be noisy, they can be utilized
with a classification and regression tree (CART) model to explore potential
treatment effect heterogeneity:
::: 


```{r}
library(rpart)
library(rpart.plot)

my_df <- my_df %>% 
  mutate(tau_hat = rowMeans(tau_draws)) # point estimate of the individual's treatment effect



# Fit the Tree Model (Using Cross-Validation)
tree_model <- rpart(
  tau_hat ~ x1 + x2 + x3, 
  data = my_df, 
  method = "anova",      
  control = rpart.control(cp = 0.05, xval = 10)  # Cross-validation with 10 folds
)

# Find Optimal Complexity Parameter (CP)
optimal_cp <- tree_model$cptable[which.min(tree_model$cptable[, "xerror"]), "CP"]

# Prune the Tree for Better Generalization
pruned_tree <- prune(tree_model, cp = optimal_cp)

# Plot the Pruned Tree
rpart.plot(pruned_tree, 
           type = 4, extra = 101, 
           fallen.leaves = TRUE,
           main = "Tree Model for Individual Treatment Effects", 
           cex = 0.8, 
           box.palette = "GnBu")  # Green-Blue palette

# Print the Pruned Tree Summary
printcp(pruned_tree)  # See the results of pruning

```
In our analysis, the CART model successfully identified $x_3$ as the primary
driver of treatment effect heterogeneity, aligning with the way we generated our
data. However, it's important to exercise caution when interpreting the results
of CART. CART models do not inherently account for uncertainty, and therefore,
their splits might not always reveal characteristics that genuinely
differentiate subgroups with meaningfully distinct impacts. In other words,
while CART pinpointed $x_3$ as the key modifier, it also says that $x_1$ is a
modifier, and we know that is not correct.

To delve deeper into the question of whether the impacts truly vary based on a
particular characteristic, we can leverage the posterior draws obtained from the
fitted BART model. These draws offer a richer representation of the uncertainty
surrounding the estimated treatment effects, enabling us to conduct more robust
analyses regarding heterogeneity. Remember, it's crucial to avoid
overinterpreting the CART results and to explore the full posterior distribution
for a more nuanced understanding of the causal relationships in our data.

::: {.callout-tip}
## Learn more
@hill2011bayesian Bayesian nonparametric modeling for causal inference.
::: 

## Accelerated BART (XBART)

TODO