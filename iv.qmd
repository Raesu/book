---
title: "Randomized Encouragement Design"
share:
  permalink: "https://book.martinez.fyi/iv.html"
  description: "Business Data Science: What Does it Mean to Be Data-Driven?"
  linkedin: true
  email: true
  mastodon: true
---

A common business question that arises is: "What is the value of a given set of
users taking some action?" You cannot simply randomly assign users to take or
not take the action—you can't force them to do anything! The most effective
alternative in such scenarios is a Randomized Encouragement Design (RED). This
study design employs an econometric approach called instrumental variables,
where a subset of users is randomly encouraged to take the action. The history
of instrumental variables is quite interesting. It was first introduced in
the 1920s by economist Philip Wright in an unlikely place: an appendix to his
book on the tariffs on animal and vegetable oils. While the book itself didn't
cause much of a stir, the appendix contained a groundbreaking solution to the
identification problem in economics. Notably, the instrumental variables
approach was a key factor in the research that earned Joshua Angrist and Guido
Imbens the 2021 Nobel Prize in Economics. Their work, which included the famous
study on the causal effect of education on earnings using the draft lottery as
an instrument, has had a profound impact on how economists and other social
scientists approach causal questions.

## Understanding REDs: Compliance and Potential Outcomes

REDs rely on understanding how users respond to encouragement:

  - **Compliers:** Change their behavior based on the encouragement.
  - **Always-takers:** Would take the action regardless of encouragement.
  - **Never-takers:** Wouldn't take the action regardless of encouragement.
  - **Defiers:** (Rarely assumed to exist) Do the opposite of what's encouraged.

The potential outcomes framework is crucial for understanding these designs. For
each participant, we consider:

  - $Y(1)$: The outcome if encouraged
  - $Y(0)$: The outcome if not encouraged
  - $D(1)$: The treatment status if encouraged
  - $D(0)$: The treatment status if not encouraged

## What We Want to Know

In the world of REDs, our curiosity often leads us down two paths. Sometimes,
we're primarily interested in the value of the encouragement itself—the nudge,
the prompt, the incentive. In this case, we estimate the Intent-to-Treat (ITT)
effect: the average change in the outcome simply due to being encouraged,
regardless of whether individuals actually comply and take the action.
Mathematically, this is represented as:

$$ITT = E[Y(1) - Y(0)]$$

However, in other scenarios, our focus shifts to the true causal impact of the
action itself. We want to know what happens when people actually take that step,
change that behavior, or adopt that product. This is where the Complier Average
Causal Effect (CACE), also known as the Local Average Treatment Effect (LATE),
comes in. It isolates the average effect of the treatment specifically for those
who are swayed by the encouragement—the compliers. The CACE is calculated as:

$$CACE = \frac{E[Y|Z=1] - E[Y|Z=0]}{E[D|Z=1] - E[D|Z=0]}$$ 

where $Z$ is the encouragement assignment and $D$ is the actual treatment
received.

## Making it Work: Key Assumptions

For REDs to yield valid causal conclusions, a few things need to hold true:

  - **Ignorability of the instrument:** The encouragement assignment is random
    and independent of potential outcomes.
  - **Exclusion restriction:** Encouragement affects the outcome only through
    its effect on treatment take-up.
  - **Monotonicity:** There are no defiers (i.e., no one does the opposite of
    what they're encouraged to do).
  - **Relevance:** The encouragement actually affects treatment take-up (i.e.,
    there are compliers).
    
Which path we choose—ITT or CACE—depends entirely on the business decisions
we're trying to inform. Are we evaluating the effectiveness of our marketing
campaigns, where the encouragement itself is the key lever? Or are we trying to
understand the inherent value of a product feature, where the actual usage is
what matters most? REDs give us the flexibility to answer both questions,
guiding our decisions with robust causal insights.  

## Examples with synthetic data