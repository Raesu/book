---
title: "Randomized Encouragement Design"
share:
  permalink: "https://book.martinez.fyi/iv.html"
  description: "Business Data Science: What Does it Mean to Be Data-Driven?"
  linkedin: true
  email: true
  mastodon: true
author:
  - name: Yichi Zhang
  - name: Ignacio Martinez
---

![Nudge](img/nudge.jpg){.lightbox}

In the dynamic world of business, a recurring question is, "What if our users
took a particular action? What would be the impact?" Yet, randomly assigning
users to specific behaviors—forcing their hand, so to speak—isn't feasible. The
most effective approach in such situations is a Randomized Encouragement Design
(RED), a clever method rooted in the econometric concept of instrumental
variables. In a RED, a subset of users is randomly nudged towards the action in
question.

The history of instrumental variables is itself a fascinating tale, originating
in the 1920s with economist Philip Wright. Surprisingly, it first appeared in an
appendix to his book on tariffs for animal and vegetable oils. While the book
itself didn't make a big splash, the appendix held a groundbreaking solution to
the identification problem in economics. This approach, instrumental variables,
even played a pivotal role in the research that earned Joshua Angrist and Guido
Imbens the 2021 Nobel Prize in Economics. Their renowned study on the causal
effect of education on earnings, using the draft lottery as an instrument,
revolutionized how economists and social scientists approach causal questions.

## Understanding REDs: Compliance and Potential Outcomes

REDs hinge on understanding how users respond to encouragement, categorizing
them into four groups:

  - **Compliers:** Those who change their behavior based on encouragement.
  - **Always-takers:** Those who would take the action regardless of
    encouragement.
  - **Never-takers:** Those who wouldn't take the action regardless of
    encouragement.
  - **Defiers:** (Rarely assumed to exist) Do the opposite of what's encouraged.

The potential outcomes framework is crucial for understanding these designs. For
each participant, we consider:

  - $Y(1)$: The outcome if encouraged
  - $Y(0)$: The outcome if not encouraged
  - $D(1)$: The treatment status if encouraged
  - $D(0)$: The treatment status if not encouraged

## What We Want to Know

In the world of REDs, our curiosity often leads us down two paths. Sometimes,
we're primarily interested in the value of the encouragement itself—the gentle
push, the subtle prompt, the enticing incentive. In this case, we estimate the
Intent-to-Treat (ITT) effect: the average change in the outcome simply due to
being encouraged, regardless of whether individuals actually comply and take the
action. Mathematically, this is represented as:

$$ITT = E[Y(1) - Y(0)]$$

However, in other scenarios, our focus shifts to the true causal impact of the
action itself. We want to know what happens when people actually take that step,
change that behavior, or adopt that product. This is where the Complier Average
Causal Effect (CACE), also known as the Local Average Treatment Effect (LATE),
comes in. It isolates the average effect of the treatment specifically for those
who are swayed by the encouragement—the compliers. The CACE is defined as:

$$CACE = E[Y(1) - Y(0)|D(1) = 1, D(0) = 0]$$ 

where $Z$ is the encouragement assignment and $D$ is the actual treatment
received.

## Making it Work: Key Assumptions

For REDs to yield valid causal conclusions, a few things need to hold true:

  - **Ignorability of the instrument:** The encouragement assignment is random
    and independent of potential outcomes.
  - **Exclusion restriction:** Encouragement affects the outcome only through
    its effect on treatment take-up.
  - **Monotonicity:** There are no defiers (i.e., no one does the opposite of
    what they're encouraged to do).
  - **Relevance:** The encouragement actually affects treatment take-up (i.e.,
    there are compliers).
    
The choice between focusing on the ITT or the CACE depends entirely on the
business decisions we're trying to inform. Are we evaluating the effectiveness
of our marketing campaigns, where the encouragement itself is the key lever? Or
are we trying to understand the inherent value of a product feature, where the
actual usage is what matters most? REDs give us the flexibility to answer both
questions, guiding our decisions with robust causal insights.

## Examples with synthetic data

![BIVA](img/biva.png){.lightbox}

The [{BIVA}](https://github.com/google/biva) R package is a Bayesian
implementation of instrumental variable analysis. The package
[vignette](https://github.com/google/biva/blob/main/vignettes/Gaussian.Rmd)
contains a simulated example demonstrating its workflow and caveats. The
simulated dataset is from a hypothetical RED under two-sided noncompliance. In
other words, those encouraged to take the treatment can opt out, and those not
encouraged can take the treatment, but there is no defier. Therefore, there
exist compliers, never-takers, and always-takers in the study.


```{r simulated_data_two_side}
set.seed(1997)
n <- 200

# Covariates
# X (continuous, observed)
# U (binary, unobserved)
X <- rnorm(n)
U <- rbinom(n, 1, 0.5)

## True memberships of principal strata (1:c,2:nt,3:at): S-model depends only on U
true.PS <- rep(0, n)
U1.ind <- (U == 1)
U0.ind <- (U == 0)
num.U1 <- sum(U1.ind)
num.U0 <- sum(U0.ind)
true.PS[U1.ind] <- t(rmultinom(num.U1, 1, c(0.6, 0.3, 0.1))) %*% c(1, 2, 3)
true.PS[U0.ind] <- t(rmultinom(num.U0, 1, c(0.4, 0.5, 0.1))) %*% c(1, 2, 3)

## Treatment assigned: half control & half treatment
Z <- c(rep(0, n / 2), rep(1, n / 2))

## Treatment received: determined by principal strata and treatment assigned
D <- rep(0, n)
c.trt.ind <- (true.PS == 1) & (Z == 1)
c.ctrl.ind <- (true.PS == 1) & (Z == 0)
nt.ind <- (true.PS == 2)
at.ind <- (true.PS == 3)
num.c.trt <- sum(c.trt.ind)
num.c.ctrl <- sum(c.ctrl.ind)
num.nt <- sum(nt.ind)
num.at <- sum(at.ind)
D[at.ind] <- rep(1, num.at)
D[c.trt.ind] <- rep(1, num.c.trt)

## Generate observed outcome: Y-model depend on X, U, D, and principal strata
Y <- rep(0, n)
Y[c.ctrl.ind] <- rnorm(num.c.ctrl,
  mean = 5000 + 100 * X[c.ctrl.ind] - 300 * U[c.ctrl.ind],
  sd = 50
)
Y[c.trt.ind] <- rnorm(num.c.trt,
  mean = 5250 + 100 * X[c.trt.ind] - 300 * U[c.trt.ind],
  sd = 50
)
Y[nt.ind] <- rnorm(num.nt,
  mean = 6000 + 100 * X[nt.ind] - 300 * U[nt.ind],
  sd = 50
)
Y[at.ind] <- rnorm(num.at,
  mean = 4500 + 100 * X[at.ind] - 300 * U[at.ind],
  sd = 50
)

df <- data.frame(Y = Y, Z = Z, D = D, X = X, U = U)
```

In this example, the CACE is set to be 5250 - 5000 = 250.

### The ITT analysis

The ITT effect point estimate can be straightfowardly computed:

```{r ITT_two_side}
ITT <- sum(Y*Z)/sum(Z) - sum(Y*(1-Z))/sum(1-Z)
ITT
```

The effect of the nudge is estimated to be very small as compared to the
true CACE. It again illustrates the difference between the effect of the
nudge and the effect of the treatment.

### The Bayesian instrumental variable analysis (BIVA)

#### Prior predictive checking

The strength of a Bayesian approach lies in its ability to seamlessly integrate
prior business knowledge and assumptions through the use of prior distributions.
A prudent first step in any analysis is visualizing these assumptions through
the prior parameters fed into the model. This allows users to refine them before
delving into the data, ensuring the priors truly reflect their domain expertise.

```{r priors_check_two_side, fig.align = 'center', message=FALSE, results = "hide"}
library(biva)

ivobj <- biva$new(
  data = df, y = "Y", d = "D", z = "Z",
  x_ymodel = c("X"),
  x_smodel = c("X"),
  ER = 1,
  side = 2,
  beta_mean_ymodel = cbind(rep(5000, 4), rep(0, 4)),
  beta_sd_ymodel = cbind(rep(200, 4), rep(200, 4)),
  beta_mean_smodel = matrix(0, 2, 2),
  beta_sd_smodel = matrix(1, 2, 2),
  sigma_shape_ymodel = rep(1, 4),
  sigma_scale_ymodel = rep(1, 4),
  fit = FALSE
)

ivobj$plotPrior()
```

In our example, the prior information assumes the impact of the treatment is
centered around zero, with the distributions of compliance types being
relatively balanced.

#### Model fitting and diagnostics

After tailoring prior parameters to reflect our existing knowledge, we proceed
with fitting the model.

```{r fit_two_side, fig.align = 'center', message=FALSE, results = "hide"}
ivobj <- biva$new(
  data = df, y = "Y", d = "D", z = "Z",
  x_ymodel = c("X"),
  x_smodel = c("X"),
  ER = 1,
  side = 2,
  beta_mean_ymodel = cbind(rep(5000, 4), rep(0, 4)),
  beta_sd_ymodel = cbind(rep(200, 4), rep(200, 4)),
  beta_mean_smodel = matrix(0, 2, 2),
  beta_sd_smodel = matrix(1, 2, 2),
  sigma_shape_ymodel = rep(1, 4),
  sigma_scale_ymodel = rep(1, 4),
  fit = TRUE
)
```

To ensure the model is behaving as expected, we inspect the trace plot of
outcomes across different compliance strata, confirming satisfactory mixing and
convergence. The BIVA package empowers us to take ownership of understanding the
results, scrutinizing any potential issues in the fitting process before drawing
conclusions.

```{r tracePlot_two_side, fig.align = 'center'}
ivobj$tracePlot()
```

With a healthy trace plot, we proceed with a weak instrument test, ensuring the
validity of our results in the face of potential weak instruments.

```{r weak_IV_test_two_side, fig.align = 'center'}
ivobj$weakIVTest()
```

The test indicates no cause for concern regarding weak instruments.

#### Posterior summary

Now, we harness the power of Bayesian analysis to extract meaningful insights
tailored to our business questions. The BIVA package provides a suite of methods
to summarize the findings.

```{r findings_two_side, fig.align = 'center'}
# Posterior distribution of the strata probability
ivobj$strataProbEstimate()
# Posterior probability that CACE is greater than 200
ivobj$calcProb(a = 200)
# Posterior median of the CACE
ivobj$pointEstimate()
# Posterior mean of the CACE
ivobj$pointEstimate(median = FALSE)
# 75% credible interval of the CACE
ivobj$credibleInterval()
# 95% credible interval of the CACE
ivobj$credibleInterval(width = 0.95)
```

Through these methods, we obtain precise point estimates, informative
probabilistic statements, and a flexible quantification of uncertainty that
fuels informed decision-making.

#### Visualizations of knowledge progression

The evolution of our understanding, from prior knowledge to the updated
posterior informed by data, can be vividly visualized.

::: {.content-visible when-format="html"}
```{r comparison_two_side, message=FALSE}
#| eval: !expr knitr::is_html_output()
ivobj$vizdraws(
  display_mode_name = TRUE, breaks = 200,
  break_names = c("< 200", "> 200")
)
```
:::

This visual comparison elucidates how the data refines our initial beliefs,
enhancing our grasp of the underlying causal relationships.

::: {.callout-tip}
## Further Exploration

To delve deeper into the intricacies of instrumental variable analysis and
causal inference, we recommend the following resources:

  - @mcelreath2018statistical14 Statistical Rethinking: Instruments and causal
    designs.
  - @angrist1996identification Identification of causal effects using
    instrumental variables.
  - @frangakis2002principal Principal stratification in causal inference.
  - @imbens2014instrumental Instrumental variables: An econometrician's
    perspective.
  - @imbens1997bayesian Bayesian inference for causal effects in randomized
    experiments with noncompliance.
  - @liu2023pstrata PStrata: An R Package for Principal Stratification.
  - @vanderweele2011principal Principal stratification--uses and limitations.
  - @li2022principal Post-treatment confounding: Principal Stratification.
:::